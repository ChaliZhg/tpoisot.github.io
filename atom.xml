<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title>Tim Poisot :: Updates</title>
  <link href="http://timotheepoisot.fr/atom.xml" rel="self"/>
  <link href="http://timotheepoisot.fr/"/>
  <updated>2014-11-21T21:26:28+13:00</updated>
  <id>Tim Poisot</id>
  <author>
    <name>Tim Poisot</name>
    <email></email>
  </author>
  
  <entry>
     <title>So you're not a programmer. Are you sure?</title>
     <link href="http://timotheepoisot.fr/2014/11/21/not-a-programmer/"/>
    <updated>2014-11-21T00:00:00+13:00</updated>
    <content type="html">&lt;p&gt;In a recent lab meeting, we had a discussion about some good practices in
programming – unit testing, defensive programming, coverage analysis, all
that. For most of my undergrad, a substantial part of my master, and some of
my PhD, I have been running experiments, either in lab classes or for actual
research. My undergrad in particular was in cell biology and genetics. The
experiments in this field are only as good as the &lt;em&gt;controls&lt;/em&gt;: a smaller
experiment, or additional condition, that you run alongside the main one
to (i) show that everything that is supposed to works actually works, and
(ii) what is supposed &lt;em&gt;not&lt;/em&gt; to work actually does not. This is why you run
PCR with a sample you know works, and a tube of with (supposedly) no DNA in
it. If any of these two gives something other than the expectation (namely,
one amplification, one lack of amplification), you throw away the gel and
start again.&lt;/p&gt;

&lt;p&gt;These are not cosmetic things. These are not things you do just to make
sure. These controls are a pre-requisite for any result to be taken
seriously. Developing the best set of controls, those that capture all
possible events and allow them to discriminate between them and give you an
idea of what went wrong, is something of an art form (and a frequent exam
question). No experimental biologists can pretend to show anything if there
are no controls. I spent my first years as an apprentice scientist being told
about, and thinking about, controls most of the day. This is because biology is
tricky, and many things can go wrong in an experiment. Controls are required.&lt;/p&gt;

&lt;p&gt;I am now a computational scientist of some sort. Most of what I do is taking
data, running simulations, formatting results, and using the output to
tell a story. This is &lt;em&gt;in practice&lt;/em&gt; extremely different from taking cells
or tissues, and running DNA extractions and PCR and cutting the gels to
send that to sequencing. But this is the same &lt;em&gt;process&lt;/em&gt;: transforming raw
material (data/cells) into results, through a process that I think is the
best (analysis/experiments). Writing scripts and pipetting have even some
&lt;a href=&quot;http://bytesizebio.net/2014/11/10/why-scripting-is-not-as-simple-as-scripting/&quot;&gt;similarities&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;Back to the lab meeting. One argument against spending too much time learning
all of the methodology associated with testing (and the even longer time
required to build the self-discipline needed to apply it) was that despite
being computational biologists, we were not &lt;em&gt;programmers&lt;/em&gt;. I tend to disagree
with that (in that I spend most of my time programming things, and the rest
of the time writing papers to discuss the output of said programs). And I
can conceive that all of this methodology is heavy to handle.&lt;/p&gt;

&lt;p&gt;But testing your code is not about being a programmer. It’s about making
sure that things work. And you can’t eyeball the results. The number of PCR
I ran is in the triple-digits. I have extracted DNA from probably more than
a thousand samples. I’ve spent enough time at the bench that I can do most
things without pausing to think about the next step. It would never occur to
me to do it, even for a single sample, without running some controls too. I
never do an experiment with bacteria without leaving an empty plate or an
empty microcosm to test for contamination. I don’t trust raw material, and
I certainly don’t trust myself. Making good use of testing and all of these
tools is not about being a programmer. It’s about upholding the same values
as experimental scientists, and recognizing that all it takes is one step
to go wrong for the whole thing to fall apart. We can’t &lt;em&gt;not&lt;/em&gt; make mistakes,
all we can do is make sure that we catch them.&lt;/p&gt;

&lt;p&gt;Yet, I’m far from blameless when it comes to writing code. Most of the
packages I release are tested in some way (the latests are as close to 100%
coverage as I can get). Because writing good test suites is hard. Because
defensive programming means that two lines to perform the actual calculation
are buried into ten lines of checks and error handling. I still have to
force myself to write tests. I usually do. I have to keep asking myself
“Are you sure you’re not a programmer?”.&lt;/p&gt;

</content>
  </entry>
  
  <entry>
     <title>Live-tweeting at conferences</title>
     <link href="http://timotheepoisot.fr/2014/11/17/live-tweets/"/>
    <updated>2014-11-17T00:00:00+13:00</updated>
    <content type="html">&lt;p&gt;The main appeal of scientific meetings is that you get to see results, and what
other people are working on, before any of it is published (and free food –
seriously, oh so much free food). Given that there is an increasing number
of people &lt;a href=&quot;/2014/08/17/twitter-esa-imcc/&quot;&gt;using Twitter&lt;/a&gt; to cover meetings, it’s no surprise that there
is an active discussion about what is off-limits, in terms of what to tweet,
and what to keep on the down low. To have an idea of the range of different
opinions and issues, you &lt;em&gt;should&lt;/em&gt; read open science advocate extraordinaire
&lt;a href=&quot;http://blogs.egu.eu/palaeoblog/2014/11/13/lets-have-a-discussion-about-live-tweeting-academic-conferences/&quot;&gt;Jon Tennant’s blog post&lt;/a&gt; on the topic.&lt;/p&gt;

&lt;p&gt;Many folks have been calling for event-level policy. &lt;a href=&quot;https://twitter.com/sfnpolice&quot;&gt;SFN14&lt;/a&gt;, for example,
banned all live-tweeting while the sessions were proceeding. &lt;a href=&quot;https://www.grc.org/about.aspx&quot;&gt;Gordon Research
Conferences&lt;/a&gt; are notoriously restrictive about what you can share of
the events that transpire during them (hint: nothing). Event-level policies
might be good, but they have the huge problem of effectively overruling
individual opinions about what is tweetable or not.&lt;/p&gt;

&lt;p&gt;I start my talks with a slide saying that &lt;em&gt;everything&lt;/em&gt; is fair game as long as
I get credit for it (that’s the basis of the &lt;em&gt;CC-BY 4.0&lt;/em&gt; license, under which
I release most of my non-code, non-data stuff, including this very blog). You
can record the audio (Worst. Podcast. Ever.), a video, take pictures of
the slides, and share them online as long as my name is attached to it in
some way. Starting from a few months ago, every slide set I presented had
a DOI attached to it, and more often than not, it is given somewhere in the
slides. And I made it a rule to &lt;em&gt;never&lt;/em&gt; present already published material
at meetings. No one wants to hear me talk for 12 minutes about &lt;em&gt;anything&lt;/em&gt;
when the alternative is reading the paper in warm, comforting silence.&lt;/p&gt;

&lt;p&gt;Slides that present results, in particular, seem to be the most problematic. I
have a lot of trouble following the argument. Some people opposed to having
them shared online (as is their right to do – if they don’t want you to
share, don’t share). But the argument that is usually given is “people might
see my results”. As opposed to the people that are sitting in the session,
that will close their eyes, put their fingers in their ears, and go &lt;em&gt;na-na-na
can’t hear you&lt;/em&gt; until the non-results part of the talk is reached, I suppose.&lt;/p&gt;

&lt;p&gt;All sarcasm left aside, I understand that the fear of getting scooped is
real in some people, and justified in some fields (I don’t think it is the
case in ecology, fortunately). But the whole point of going to a conference
is that people will see your slides. And hear what you have to say about
them. If your fear is of being scooped, the people &lt;em&gt;in the room&lt;/em&gt; that have
access to the slide in context are much more threatening that the people
that will only see one blurry picture and a one-sentence summary on twitter.&lt;/p&gt;

&lt;p&gt;Having your science shared on twitter (unless it is “holy cow you guys
look at how bad this paper is!”) is all win for you. It’s free publicity
about what you do (so people might go to your website and check your other
papers, or go read the preprint, or send you emails to talk about it). And
because twitter only allows short messages, at best, people will only have
a photo and a single sentence. This is very, very far from enough material
to reproduce a paper and scoop you.&lt;/p&gt;

&lt;p&gt;And it’s a great ego boost too! A few months ago, I was talking in a room
with no clock, so I laid my phone on the stand in front of me. And people
&lt;em&gt;in the room&lt;/em&gt; where saying good things about my talk. This is real-time
feedback, and it feels &lt;em&gt;awesome&lt;/em&gt;. Seeing that the fruit of your labor is
making people in a positive way in real time never happens in science except
in this particular context.&lt;/p&gt;

&lt;p&gt;To wrap things up – I am fine with conference-level policies, as long as
they allow each individual speaker to decide how much can be shared, and
how. This is why we have open (and closed, too) licenses, and they apply to
all sorts of intellectual property. It would be productive for conferences
to state the default (assumed) policy, in case speakers are not explicit. It
would make sense that this default be opt-in: if the speaker says nothing,
you can’t share. But saying &lt;em&gt;all sharing is prohibited&lt;/em&gt; is absurd; conference
organizers have many jobs, none of which involves deciding, for me, how I
can or cannot share my own research.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>We all took baby steps towards open science</title>
     <link href="http://timotheepoisot.fr/2014/11/16/baby-steps-open-science/"/>
    <updated>2014-11-16T00:00:00+13:00</updated>
    <content type="html">&lt;p&gt;The always excellent and insightful Christie Bahlai has a great blog post
about &lt;em&gt;&lt;a href=&quot;https://practicaldatamanagement.wordpress.com/2014/10/23/baby-steps-for-the-open-curious/&quot;&gt;Baby steps for the open curious&lt;/a&gt;&lt;/em&gt;. &lt;em&gt;Open curious&lt;/em&gt;, a term
that was coined during a meeting of &lt;a href=&quot;http://innge.net/wiki/index.php/Open_Science&quot;&gt;INNGE’s Open Science group&lt;/a&gt; refers
to people that &lt;em&gt;would like&lt;/em&gt; to start adopting more open practices, but are
not sure how to do it, or are still concerned by some of the perceived risks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/babysteps.jpg&quot; alt=&quot;Baby steps!&quot; /&gt;Our son is learning
to stand up and walk, so the metaphor of baby steps is one I can relate
to. There are three things to baby steps. You can’t be too ambitious,
you can’t (probably) do it on your own, and it’s scary (for both the baby
and the parents, as far as actual baby steps are concerned). You need to
build up, slowly, and have someone guide you through the whole process.
This is a perfect metaphor for open science.&lt;/p&gt;

&lt;p&gt;In a few months, I will have my first (very own) students, and I want them
to be open scientists. I want to train them to do transparent research, be
accountable for what they do and how they do it, and make sure their research
has &lt;em&gt;all&lt;/em&gt; of the possible sorts of impact. Open science is the shortest path
to success, because people can actually build on what you do, reproduce it,
and give you credit for it. And it makes you a good &lt;a href=&quot;https://peerj.com/preprints/549/&quot;&gt;academic citizen&lt;/a&gt;
too! I am in the process of completing the &lt;a href=&quot;http://software-carpentry.org/&quot;&gt;Software Carpentry&lt;/a&gt;
instructor training; in part because it will help me train these and other
students better. There is much emphasis on how to bring people that know
very little to being able to do things independently. This, too, applies to
open science as a whole.&lt;/p&gt;

&lt;p&gt;Being an open scientist is &lt;em&gt;complicated&lt;/em&gt;, because we don’t even have a clear
and consensual definition of what it means. I publish in non open access
journals, for example. But these non-OA papers are available as preprints. It
is open, but is it open enough? This is one of the problems that we will have
to overcome if we want to help people do more open science; we can’t just
say &lt;em&gt;release ALL your things&lt;/em&gt;. It’s vague, it’s frightening, and I don’t know
a single person that woke up one morning, and released &lt;em&gt;ALL&lt;/em&gt; its things. It
has been, I guess, a long, gradual, and iterative process for most of us.&lt;/p&gt;

&lt;p&gt;When someone asks “How do I do open science?”, it’s rude to say “See all the
things I do? Do them too.”. Everyone has a different workflow. Saying “To
do open science, do things this way” is counterproductive. The constructive
reply would be “Show me what you do, and I will try to give you tips about
how to make things more open”.&lt;/p&gt;

&lt;p&gt;This approach has a lot of benefits over presenting The One True Open Science
Workflow. First, it slices the problem (How to make my research open?) into
a lot of very small tasks, most of which will require very little time to
implement. Second, people will be able to &lt;em&gt;decide&lt;/em&gt; which to do, if any, and
when they do it. I suspect that most people will be tempted to follow the
advice of &lt;em&gt;&lt;a href=&quot;http://choosealicense.com/&quot;&gt;pick an open source license&lt;/a&gt;&lt;/em&gt;, instead of &lt;em&gt;&lt;a href=&quot;http://software-carpentry.org/v5/novice/sql/index.html&quot;&gt;ditch Excel
and use a database&lt;/a&gt;&lt;/em&gt;. We won’t reach the point where 100% of research
is open (unless funding agency enforce it, and there is a non-0 probability
that this will happen); that’s too bad.&lt;/p&gt;

&lt;p&gt;If people are &lt;em&gt;open curious&lt;/em&gt;, and I suspect most people are, they &lt;em&gt;want&lt;/em&gt;
to learn about open science. It would be awfully counterproductive to turn
them off by requiring that they start again from scratch. Working with them
at identifying areas where small changes can have a big impact is key. I
think we need more mentoring, and more of an open-door policy for open
curious people, if we want more research to be open over time.&lt;/p&gt;

</content>
  </entry>
  
  <entry>
     <title>Hey Matt</title>
     <link href="http://timotheepoisot.fr/2014/11/13/hey-matt/"/>
    <updated>2014-11-13T00:00:00+13:00</updated>
    <content type="html">&lt;p&gt;Hey &lt;a href=&quot;http://rosetta.jpl.nasa.gov/matt-taylor&quot;&gt;Matt&lt;/a&gt;,&lt;/p&gt;

&lt;p&gt;Hi. I’m Tim. I do some science. Some is good. Today, I managed to screw up
one calculation, but I compensated by writing a paragraph for a paper that
I think makes a nice compelling argument for some thing that may interest a
few of my colleagues. This does not amount to much, since you contributed to
landing a robot on a comet. But in terms of scientific achievement, I still
come up way ahead today; and so do many, many other scientists over the world.&lt;/p&gt;

&lt;p&gt;Want to know why? How is that possible? Simple. We managed not to alienate
and objectify our female colleagues today.&lt;/p&gt;

&lt;p&gt;I know, that’s crazy, right? Want to know how? We woke up this morning, and
thought, &lt;em&gt;Hey, perhaps I should &lt;a href=&quot;http://www.businessinsider.com.au/rosetta-scientist-matt-taylor-wore-sexist-shirt-for-philae-launch-2014-11&quot;&gt;dress like an asshole today&lt;/a&gt;&lt;/em&gt;. Then
we realized that it would be ridiculous, and that no one in their right mind
would do so.&lt;/p&gt;

&lt;p&gt;See, there are these people called &lt;em&gt;women&lt;/em&gt;. Look on your shirt – they’re
like that, but dressed. They do science, too. In fact they do most things as
well as most of us. And they deserve every bit of our admiration, because boy
are we giving them a hard time. We might not even realize it; that’s the way
being a white middle-class 30-something-or-so male works. The academic life
is though with them. You know how though academia is for all of us? Well
that’s one of the point where women are not like us: for them, it’s worse.&lt;/p&gt;

&lt;p&gt;So anyway, that shirt. It sends a message. &lt;em&gt;Hi, I’m Matt Taylor, and I
like my women naked&lt;/em&gt;. This is the message you chose for your day under the
limelight. Or did you got women and potted plants confused? Distasteful shirts
the world over have plants patterns, and these are usually naked in that
clothes on a plant makes no sense. It’s OK to objectify potted plants. If you
have some sort of impairment that results in you mixing up shapes, I apologize.&lt;/p&gt;

&lt;p&gt;Seriously, that shirt. It costed, what, 25 bucks? Do you realize that a 25$
piece of clothing made &lt;em&gt;scientists&lt;/em&gt; are getting angry enough to talk about it
more about the fact that you contributed to landing a spacecraft on, holly
shit, a comet? There is no prefix in the international system to measure
how hard you have to screw up to &lt;em&gt;make scientists care less about one of the
biggest achievements of science than about the clothes you wear&lt;/em&gt;. I humbly
suggest the Taylor: a measure of how much your douchebaggery prevents people
from appreciating your scientific achievements.&lt;/p&gt;

&lt;p&gt;But I ought to thank you. Yeah! Because your shirt allowed to reveal something
important and positive about scientists. We don’t tolerate this shit. A lot
of us, a majority perhaps, are working everyday to make science a welcoming
place, where everyone is allowed to be different, and no one is allowed to
discriminate or objectify. This involves a lot of positive things. Hiring
more women, more minorities. Being sensitive to cultural differences, and the
fact that the system is inherently harder on some. Giving equal opportunity
to all. Judging people by what they bring to the field, not by whether or not
you’d “hit that”. Not being an all-around terrible person. Easy things. But
as you’ve discovered, there is a dark side too. If you screw up, and if you
unmake all of the work that we do day by day, you’ll take shit. We’ll make
sure that people hear that we disagree with that.&lt;/p&gt;

&lt;p&gt;That shirt. It’s so bad that instead of saying &lt;em&gt;how cool is it that we
landed a spacecraft on a comet&lt;/em&gt;, we’re saying to our female colleagues &lt;em&gt;I’m
sorry you have to deal with sexist assholes like Matt Taylor all the time,
how can I help&lt;/em&gt;. You’re supposed to make science &lt;em&gt;sexy&lt;/em&gt;, not &lt;em&gt;sexist&lt;/em&gt;. You
though that would not be rocket science – that’s apparently way harder.&lt;/p&gt;

&lt;p&gt;Cheers, &lt;br /&gt;
Tim&lt;/p&gt;

</content>
  </entry>
  
  <entry>
     <title>Tales from the job search II - The Crossing of the Desert</title>
     <link href="http://timotheepoisot.fr/2014/11/11/tales-job-search-II/"/>
    <updated>2014-11-11T00:00:00+13:00</updated>
    <content type="html">&lt;p&gt;&lt;a href=&quot;http://timotheepoisot.fr/2014/09/24/tales-job-search-I/&quot;&gt;Last time&lt;/a&gt;, I told the story of my application to French academic
jobs, the really weird process of applying and interviewing, and (ultimately)
the rejection. After taking a few months to think about it, I decided to
apply to more “standard” positions. My main motivation was (still is) that
I wanted more teaching, and more interactions with students, than the French
system usually offers.&lt;/p&gt;

&lt;p&gt;There were a few jobs openings at this time, and I went through the process
of putting together a research and teaching statement, updating my resume,
and all that. Over a period of a few months, I ended up with a quite nice
package to send around. Papers were coming out, too, so I figured that my
first interview was going to happen soon enough (I was wrong). I was not in
any particular hurry to get a job (I just signed up for a post-doc with two
years of funding), so I carefully selected where I would like to apply.&lt;/p&gt;

&lt;p&gt;I sent two applications. I got two negative replies. This chapter or my job
search was a whole year of nothing much happening (by choice), and a few
weeks of wondering when I would have a decision, followed by a rejection
letter. It was boring, and anti-climactic to an extreme.&lt;/p&gt;

&lt;p&gt;Then some time in spring (or whatever passes for spring in Québec),
my wife and I discovered that we were soon going to be parents. Both of
us are biologists, so we were well aware that this event was well within
the expected confidence interval. But still we had to deal with all of the
practical details. One of the most pressing being that I kicked myself in
the metaphorical nuts, and decided to get a position.&lt;/p&gt;

&lt;p&gt;I would have been happy to keep on applying to position periodically for a
year or two – life as a post-doc is pretty sweet when you are not running
after funding – but having to take care of baby required a little bit
more certainties about where we will be in the medium term. And when I was
attending a conference in Montreal in August, I heard about an opening in
numerical ecology. That seemed like a dream job (still does!), so I decided
to apply. That will be the story of part 3…&lt;/p&gt;

</content>
  </entry>
  
  <entry>
     <title>How I quit academia and vowed to never come back (for a month)</title>
     <link href="http://timotheepoisot.fr/2014/10/14/how-i-quit-academia/"/>
    <updated>2014-10-14T00:00:00+13:00</updated>
    <content type="html">&lt;p&gt;I am currently going through the Instructor Training for &lt;a href=&quot;http://software-carpentry.org/&quot;&gt;Software
Carpentry&lt;/a&gt;. It is a really rich and transformative experience, and besides
the software side of things, full of useful elements for teaching in any
context. As we are currently discussing how motivation affects learning, our
weekly assignment is to write the story of our biggest de-motivation, how
it affected us, and what happened (and should have happened) next. Here’s
mine. Very few people know it, even fewer know the name of all parties
involved.&lt;/p&gt;

&lt;h1 id=&quot;the-story&quot;&gt;The story&lt;/h1&gt;

&lt;p&gt;I was finishing my BSc in biology (with a strong bias towards cellular
biology and physiology), and I had decided a few months ago that I wanted
to switch fields a bit, to do more ecology and evolution. I applied for
one particular program somewhere in France, that had all of the elements I
wanted: broad overview of the field, good reputation, possibility to pick
from a lot of classes.&lt;/p&gt;

&lt;p&gt;I made the pre-selection, and somewhere in July, I called the university to
have the final decision. It was no, without more details. To have an idea
of what was wrong, I called the head of the program, and asked him. I head
him shuffle through a few papers, and then he told me&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Well, let’s not fool yourself. With your academic record, the best you
could hope for is to end up as a technician in some lab. You’ll never be a
researcher, and we don’t have time to waste nurturing your delusions. Goodbye.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;He hang up. This is an almost verbatim transcription, because trust me, the
memory is still with me these days. These were the single most important,
transformative, defining 12 seconds of my academic life.&lt;/p&gt;

&lt;h1 id=&quot;how-i-reacted&quot;&gt;How I reacted&lt;/h1&gt;

&lt;p&gt;I decided to quit academia, and become a technician in a lab. Private sector,
because I had a few contacts, and mostly because if working at a university
means risking working with someone like him, then there was no chance I
would work at a university.&lt;/p&gt;

&lt;h1 id=&quot;what-should-have-been-done&quot;&gt;What should have been done&lt;/h1&gt;

&lt;p&gt;Nothing? Nothing. He was (he still is) a full professor in a big lab, with
a good reputation, and &lt;em&gt;prestige&lt;/em&gt;. I was (I’m not anymore) an undergraduate
student. These things just happen, and (from the few observations I’ve had),
even when they happen, things turn out well for the people with the most
established reputation. This is the logic by which a PhD advisor can tell
his student&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I don’t care about your financial issues, you should work harder to be as
productive as I tell you to be.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;When the student talks the situation out with another professor, he hears&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Well, try to see things from his perspective, he cares about the reputation
of his group; are you sure you really can’t work harder?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is the logic by which an advisor can take his foreign PhD student’s
results, give them to its postdoc to publish without acknowledging the
student contribution, then “forgot” to write the letter so that the student
can renew his visa, and has to go back to its home country without being able
to either stand his case or even defend his thesis. Which was summarized by
the advisor in question as&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;an unfortunate series of mishaps and misunderstandings&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The advisor is still working, the student defended and quit research.&lt;/p&gt;

&lt;p&gt;These things happened; I singled them out not because they are particularly
ugly (they are), but because I knew these situations well enough to understand
the dynamics that caused them.&lt;/p&gt;

&lt;p&gt;These things just happen, and will keep on happening as long as people
encourage students to go with advisors that have &lt;em&gt;a good reputation, but a
few quirks, you should not pay too much attention to them because his name
will help you in the long run&lt;/em&gt;. These things will keep on happening as long
as some people think that being something else than a professor is shameful,
or is for people of lower intellectual status than they are. This thing will
keep on happening as long as some have the idea that first-author publications
and academic prestige by proxy give you a blank check for any behavior. These
things will probably keep on happening forever.&lt;/p&gt;

&lt;p&gt;There is nothing to be done in these situations, because they happen when
people have understood so well that they are in a position of consequence-free
power, that they can behave however they please. These situations happen
when people with power, reputation, or prestige, see people without any of
these attributes as expendables, and don’t feel particularly pressured to
care for their needs, aspirations, or well-being. Whenever a system allows
(and rewards, or at least fails to punish, which is equally bad) people
to us &lt;em&gt;people&lt;/em&gt; as tools for their own motives, then these situations will
arise. There is nothing motivational, no pedagogic objective behind them. They
are the ugly manifestation of a system in which you won’t face any punishment
for being exploitative, demeaning, or disrespectful, as long as you’re on top.&lt;/p&gt;

&lt;h1 id=&quot;what-has-been-done-in-practice&quot;&gt;What has been done in practice&lt;/h1&gt;

&lt;p&gt;One lecturer from my BSc emailed me asking for news a month or so later, and
which MSc program I would be in next year. I told her none, and explained why.&lt;/p&gt;

&lt;p&gt;Then she emailed me back. And like that, I saw the worse, and the best of
the academic world. I saw the type of person I wanted to be, and the type of
person I want everyone to avoid. She told me &lt;em&gt;Here’s my office number. Call
me. Now.&lt;/em&gt;. I did.&lt;/p&gt;

&lt;p&gt;We had a long conversation. And it’s funny, in a very sad way, that I
barely remember any details of this conversation. I remember a feeling of
being overwhelmed with so many strong and conflicting emotions, but none of
the details. But I remember the message. She told me that she had no idea
whether I would end up doing research, but she’s observed me enough to know
that I have the drive and the curiosity to at least try. She told me that
she didn’t believe there was anyone that did not deserve a chance to try,
but even if there were, that would not be me. She told me she forwarded my
academic transcript to another masters program, that I have invested time
and energy in my training, that she and her colleagues did too, and that I
owed it to myself, and to them, to give it a shot. She told me that I had
to day to get my ass into the admission office, and sign my enrollment form
before she dragged me there herself.&lt;/p&gt;

&lt;p&gt;She gave me a chance. I took it. That was seven years ago, and I’ve done
some research since. Some of it might even be good, and (it’s the part I
like the most about this story), guess who cited some of it? I’m sure he
doesn’t remember the story. I don’t care. I do remember it. I’ll make my
best not to forget it. I’ll make my best to be the one who gives chances
and believes in people, because that’s why I’m still here.&lt;/p&gt;

</content>
  </entry>
  
  <entry>
     <title>Busy is no myth</title>
     <link href="http://timotheepoisot.fr/2014/09/30/busy-no-myth/"/>
    <updated>2014-09-30T00:00:00+13:00</updated>
    <content type="html">&lt;p&gt;I have seen a few blog posts recently about &lt;em&gt;the myth of busy&lt;/em&gt; in PhD students
and other academics. In short, it seems that everyone is incredibly busy,
all the time, and that how much time you &lt;em&gt;don’t&lt;/em&gt; have is a proxy of your
academic credentials. I disagree.&lt;/p&gt;

&lt;p&gt;Not with the premise – we are busy. All of us. We have plenty of things
to do, and plenty of pressure to do them fast, so that we can do some more
things fast and show our leadership and competitiveness and all of these
things that look good on your grant applications. That, I agree with.&lt;/p&gt;

&lt;p&gt;What I disagree with is that being busy is a choice. I was struck by how
incredibly condescending it was to tell graduate students &lt;em&gt;make the choice to
stop being so busy&lt;/em&gt;. But on the other hand, I understand some of the reasons.&lt;/p&gt;

&lt;p&gt;When I started my masters, I was amused by undergraduates running around
being overworked; they were, after all, far less busy than I was. When I was
a PhD candidate, I always thought that masters students where complaining
too much about how much they had to do. At some points during my post-doc,
I was kindly looking back to my PhD years – how less busy I would be if I
was expected to produce three papers in four years! Last year, I was talking
to a far more senior colleague, and told him that I have been keeping busy
lately; he told me &lt;em&gt;Tim, you have no idea what busy feels like&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;But taking a step back, no one seems particularly more overworked. First
years BSc students seems to chasing after deadlines as often and with as
much despair as I do. So I wanted to come up with an explanation. Here it
is: the raw volume of things we have to do increases over time; so does our
productivity, but with a delay. We are essentially in a Red-Queen dynamics
with ourselves: more work to do means that we have to develop a new coping
strategy, in the form of more productive habits. Then when we feel comfortable,
we take on more work, and become overworked again.&lt;/p&gt;

&lt;p&gt;Busy is no myth, busy is a statistical effect. It’s the difference between our
workload and our productivity, and we implement time-saving measures &lt;em&gt;because&lt;/em&gt;
we need them &lt;em&gt;because&lt;/em&gt; we have too much work to do at the moment. If you
have very little to do, but waste a lot of time, you are as busy as someone
doing a lot but with a good productivity.&lt;/p&gt;

&lt;p&gt;And asking of graduate students to stop looking so damn busy is futile;
what we need is share out time-effectiveness habits, early. If they seem
like overkill, good! It means that they are buying us some time. I wish good
working habits have been touched upon &lt;em&gt;more&lt;/em&gt; during my training. And unless
we agree to do it, and show students how to organise, we have no right to
dismiss their business as a myth.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>How to write a paper</title>
     <link href="http://timotheepoisot.fr/2014/09/25/writing-paper/"/>
    <updated>2014-09-25T00:00:00+12:00</updated>
    <content type="html">&lt;p&gt;I must make a confession. Through no one’s fault bu my own, I made it through
my PhD without a writing method. Not that I knew how to write, and did not
have a routine. I had no method. When it was time to write a paper, I opened a
text editor and started writing a paper. It now sounds head-bashingly stupid;
it is. It is a wonder that I managed to get any writing done. I started
developping an idea of how to write during my post-doc. Mostly by osmosis,
and in part because I needed to do more in less time. And before you get
the wrong idea, this is not about &lt;em&gt;what&lt;/em&gt; to write, or whether or not your
paper will be accepted faster. This is not a perfect approach, and different
people probably do need different ways to tackle a paper. But it works for
me, and it is easy to do.&lt;/p&gt;

&lt;p&gt;Whenever I have a chance, I tell a variation of the following to students:
“You don’t start a paper by opening a text editor, you don’t start a
presentation by making the title slide, and you don’t start a program by
opening a code editor”. These are the &lt;em&gt;last&lt;/em&gt; steps. What comes before,
is the thinking and the planning. Writing the paper is the &lt;em&gt;boring&lt;/em&gt; part,
because (in my opinion), it is the &lt;em&gt;obvious&lt;/em&gt; part. When I follow my usual
writing method, I have no surprises when writing the paper. So here it goes,
the three-step approach to eternal success and good luck in your academic
writing (&lt;em&gt;patent pending&lt;/em&gt;): start with an outline, tell a story, go top-down.&lt;/p&gt;

&lt;h1 id=&quot;start-with-an-outline&quot;&gt;Start with an outline&lt;/h1&gt;

&lt;p&gt;Your are probably disappointed at this point. Start with an outline, is that
the big advice? Oh boy. Start with an outline is probably the most often given
advice when writing a paper, and for a reason; unless you are writing a very
narrative piece, it works. But I really do mean &lt;em&gt;start&lt;/em&gt; with an outline,
that is, before or as long as you are doing the things that will go in the
paper. I’ve had a few brainstorming session when we thought about what the
figures would look like, what type of data we would get, and how different
mechanisms would give different results. In 90% of the cases, the final
figures where different, but at least we had an idea of where we were headed.&lt;/p&gt;

&lt;p&gt;So the first outline is not necessarily a list of bullet points. It is a
loose mix of drawings, keywords, references, and the relationships between
them. It’s more of a roadmap than an outline, but it serves (for me) one really
important purpose: how different results (represented by figures) fall into
the global narrative of the paper. Because, you see, I like it when papers…&lt;/p&gt;

&lt;h1 id=&quot;tell-a-story&quot;&gt;… tell a story&lt;/h1&gt;

&lt;p&gt;And by a story, I don’t mean the particular story of how I came from the
idea to the first figures to subsequent fine-scale analysis to the grand
conclusion. This is only marginally interesting to me, and it is certainly
not interesting to people reading the paper. And so most of the times, the
results or idea are introduced in the paper in a way that does not reflect
the chronology of how the research happened. More than once, things that
emerged later in the analysis where re-pitched into being the main focus of
the paper, and so we ended up telling a different story.&lt;/p&gt;

&lt;p&gt;What I mean is that a paper needs to have a plot: expose the initial situation
(&lt;em&gt;We have this theory about A, and it’s working well at predicting this and
this, and here are the core mechanisms&lt;/em&gt;), then introduce a perturbation
(&lt;em&gt;But you see, there is dataset D, and it contradicts the predictions
made under the relevant theory&lt;/em&gt;). This is what I write first, and this is
usually the introduction. Then, write the dénouement (&lt;em&gt;In fact, mechanisms
B and C interact in this weird way under particular circumustances, so you
need to account that.&lt;/em&gt;). This is the plot (also the first paragraph of the
discussion). What I do next is flesh out the story, that is all the little
steps that go from the initial question to the conclusion. And for that,
I usually…&lt;/p&gt;

&lt;h1 id=&quot;go-top-down&quot;&gt;… go top-down&lt;/h1&gt;

&lt;p&gt;Let’s get back to the outline. At this point, the context is defined, the
question in known, and because I have usually preliminary results, I can write
the conclusion. What I do next is writing the outline, in a top-down way. I
start with the big sub-divisions of the paper (introduction, model/methods,
results, discussion). Then below each (except model/methods), I list the
main ideas of each paragraph, in a very explicit way. I try to keep that
within one, active voice, declarative sentence (&lt;em&gt;Coexistence happens under
source-sink dynamics at intermediate dispersal levels&lt;/em&gt;). This is the summary
of each paragraph. When this is done for all main headings (and I usually
spend a bit of time tweaking it about), the whole outline should read like
a coherent paragraph. This is when I break out my red pen, and under each
single sentence, I list (i) the 2 or 3 key references, and (ii) the figures
or tables that will be refered to. In most cases, I will also break each
paragraph into three sub-points, especially if it is a key paragraph.&lt;/p&gt;

&lt;p&gt;I think that this method has important advantages. First, because I write the
actualy outline late (I start with a conceptual idea of the paper, then go
through producing the results, then start writing the outline), it’s easy to
base it on whatever material I already have.  Second, because paragraphs tend
to be of approximately equal size, I can see whether one or more parts are
bloated, and move things around (also good to estimate word count). Third,
it is easier to get an idea of the overal flaw of the paper. If one of the
paragraphs reads &lt;em&gt;Ponder on semi-related litterature from sub-field you
enjoy despite never having worked in it&lt;/em&gt; (one of my biases), then I can
immediately see that it does not fit within the macro-level structure of
the paper. Cut. Finally, it gives constraints when writing. For this is the
final step:&lt;/p&gt;

&lt;h1 id=&quot;writing&quot;&gt;Writing&lt;/h1&gt;

&lt;p&gt;Once I have the outline, that I have made sure that the story flows and
there are no gaps in the story, and that the paragraphs are balanced, it’s
time to start writing.&lt;/p&gt;

&lt;p&gt;This is as good a place as any to mention that all of the above takes place
in a notebook (currently a hardcover plain white Moleskine, though I’m a
bit disappointed by the shoddy quality of the paper, especially since they
are so expensive). Notebooks are good, because you can carry them anywhere,
they don’t run out of power, and they boot faster than a laptop. See? This
paragraph cuts the flow. I would have removed it from the outline.&lt;/p&gt;

&lt;p&gt;Anyways. When it’s time to start writing – on a computer, I can start
wherever I want. If I’m in the mood to review some conceptual papers, I
can write parts of the introduction. Or the results. I just need to look
at the summary of the paragraph to know what to talk about. Then I look at
the paragraph before and after, to know how my paragraph shoul begin and
end. Everything in the paragraph needs to help the “story” move forward.&lt;/p&gt;

&lt;p&gt;The other advantage (and it has became a selling point over the last few
months, where I have mostly been able to work on very short chunks), is that
I don’t have to think too much about what to write. The job is already broken
down. So I don’t say &lt;em&gt;I have to write that paper&lt;/em&gt;. I say, &lt;em&gt;I have to write 300
words about how this statistical bias affects the computational efficiency
of null models&lt;/em&gt;. This is much less daunting. Writing 300 words is nothing,
especially when you already know the big story.&lt;/p&gt;

&lt;h1 id=&quot;parting-words&quot;&gt;Parting words&lt;/h1&gt;

&lt;p&gt;So here is my method – if you want to make your life easier when writing a
paper, try to delay the actual writing until the last moment (advisors of
the world, sorry!). If all of the planning and all of the thinking ahead
has been done, then the actual writing is a matter of connecting the dots.&lt;/p&gt;

&lt;p&gt;I have also noticed that it made me writer shorter, thighter papers. Probably
because it is easier (for me) to spend one or two additional paragraphs
rambling about things, which I can’t do if I stick to my outline (this method
is &lt;em&gt;entirely&lt;/em&gt; about me trying to cope with my terrible habits). If found
that it worked surprisingly well for grant applications, too! The sections
are already given, and (in one recent case) there are size limits for each
sub-header: I spent a lot of time working on which paragraphs to write whithin
each to meet the space constraints, and it made writing considerably easier
than under my previous strategy (write 15000 words and cut).&lt;/p&gt;

&lt;p&gt;I’m not pretending that this method will work for you – or that I follow
it strictly at all times. But overall, it made me more productive in my
writing. If the outline is good, and the story is thight, there are much
less chances than a massive re-writing would be needed. And although I don’t
have any other evidences than anecdotal ones, I suspect that it makes things
easier to read and follow for the readers.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>Tales from the job search I - The French Connection</title>
     <link href="http://timotheepoisot.fr/2014/09/24/tales-job-search-I/"/>
    <updated>2014-09-24T00:00:00+12:00</updated>
    <content type="html">&lt;p&gt;On January 5, I will start a position as an assistant professor at the
&lt;a href=&quot;http://en.bio.umontreal.ca/home/&quot;&gt;Department of Biological Sciences, Université de Montréal&lt;/a&gt;. So spoiler
alert, this story has a happy ending. Looking for a job has been an incredibly
formative experience, and I wanted to share some of it with you.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This is part 1 out of 4 of a series of posts in which I summarize my
year and a half spent looking for, and ultimately getting, an academic
position. I will try to publish one every two weeks. It is mostly intended
as an excuse to look back at what happened, and hopefully hear about your
experiences too. Simon Goring explained the &lt;a href=&quot;http://downwithtime.wordpress.com/2014/04/08/the-job-hunt-in-france/&quot;&gt;application  process&lt;/a&gt;
and his &lt;a href=&quot;http://downwithtime.wordpress.com/2014/04/08/the-job-search-in-france-part-ii/&quot;&gt;overall feeling&lt;/a&gt; in a way that matches what I’m about to
say, and you should read his perspective too; you will  see that we had the
same feeling about the interview, in particular.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;When I started my first post-doc in November 2011, I decided to take some
time to enjoy doing research, without thinking about much else (besides
finishing my PhD projects). After a year and a half, I decided that it was
time to look for a position somewhere. For family and historical reasons,
I first decided to try the French CRNS (it’s short for National Center for
Scientific Research). CNRS positions have great upsides (lifetime position,
no teaching, no administrative work unless you absolutely want to), and
great downsides (low pay, almost no recognition, funding concentrated on a
minority of people, limited opportunities to train students). But most of the
CNRS staff scientists I worked with seemed happy enough, and a pure-research
position was appealing to me (at this time).&lt;/p&gt;

&lt;p&gt;The CNRS is a public organism, and so researchers are civil servants. This
means that the application and hiring process is relatively strange, and
possibly unique. There is a yearly call for applications, with a fixed
number of positions for each of the 52-or-so thematic sections. Lucky me,
I had a general enough background to apply to 29 (Ecology and evolution),
30 (Ecosystems), and 51 (Multi-disciplinary, mathematics and informatics
for life sciences and physics). Some of these positions are “coloured”
(&lt;em&gt;A candidate with knowledge of information theory to work on connectomes
in a human health group&lt;/em&gt;), but the others (that would be between one and
four for each section) are open.&lt;/p&gt;

&lt;h2 id=&quot;the-application&quot;&gt;The application&lt;/h2&gt;

&lt;p&gt;To apply, you need a research program. This is approximately all of the
information you are given (&lt;em&gt;Describe your research program&lt;/em&gt;), and so you won’t
make it without insider knowledge. I sent a couple emails to newly hired CNRS
researchers, and received four entirely different documents. I decided to do
it my way, and wrote a 10-pages, 100-references, 5-years research program
with short terms projects and long terms objectives. I later got feedback
on it from jury or jury-related people: &lt;em&gt;It showed great maturity&lt;/em&gt;; &lt;em&gt;It did
not seem really mature&lt;/em&gt;; &lt;em&gt;It was a bit fuzzy&lt;/em&gt;; &lt;em&gt;It was too specific&lt;/em&gt;. I have
a feeling that the candidates are not the only ones not to have guidelines…&lt;/p&gt;

&lt;p&gt;The next thing you need is a summary of your research, with an indication of
what was your role in each contribution (paper), and what was the impact of
each contribution. Canadian grant applications ask that for your top 3 or top
5 contributions (or group of contributions). For the CNRS, you have to do it
for all your papers. I had around 20 when I applied. Once again faced with no
guidance, I put the publications in chronological order, and wrote a 300 to
500 words blurb addressing my role and the impact it had on the field. Did I
got informal feedback on it after the decisions? Yes! &lt;em&gt;You have published on
very different topics, it shows you can adapt to new situations&lt;/em&gt;; &lt;em&gt;You have
published on very different topics, it shows you don’t know what you are
doing&lt;/em&gt;. More? Yes!! &lt;em&gt;You have published in high-impact journals, it means
you’re good&lt;/em&gt;; &lt;em&gt;You have published in high-impact journals more than young
candidates usually do, it means somebody probably wrote the papers for you&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;With these two documents written, I submitted them to the three different
sections I applied to, and waited. A few months later, I had the decisions: I
was invited for an interview in the Ecology &amp;amp; evolution and Multi-disciplinary
section. So far, so good. The interviews were a month and a half apart. In
France. During spring time. I had to pay for one of the tickets from Canada
myself, and the lab I was applying to paid for the second plane tickets. I
have family in France, so my expenses were remarkably low, but the whole
ordeal cost me 1500 CAD. There are a lot of discussions in France about the
lack of interest from foreign scientists, or from French post-docs currently
abroad to return; this is why.&lt;/p&gt;

&lt;h2 id=&quot;the-interview&quot;&gt;The interview&lt;/h2&gt;

&lt;p&gt;As &lt;a href=&quot;http://downwithtime.wordpress.com/2014/04/08/the-job-search-in-france-part-ii/&quot;&gt;Simon said&lt;/a&gt;, the interview is best summarized as
&lt;em&gt;anticlimactic&lt;/em&gt;. Depending on the sections, you talk for 12 to 15 minutes,
and have 10 to 15 minutes of questions. My interview in the Ecology &amp;amp;
evolution section went “OK enough, I guess”. I didn’t made it into the
short-list. The interview with the Multi-disciplinary section went really
well. I made it into the short-list (ranked 6, 4 positions), didn’t got the
position in the end. The big difference between the two was how receptive
the jury was to the methodological side of things. To the surprise of
absolutely no one, my proposal was strongly leaning towards data analysis,
computational science, and applications of graph theory. The math-inclined
people in the Multi-disciplinary section seemed to enjoy it, and we had good
(and very technical) discussions about some particularities of algorithmic
and computational issues. The jury in the Ecology &amp;amp; evolution seemed to
block on the methodological side of things.&lt;/p&gt;

&lt;p&gt;As an aside, this has been a recurring problem for me. Network-related things
seems off-putting to a lot of people, and I have read more than once in reviews
that the very precise and well-defined terminology of network analysis was
“jargon”. I know that the onus is on me to translate the vocabulary and do
a lot of pedagogy, but I suspect that there is a - small - core of people
that want nothing to do with networks.&lt;/p&gt;

&lt;h2 id=&quot;the-after&quot;&gt;The after&lt;/h2&gt;

&lt;p&gt;I received the decisions around June. Even though I am fairly critical of how
hard it is to navigate the whole process, and how inconsistent the feedback
I received was, I am quite happy about the experience. It’s hard not to think
that there is a lot of place left to personal feelings of jury members instead
of having well-defined items for evaluation; this is understandable if you
are going to interact with these people on a daily basis, but if you are only
going to meet them during the interview, this is less understandable. Anyway,
this is symptomatic of how France works as a country (or of how us French work
as people); it has advantages (you can adjust the rules to your feelings),
and inconveniences (you can adjust the rules to your feelings), and in any case
there will be numerous and loudly explained complaints with much arms waving.&lt;/p&gt;

&lt;p&gt;What did I get out of the experience? First, a good research program. It
changed over time, I think for the better, but it was the first time I had
to put the last 4 years of my research in perspective in an integrated,
meaningful ways. I also thought in depth about what type of projects I
wanted to do in five or ten years, and that resulted in re-directing current
projects, and starting new ones, and merging one or three that felt out of
touch with the new insights I gained on what I wanted to do. It also gave
me a full year to think about the type of position I wanted. After spending
the previous winter teaching, I decided that a full-time research position
would be good, but a more North-American professorship would be better. A
few months of self-introspection later, I started writing up a teaching and
research statement for future job searches. And so began the second chapter
of my tales, &lt;em&gt;The Crossing of the Desert&lt;/em&gt;.&lt;/p&gt;

</content>
  </entry>
  
  <entry>
     <title>The use of Twitter at ESA and IMCC</title>
     <link href="http://timotheepoisot.fr/2014/08/17/twitter-esa-imcc/"/>
    <updated>2014-08-17T00:00:00+12:00</updated>
    <content type="html">&lt;p&gt;August is conference season. But for the last few years, not being able
to attend does not mean that you can’t partake in the global discussion,
as more and more scientific meetings have been using twitter hashtags to
keep the discussion going online. Being both a fervent twitter user, and
interested in all things networks, I decided to explore the assembly of the
graph of interactions among scientists over time. A bunch of us are doing a
more formal analysis of the &lt;em&gt;Ecological Society of America&lt;/em&gt; 2014 meeting,
but here I will present a brief comparison of &lt;code&gt;#ESA2014&lt;/code&gt; with &lt;code&gt;#IMCC3&lt;/code&gt;,
the &lt;em&gt;International Marine Conservation Congress&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Both meetings took place almost at the same time, were massively using their
respective hashtag, and are within the same broad field of environmental
sciences.  I won’t go into the details, but here is what I’ve been doing. I
used the Twitter API to pull all tweets containing either &lt;code&gt;#ESA2014&lt;/code&gt; or
&lt;code&gt;#IMCC3&lt;/code&gt;. For all of these tweets, I looked at who wrote it, and who was
mentioned in it. If a tweet written by &lt;code&gt;@bob&lt;/code&gt; mentionned &lt;code&gt;@alice&lt;/code&gt;, I recorded
one interaction from &lt;code&gt;@bob&lt;/code&gt; to &lt;code&gt;@alice&lt;/code&gt;, along with the time, and the meeting
in which it occured. Because the use of twitter was &lt;em&gt;massive&lt;/em&gt;, this resulted
in a little more than 3500 people, mentionning each other over 20000 times.&lt;/p&gt;

&lt;p&gt;I was mostly interested in how using Twitter can help bring people together
over time, which in a network perspective is akin to measuring modularity,
and how it decreases.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;r
load(&#39;../data/twitter_meetings.Rdata&#39;)
opts_knit$set(base.dir = &#39;..&#39;)
library(plyr)
library(igraph)
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;OK, so first (keeping in mind that &lt;code&gt;#IMCC3&lt;/code&gt; is still going), what does
the dynamics of number of tweets looks like? I divided the dataset in bins
of approximately 10 hours, and looked at the number of tweets, and number
of users.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;r
plot(xseq, laply(t_all, vcount), lwd=2, type=&#39;l&#39;, lty=3, xlab=&#39;Date&#39;, ylab=&#39;Participants&#39;)
lines(xseq, laply(t_esa, vcount), col=&quot;darkgreen&quot;)
lines(xseq, laply(t_imcc, vcount), col=&quot;darkblue&quot;)
legend(&#39;topleft&#39;, legend=c(&#39;All&#39;, &#39;#ESA2014&#39;, &#39;#IMCC3&#39;), lty=c(2, 1, 1),
       col=c(&#39;black&#39;, &#39;darkgreen&#39;, &#39;darkblue&#39;), lwd=c(2,1,1), bty=&#39;n&#39;)
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/figure/tweet_vcount.png&quot; alt=&quot;Looking at the number of people using twitter in bins of ten hours gives an idea of when, during the meeting, the people are more active.&quot; /&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;r
plot(xseq, laply(t_all, ecount), lwd=2, type=&#39;l&#39;, lty=3, xlab=&#39;Date&#39;, ylab=&#39;Tweets&#39;)
lines(xseq, laply(t_esa, ecount), col=&quot;darkgreen&quot;)
lines(xseq, laply(t_imcc, ecount), col=&quot;darkblue&quot;)
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/figure/tweet_ecount.png&quot; alt=&quot;The same goes for the number of unique tweets. At `#esa2014`, most of the action was during the second and third days.&quot; /&gt; &lt;/p&gt;

&lt;p&gt;Interestingly, the use of hashtags allows the conversation to start &lt;em&gt;before&lt;/em&gt;
the actual meeting does. And, oh yeah, shame on us ecologists, the IMCC
crowd is way better at twitter than we are! With roughly the same number
of people involved, they manage to talk twice as much as we do. Actually,
let’s look at the cumulative number of tweets exchanged:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;r
plot(xseq, laply(a_all, ecount), lwd=2, type=&#39;l&#39;, lty=3, xlab=&#39;Date&#39;, ylab=&#39;Cumulative tweets&#39;)
lines(xseq, laply(a_esa, ecount), col=&quot;darkgreen&quot;)
lines(xseq, laply(a_imcc, ecount), col=&quot;darkblue&quot;)
legend(&#39;topleft&#39;, legend=c(&#39;All&#39;, &#39;#ESA2014&#39;, &#39;#IMCC3&#39;), lty=c(2, 1, 1),
       col=c(&#39;black&#39;, &#39;darkgreen&#39;, &#39;darkblue&#39;), lwd=c(2,1,1), bty=&#39;n&#39;)
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/figure/tweet_vcount_cum.png&quot; alt=&quot;Cumulative number of participants in both meetings. `#IMCC3` is taking off extremely fast, compared to the more modest growth of `#esa2014`.&quot; /&gt; &lt;/p&gt;

&lt;p&gt;Whereas the number of &lt;code&gt;#ESA2014&lt;/code&gt; tweets ends-up stagnating, the &lt;code&gt;#IMCC3&lt;/code&gt;
conversation keeps on going. If I had to propose an explanation, I think
it would be that marine sciences and conservation are really to get people
interested in (especially since it was &lt;em&gt;Shark Week&lt;/em&gt; recently). Ecology &lt;em&gt;should&lt;/em&gt;
be the same, but we have a long way to go.&lt;/p&gt;

&lt;p&gt;So what about modularity? I used the walktrap method, which is based on
short (of size 3) random walks, to determine the membership of people to
communities, and then I measured the overall modularity. This goes from 0
(everyone is connected to everyone else) to 1 (groups of well connected
people are not connected to one another).&lt;/p&gt;

&lt;p&gt;&lt;code&gt;r
mod &amp;lt;- function(x) modularity(walktrap.community(x))
plot(xseq, laply(a_all, mod), lwd=2, type=&#39;l&#39;, lty=3, xlab=&#39;Date&#39;, ylab=&#39;Modularity&#39;, ylim=c(0,1))
lines(xseq, laply(a_esa, mod), col=&quot;darkgreen&quot;)
lines(xseq, laply(a_imcc, mod), col=&quot;darkblue&quot;)
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/figure/tweet_modularity.png&quot; alt=&quot;In both meetings, modularity decreases over time. Note that the `#imcc3` meeting ws *less* modular even at the beginning of the analysis.&quot; /&gt; &lt;/p&gt;

&lt;p&gt;I like this figure! The &lt;code&gt;#ESA2014&lt;/code&gt; meeting is not that modular by the end
(a modularity around 0.4 indicates that some clusters exists, but they
are overall well-connected). On the other hand, &lt;code&gt;#IMCC3&lt;/code&gt; is really well
connected. Keeping in mind that the number of contributors is similar, this
means that the &lt;code&gt;#IMCC3&lt;/code&gt; community is much more uniform than the &lt;code&gt;#ESA2014&lt;/code&gt;
community is. This can be because the span of &lt;code&gt;#IMCC3&lt;/code&gt; is more narrow (whereas
&lt;code&gt;#ESA2014&lt;/code&gt; is about &lt;em&gt;all&lt;/em&gt; ecology), or this can be that the sense of belonging
to a community is more developed in marine conservation people.&lt;/p&gt;

&lt;p&gt;Let’s now look at the diameter (the maximum distance between two nodes)
in the two graphs:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;r
plot(xseq, laply(a_all, diameter), lwd=2, type=&#39;l&#39;, lty=3, xlab=&#39;Date&#39;, ylab=&#39;Diameter&#39;, ylim=c(0,20))
lines(xseq, laply(a_esa, diameter), col=&quot;darkgreen&quot;)
lines(xseq, laply(a_imcc, diameter), col=&quot;darkblue&quot;)
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/figure/tweet_diameter.png&quot; alt=&quot;Diameter (maximal distance between two contributors) in both conferences. Keeping in mind that `#IMCC3` has more contributors, the fact that it has a lower diameter means that the interactions between them are indeed closer.&quot; /&gt; &lt;/p&gt;

&lt;p&gt;This tells the same story: with a diameter of 10, against 7 for &lt;code&gt;#IMCC3&lt;/code&gt;,
&lt;code&gt;#ESA2014&lt;/code&gt; is a more disjointed community. As a final note, I will release
both datasets when &lt;code&gt;#IMCC3&lt;/code&gt; tweets stop accumulating.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>The challenge of relatable theoretical ecology</title>
     <link href="http://timotheepoisot.fr/2014/08/13/making-theory-relatable/"/>
    <updated>2014-08-13T00:00:00+12:00</updated>
    <content type="html">&lt;p&gt;Last November, I was attending the &lt;em&gt;International Biogeogaphic Society&lt;/em&gt;
special meeting on species interactions. The first keynote speaker, &lt;a href=&quot;http://nature.berkeley.edu/~gillespie/Home.html&quot;&gt;Rosemary
Gillespie&lt;/a&gt;, mentioned that one of the most important piece of advice she
received was to find a model system, understand it as much as you can, and use
it as a basis against which other systems can be compared. Her and her group
are doing some stunning things on the biogeography of spiders on the Hawaii
archipelago, so it’s hard to argue against this approach. This little piece
of advice was picked up by a majority speakers during the meeting, and they
were all very passionate about the species or field sites they were studying.&lt;/p&gt;

&lt;p&gt;On the other hand, I’m essentially system-agnostic. So I decided to make
a little joke during my talk, and showed a picture of my own model system
(the laptop) at the margin of its habitat (a two minutes walk from the nearest
power outlet) – this particular species has since been displaced by a fitter
invader. It’s true that there are some systems I know more than others. I’m
biased towards on microbial interactions, especially involving bacteria and
phages. I’m partial to parasites, especially the ones you find in fishes or
rodents. But I really don’t care if I have to work on something else.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;left&quot;&gt;
&lt;img src=&quot;/images/laptop.jpg&quot; alt=&quot;&quot; /&gt;
   &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;My point of view is: find the most suitable (which to say, the less
unsuitable) dataset for the question you have in mind at the moment, and
work to understand its particular features that makes things general or
not. Is it pollinators? Good. Is it from literature surveys? That’s good
too. In fact, as long as it matches your question, anything goes. Which is
a central point of my “philosophy” as a data &lt;em&gt;consumer&lt;/em&gt;: there is no good,
and no bad, dataset. There are datasets matching the question you have in
mind, and there are all of the other datasets in the world. I have some
projects on the back-burner because I’m still waiting for the good dataset
to use to finish them (note that I’m not waiting for the &lt;em&gt;perfect&lt;/em&gt; dataset,
there is almost never such a thing).&lt;/p&gt;

&lt;p&gt;One of the dynamics of scientific collaboration I enjoy the most is when people
with a good knowledge of the natural history and people with more conceptual
interests team up, and both sides gain improved knowledge. Theoretical
(a catch-all term for numerical, computational, conceptual) science cannot
exist in a vacuum for a long time, which makes it necessary to work closely
with people knowing their system very well. In the preface to a special
issue of &lt;em&gt;Interface Focus&lt;/em&gt; on theoretical ecology, @lev12a gives a really
convincing historical argument for more interactions between theory and data,
and at the same time has this impeccable criticism of purely mathematical
approaches to ecological questions:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The legacy of Volterra and Lotka has not been universally positive,
  although this is certainly not their fault. The attractive simplicity of
  the model equations proved irresistible to mathematicians eager to add
  bells and whistles, with little concern for biological relevance, and to
  explore their tortured implications in painful detail. &lt;strong&gt;This has produced
  a large literature, harmless except for its effect on perceptions of the
  field of mathematical biology, and its obfuscation of the cryptic nuggets
  that sometimes lie within&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;A while ago, we submitted a purely theoretical paper (it has been accepted
since I started writing this post), which one may be tempted  to file into
the &lt;em&gt;adding bells and whistles&lt;/em&gt; category. This paper is, for the largest part,
an analytical treatment of a modified version of host-symbiont dynamic models,
with some simulations by the end. It was broad enough that we used a general
language to describe the type of organisms we were modelling. We didn’t just
called them &lt;em&gt;organisms&lt;/em&gt;, but that was close enough; that’s how general that
paper is, and it certainly does not apply to a particular ecosystem. One of
the referees reported that it was interesting and sound, but he couldn’t
recommend acceptation, because it did not bring any new insight about any
specific system in nature. It is always a harsh thing to read about your work
as a theoretician (seeing your work dismissed as, essentially, &lt;em&gt;unreal&lt;/em&gt; –
an interesting but futile exercise). Luckily, the associate editor replied
with the following comment:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;This is always one of those complicated issues for models - true biological
  reality means it isn’t a model, it is nature. But it is also true that
  for a model to make sense to empiricists it has to relate to something
  they understand.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;These are good words. What I enjoy about this comment is that it highlights
both sides of the responsibility to close the gap between empirical and
theoretical sciences. On one hand, we have to accept that all of the details
or our favorite empirical system are unlikely to be captured by a model,
and that this is not (necessarily) a reason not to consider the model at
all. On the other hand, as Levin pointed out, us theoreticians have to make
modelling relatable, and I’ll talk a bit more about that.&lt;/p&gt;

&lt;p&gt;I think there are two ways to make mathematically-grounded theoretical
work relatable to non theoreticians. The first is trying to reproduce an
empirical system in all of its details, so as to be able to predict a lot
about little. The second way (which is close to what I do) is to use models
that are general enough to capture a broad range of phenomenons, and explain
(in the text) where they do and don’t apply to ecological systems. Both sides
of the discussion have been covered by @eva13 (and surprisingly they call
for more focused, small-scale models, because the high-order ones make so
many assumptions that they end up being irrelevant).&lt;/p&gt;

&lt;p&gt;One of the issues I have with pure-theory journals (which I think is why
I read them, review papers for some, but don’t intend to publish there any
time soon), is that they tend to favor papers in which a lot of emphasis is
put on mathematical prowess, and very little on making the paper relatable
to a broad audience. Not that this is a bad thing &lt;em&gt;per se&lt;/em&gt;, but perhaps it
explains the citation gap between empirical and theoretical works [@faw12a]:
this indicates a tendency to discard papers with equations as being primarily
mathematics, even though they address biological questions – I wonder if
the citation gap is as strong in ecology, which has a rich tradition of
quantitative research. If there is an echo chamber of theoretical papers
(I’m not assuming there is, or that it would be a bad thing assuming we can
have some synthesis once in a while), in which making the paper relatable
to empiricists is not a priority, then it should not be a surprise that
empiricists do not relate to the paper (and therefore do not cite it).&lt;/p&gt;

&lt;p&gt;An interesting point to keep in mind is that (most) theoretical papers
require empirical science as an input. So most of the efforts needed to
close the loop would be to make sure that more theoretical science goes into
empirical one. Which takes (I think) three different ingredients. First,
at the system scale, increasing quantitative skills [@bar13b] so that no
one is &lt;em&gt;afraid&lt;/em&gt; of equations anymore. Second, putting an end to the idea
that results not obtained through empirical approaches are untrue (they are
most likely quantitatively wrong but qualitatively insightful). And finally,
an increase in the efforts made by theoreticians to &lt;em&gt;reach out&lt;/em&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;My own perspective on the way forward is to increase collaborations. I am not
comfortable with the ideas of &lt;em&gt;theoretical ecology&lt;/em&gt; sessions in conferences,
because it is essentially a statement that while other fields are defined
by their questions (&lt;em&gt;community&lt;/em&gt;, &lt;em&gt;landscape&lt;/em&gt;, &lt;em&gt;evolutionary&lt;/em&gt;, …), one
sub-field can be summarized only by its approaches. This contributes to
the reproductive barrier (for ideas!) between the sub-fields. Projects
relying heavily on data analysis are always a good opportunity, because the
people having collected the data have some knowledge about them that is not
necessarily apparent in the data themselves. But I have been involved lately
in two really good projects where toy models were used as a supplement to
purely empirical science. This felt really right! My personal preference
is clearly for this type of approaches. None of these papers are going to
break any new ground from a mathematical modelling point of view, but the
addition of models helped greatly in the interpretation of the results
(hence, breaking some new &lt;em&gt;ecological&lt;/em&gt; ground).&lt;/p&gt;

&lt;p&gt;And lastly, let’s not forget that models often do not have the last
word. Charles Krebs recently synthesized &lt;a href=&quot;https://www.zoology.ubc.ca/~krebs/ecological_rants/?p=786&quot;&gt;40 years of research on the
lynx-hare population cycles&lt;/a&gt;, and this is a really entertaining and
informative post to read. The lynx-hare data are often mentioned as an example
of demographic control by predators, or of tri-trophic interactions. And
indeed, several population dynamical models are able to reproduce cyclic
oscillations really well. The interesting point, however, is that in nature, it
all boils down to predator-induced stress, which is inherited through maternal
effects. The models are able to reproduce the data very well, but the models
are wrong. I like this idea of models that help sorting through hypotheses,
and acting as prior information. But if we are not doing mathematical biology
for the sake of mathematics, then empirical confirmation matters.&lt;/p&gt;

&lt;p&gt;It’s fine to have people passionate about a species, a field site, or an
experimental system. It’s fine to have people doing ecological research
from the comfort of their office chair. But the most important is to have a
concerted effort to use as much methods as we can to answer questions. Whether
this involves muddy boots, a few hundred microcosms, or whatever it is
theoreticians scribble in their notebooks… the more the merrier!&lt;/p&gt;

</content>
  </entry>
  
  <entry>
     <title>Estimating SIR model parameters with ABC</title>
     <link href="http://timotheepoisot.fr/2014/07/25/sir-abc/"/>
    <updated>2014-07-25T00:00:00+12:00</updated>
    <content type="html">&lt;p&gt;I am currently working with Approximate Bayesian Computation for a project. As
always when learning a new method, I like to revisit examples i already
know about, to see how it compares. So I decided to look at classical data
about a flu epidemics in an English Boarding School. It’s a good dataset to
estimate SIR parameters, because (1) Influenza transmits in a flu-like way,
(2) boarding schools are a closed system from a demographic point of view, and
(3) there was no mortality event during the epidemics. Also, it is likely that
there was a single individual responsible for the epidemics at the beginning
(but this is something I’ll look into).&lt;/p&gt;

&lt;p&gt;:x
distribution of parameter values when there is no analytical expression of
the likelihood function. Just to give a very short overview: knowing data
(the number of cases every day), I want to estimate the parameters of the
SIR model that represents the epidemics. The SIR model is a compartmental
model, in which individuals start as susceptible, become infectious, then
finally recover. Formally, it is expressed as a set of ordinary differential
equations, $\dot S = -\beta I S$, $\dot I = \beta I S - \nu I$, and $\dot R =
\nu I$. There are two parameters of the model itself (the transmission rate
$\beta$ and recovery rate $\nu$), to which one should add the initial state
$S, I, R = {S_0, I_0, R_0}$.&lt;/p&gt;

&lt;p&gt;There are a few noteworthy things about this model. First, at any time $t$,
$S_t+I_t+R_t=N$, where $N$ is the total population size – this assumes no
demography, but this is a very reasonable assumption for diseases with a
short course and low/no mortality. The other noteworthy things are not really
relevant to what I’m about to do, so I’ll skip them. The important information
is the value of $R_0$, which gives the expected number of infections from a
single infected case in an otherwise entirely susceptible population. In the
SIR model, $R_0 = N\beta/\nu$. The $R_0$ of influenza viruses is usually in
the $[1.3-1.9]$ range. Values of $R_0 &amp;gt; 1$ imply that epidemic progression
will likely happen.&lt;/p&gt;

&lt;p&gt;What I want to do is use ABC to estimate the distribution of $\beta$ and
$\nu$, as well as $I_0$, and thus estimate the $R_0$ of this particular
epidemics. Here is the basic approach. I will first establish the prior
distribution of my three parameters. I will assume that there are no way to
guesstimate values of $\beta$ and $\nu$, so I will sample a uniform $[0;1]$
distribution for $\nu$, and 10 to the power of random numbers in $[-4;
0]$ for $\beta$ (these are reasonable estimates in my experience). The
only thing I know about $I_0$ is that it is most likely 1, so I will use a
Poisson distribution with $\lambda = 1$ and add 1, which will make $I_0 =
1$ and $I_0 = 2$ equally likely.&lt;/p&gt;

&lt;p&gt;With these random parameters, I will simulate a SIR epidemics, and measure
$\rho(I_t,\hat I_t)$, which is to say the distance between the empirical time
series, and the simulated one. If this distance is lower than an arbitrary
treshold $\epsilon$, I will keep the random set of parameters as &lt;em&gt;likely&lt;/em&gt;
– by replicating this process enough times, I should hopefully end up with
a posterior distribution of all three parameters.&lt;/p&gt;

&lt;p&gt;Oh, and also I will do this in &lt;code&gt;julia&lt;/code&gt;. Let’s go!&lt;/p&gt;

&lt;h1 id=&quot;writing-the-code&quot;&gt;Writing the code&lt;/h1&gt;

&lt;p&gt;First, let’s load a bunch of packages needed to do the processing and output:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;using Distances
using Distributions
using DataFrames
using Gadfly
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once this is done, we need a function &lt;code&gt;sir&lt;/code&gt; to run the simulations. I will use
an extremely simple one with Euler integration, because using a sophisticated
numerical integration scheme is &lt;em&gt;not&lt;/em&gt; the point.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;function sir(N, I0, beta, nu, t)
   # SIR model
   # N     - total population size
   # I0    - number of infected cases at t = 0
   # beta  - transmission
   # nu    - recovery
   # t     - number of timesteps
   s = zeros(t)
   i = zeros(t)
   r = zeros(t)
   s[1] = N-I0
   i[1] = I0
   for step in [1:(t-1)]
      ds = -beta*s[step]*i[step]
      di =  beta*s[step]*i[step] - nu*i[step]
      dr =  nu*i[step]
      s[step+1] = s[step] + ds
      i[step+1] = i[step] + di
      r[step+1] = r[step] + dr
   end
   return (s, i, r)
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This function will return a tuple of time series, for the S, I, and R
values. The next step is to define a bunch of arrays, parameters, etc etc.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;number_samples = 10000

beta_prior = 10.^rand(Uniform(-4, -2), number_samples)
nu_prior   =     rand(Uniform(0, 1), number_samples)
i0_prior   =     rand(Poisson(1), number_samples).+1

param_N = 763   # We know the total number of students
param_t = 15    # The data cover 15 days
param_d = 200   # This is my arbitraty treshold, feel free  to experiment

beta_posterior = Float64[]
nu_posterior = Float64[]
i0_posterior = Int64[]
fit_posterior = Float64[]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, we can load the data which are stored in a file called &lt;code&gt;data.csv&lt;/code&gt;,
&lt;a href=&quot;https://gist.github.com/9dd174a411b0ae5f0fa7&quot;&gt;available as a gist here&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;observed_cases = convert(Array{Float64}, readtable(&quot;data.csv&quot;,separator=&#39;;&#39;, header=true)[:I])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And now, we need to loop through all sets of parameters, and measure the
distance. Each iteration of this loop will run a simulation of the SIR model,
then measure the Euclidean distance between the observed and simulated time
series, and finally if this distance is lower than a treshold, consider that
the parameter set can be kept:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;for run in [1:number_samples]
   simulated_timeseries = sir(
         param_N, 
         i0_prior[run],
         beta_prior[run],
         nu_prior[run],
         param_t)
   distance = evaluate(Euclidean(), observed_cases, simulated_timeseries[2])
   if distance &amp;lt;= param_d
      push!(beta_posterior, beta_prior[run])
      push!(nu_posterior, nu_prior[run])
      push!(i0_posterior, i0_prior[run])
      push!(fit_posterior, distance)
   end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To make plotting faster, we will wrap up the results in a &lt;code&gt;DataFrame&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;posteriors = DataFrame(beta = beta_posterior, nu = nu_posterior, i0 = i0_posterior)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By the way, I ran the following figures with 10^6^ prior samples – in under 2 seconds. &lt;/p&gt;

&lt;h1 id=&quot;the-results&quot;&gt;The results&lt;/h1&gt;

&lt;p&gt;Because this dataset is well-known, there are estimates of the parameters
using $I_0 = 1$. They are, respectively, $\beta \approx 2.18\times 10^{-3}$,
and $\nu \approx 4.41\times 10^{-1}$. &lt;strong&gt;So&lt;/strong&gt;, what does the fitting says?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;julia&amp;gt; mean(posteriors[:beta])
   0.0023823647134406517

julia&amp;gt; mean(posteriors[:nu])
   0.5571488709952898

julia&amp;gt; mean(posteriors[:i0])
   2.8266301035953685
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;First, although $\beta$ estimated this way is quite close to the proposed
estimate, the value of $\nu$ differs. Specifically, I predict an easier
remission than the original estimation (the recovery time is $1/\nu$ days,
&lt;em&gt;i.e.&lt;/em&gt; 2.27 in the original model, and 1.7 in my estimate). Why? Simply because
I decided not to trust the assumption that $I_0 = 1$. The original $I_0$
is based on the fact that there was a single student admitted for flu-like
symptoms on day 0, so it can be that by day 0, there were already infected
(but asymptomatic) individuals. My estimation tends to predict that there
were between two and three infected students.&lt;/p&gt;

&lt;p&gt;With this information, we can easily plot the predicted timecourse of the
epidemics:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;pred = sir(763, 3, 0.00238, 0.5571, 15)
draw(
      PNG(&quot;infected.png&quot;, 12cm, 8cm),
      plot(
         layer(x=[0:14], y=pred[2], Geom.point),
         layer(dat, x=&quot;day&quot;, y=&quot;I&quot;, Geom.line)
         )
    )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This yields the following figure:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/infected.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The data are the continuous line, and the predictions are the dots. It’s
not a perfect fit, but it reproduces the main features of the epidemics
quite well. When compared to the original parameters, notably, it predicts
the correct date at which no more cases are observed, but it slightly
over-estimates the number of cases during the peak.&lt;/p&gt;

&lt;p&gt;We can also have a look at the distribution of $\beta$&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/dist_beta.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;and $\nu$&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/dist_nu.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In both figures, the vertical lines indicate the previous estimations of the
parameters. While they are well within the posterior distribution, it seems
that (i) using a different method for fitting and (ii) estimating the value
of $I_0$ gives different estimates.&lt;/p&gt;

&lt;p&gt;Finally, what about the distribution of $R_0$? The original $R0$ was estimated
to 3.76. Using the result of the ABC, I found&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;julia&amp;gt; mean(763.*(posteriors[:beta]./posteriors[:nu]))
   3.292957996735042
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which is not &lt;em&gt;all&lt;/em&gt; that different from the original prediction. Note that
a consistent result is that the epidemic parameters estimated through ABC
make the epidemic &lt;em&gt;less&lt;/em&gt; aggresive than the original one. The recovery rate
is higher, the $R_0$ is lower, but the transmission rate is higher.&lt;/p&gt;

&lt;h1 id=&quot;as-a-brief-wrap-up&quot;&gt;As a brief wrap-up…&lt;/h1&gt;

&lt;p&gt;I love &lt;code&gt;julia&lt;/code&gt;. I haven’t even thought about possible optimizations and it’s
fast already. Also, it’s simple to write. Clearly sliced bread was the best
thing until &lt;code&gt;julia&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;ABC – at least as long as a simple rejection scheme is used – is not that
hard. This gives me a bunch of ideas.&lt;/p&gt;

&lt;p&gt;I love epidemiology. For some reason, I just find it fascinating. This is the
reason why I switched from immunology to parasitology to ecology. I love it.&lt;/p&gt;

</content>
  </entry>
  
  <entry>
     <title>Tracking changes in markdown</title>
     <link href="http://timotheepoisot.fr/2014/07/10/markdown-track-changes/"/>
    <updated>2014-07-10T00:00:00+12:00</updated>
    <content type="html">&lt;p&gt;Using &lt;code&gt;markdown&lt;/code&gt; to write papers is an insanely great experience, because
it is a concise yet powerful markup language, that &lt;code&gt;pandoc&lt;/code&gt; can export to
almost anything you like (and Word). Some journals, though, require that you
upload a document with all changes highlighted in addition to the revised
manuscript. As a reviewer, I find this helpful, but as an author, I’m always
trying to find a way &lt;em&gt;not&lt;/em&gt; to do it because it is not really straightforward.&lt;/p&gt;

&lt;p&gt;Well, as it turns out, this is not true. I decided that it was time to stop
being lazy, and I found a very simple way to get a good-looking marked copy
with all changes.&lt;/p&gt;

&lt;p&gt;Here is my originally submitted manuscript, in a file called &lt;code&gt;orig.md&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-markdown&quot;&gt;# My cool project

This is a really cool paper written in `markdown`. It has equations like
$(1-x)^\rho$ and, also some references to *really* cool papers [@fra92].

I hope it will be accepted!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I can compile that using &lt;code&gt;pandoc&lt;/code&gt;, and get the PDF file that I send for
review. The referees, being referees, required some changes, so I made the
revisions in a file called &lt;code&gt;revised.md&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-markdown&quot;&gt;# My cool revised project

This is a really cool paper written in `markdown`. It has equations like
$(1-x)^\alpha$ because $\rho$ was not a good parameter name. Also there are
some references to *really* cool papers [@fra92; and references therein].

I hope it will be accepted *now*!

# References
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Producing the marked-up copy is relatively easy. The first step is to convert
both documents to &lt;code&gt;latex&lt;/code&gt;, then use &lt;code&gt;latexdiff&lt;/code&gt;. This is best done with a
simple &lt;code&gt;makefile&lt;/code&gt;, which I reproduce here:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-makefile&quot;&gt;OPTS= --bibliography=/home/tp/.pandoc/default.json --csl=/home/tp/vrac/styles/ecology.csl --template=template.tex

all: orig.pdf revised.pdf diff.pdf

orig.pdf: orig.md
	pandoc $&amp;lt; -o $@ $(OPTS)

revised.pdf: revised.md
	pandoc $&amp;lt; -o $@ $(OPTS)

diff.pdf: orig.md revised.md
	pandoc orig.md -o orig.tex $(OPTS)
	pandoc revised.md -o revised.tex $(OPTS)
	latexdiff orig.tex revised.tex &amp;gt; diff.tex
	pdflatex diff
	rm {revised,orig,diff}.tex
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;OPTS&lt;/code&gt; variable can be changed to whatever you like (and the whole file
should be made nicer, but hey…), and once this is done, you just need to do
&lt;code&gt;make diff.pdf&lt;/code&gt; in the directory where your two files (&lt;code&gt;orig.md&lt;/code&gt; and &lt;code&gt;revised.md&lt;/code&gt;)
are. The resulting PDF file will look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/md-diff.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is what I &lt;em&gt;really&lt;/em&gt; like with &lt;code&gt;markdown&lt;/code&gt; and &lt;code&gt;pandoc&lt;/code&gt; – you can have a
whole &lt;code&gt;make&lt;/code&gt;-based production workflow. Which means that automating tasks
is easy, and once you spend 10 minutes figuring out a solution once, it
will take you &lt;em&gt;seconds&lt;/em&gt; to apply it every time the situation arises. If we
compiled all of the dirty little hacks like this one in a single &lt;code&gt;makefile&lt;/code&gt;
specifically to make editing with &lt;code&gt;markdown&lt;/code&gt; easier, we could have a really
good do-it-yourself manuscript preparation system. Just sayin’…!&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>The costs of Open Science</title>
     <link href="http://timotheepoisot.fr/2014/07/05/costs-open-science/"/>
    <updated>2014-07-05T00:00:00+12:00</updated>
    <content type="html">&lt;p&gt;Over the last year, I have had &lt;em&gt;a lot&lt;/em&gt; of discussions about upsides,
downsides, and other directionless-sides (assuming there are such things)
of open science. I talked about the upsides many times (and I will keep on
doing it), but it would not be really honest not to mention the potential
disadvantages and risks of adopting new practices: publishing in new journals,
using preprints in a field where it is not the norm, open peer-review,
publishing raw datasets, … So, roll yourself in your favorite blanket,
and brace yourself for the impending wave of negativity (also a &lt;em&gt;Jurassic
Park&lt;/em&gt; quote somewhere to keep you on the edge).&lt;/p&gt;

&lt;p&gt;Most of the risks come from the fact that these practices are, to a certain
extent, &lt;em&gt;new&lt;/em&gt; &lt;sup id=&quot;fnref:new&quot;&gt;&lt;a href=&quot;#fn:new&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, and science is historically well attached to its
“rules and standards” [@kuh70, chap. 2]:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Because [the new scientist] there joins men who learned the bases of
their field from the same concrete models, his subsequent practice will
seldom evoke overt disagreement over fundamentals. &lt;strong&gt;Men whose research is
based on shared paradigms are committed to the same rules and standards
for scientific practice&lt;/strong&gt;. That commitment and the apparent consensus it
produces are prerequisites for normal science, &lt;em&gt;i.e.&lt;/em&gt;, for the genesis and
continuation of a particular research tradition.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Keeping the same core of (currently) common practices, in this perspective,
is a matter of self-preservation (and it is amusing to note that in Kuhn’s
view, at least as it is expressed in this paragraph, there is little
chance that &lt;em&gt;established&lt;/em&gt; scientists will be the source of change – my own
observations don’t support that, but this is 45 years after this paragraph
was written). Deviation from these practices carries a two-fold risk of
either excluding yourself from the boy’s club (I’m most likely reading too
much into it, but I always found Kuhn’s use of &lt;em&gt;men&lt;/em&gt; instead of &lt;em&gt;scientists&lt;/em&gt;
meaningful), or being viewed as opposed to the “continuation” of your field.&lt;/p&gt;

&lt;p&gt;Falling exactly in line with this argument, most of the objections to open
science that I have received, were about the need to preserve either oneself
or the field as a whole. Preprints are bad because they will facilitate
the spread of (and legitimate) bad or biased science. Open reviews are bad
because people will hold grudges against you. Publishing in new journals is
bad because – whether we like it or not – the journals you publish in are
still used as a measure of your worth, and the name of a journal is most
likely a “green-beard trait”.&lt;/p&gt;

&lt;p&gt;All of these potential costs are real (although I think that the benefits,
both collective and individuals, far outweigh them), and so it should be
expected that open science would hurt early-career scientists the most. This
attitude is exemplified by the recent decision of &lt;em&gt;Ecology Letters&lt;/em&gt; not
to allow preprinted papers to be submitted &lt;a href=&quot;http://jabberwocky.weecology.org/2014/06/30/why-the-ecology-letters-editorial-board-should-reconsider-its-no-vote-on-preprints-2/&quot;&gt;so as to protect early career
people from themselves&lt;/a&gt; (although they should be given credit for at
least discussing the situation). Yet as far as I can tell, the opposite is
happening: those of us with supposedly the most to lose are also spending
the most time and energy experimenting, advocating, and generally having
tons of fun trying to make ourselves at home.&lt;/p&gt;

&lt;p&gt;And this is a side of the discussion that is (I think) often overlooked. It’s
about settling into a new home, and shuffling the furniture around a bit
to see if we can make it more to our liking. It’s not about tearing down
the house to build a hippie commune. And it’s certainly not about shifting
the system towards practices that would allow us to be sloppy and generally
unprofessional. Most of the calls for openness in science are emphasizing
accountability and transparency; I really am convinced that in an entirely
open system, it is virtually impossible to cut corners (tho in the immortal
words of Dr. Ian Malcolm, “Life finds a way”). In short, deciding whether or
not it’s worth bearing whatever (real of perceived) costs of open science
is worth it is about deciding whether or not we trust that the system is
currently at its global optimum, and whether or not less seasoned scientists
also have a saying in shaping the system they will practice their science in.&lt;/p&gt;

&lt;p&gt;Marie Curie is quoted as saying “Nothing in life is to be feared, it is
only to be understood; now is the time to understand more, so that we may
fear less.”. This captures my feelings about all of the above particularly
well. I think it’s important not to overlook the fact that some of the people
the most able to bear the “costs” of these practices are also (sometimes)
opposed to them for fear of said costs. But without seriously considering these
practices, that is by not seeing them as marginal anymore, it’s difficult
to say whether or not these costs exist at all, and whether they will erode
over time because we will also better understand and measure the benefits.&lt;/p&gt;

&lt;p&gt;I would hate that you think that I am reducing the issue to a generation-gap
thing, which I don’t believe it is. But it’s true that those of us with less
reputation attached to our name should be the most fearful of any new practice
that would supposedly attract any cost, because it would hit us proportionally
harder than it would hit someone with a number of publications in the triple
digits. Nor am I expecting that everything will change overnight. But without
being really engaged in some experimentation, I don’t think we will ever be
able to correctly assess the costs and the benefits.&lt;/p&gt;

&lt;p&gt;As a final note – I could write a text twice as long about all of the good
things that happened to me as a consequence of being open. I’ve heard the
argument that some of the costs may be expressed years after the fact. I really
don’t think this is a particularity of either the new or old ways of doing
things. And I understand that all of us have different experiences. But from
what I have heard of people trying new things, the result was “mostly good”.&lt;/p&gt;

&lt;p&gt;new. They are certainly variations around the way things are done currently,
but they appeared through mutation rather than being invasions.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:new&quot;&gt;
      &lt;p&gt;I don’t really think that any of these practices are inherently &lt;a href=&quot;#fnref:new&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
  </entry>
  
  <entry>
     <title>Preprints, and the issue of (misplaced) trust</title>
     <link href="http://timotheepoisot.fr/2014/06/28/preprints-trust/"/>
    <updated>2014-06-28T00:00:00+12:00</updated>
    <content type="html">&lt;p&gt;Yesterday, a bunch of people (and I) were discussing what type of contributions
to list in your CV when you are looking for a job. Papers that are in press,
and accepted clearly belong here, but the discussion revolved around preprints
quite fast. I suggested to include them, and I got generally negative feedback:&lt;/p&gt;

&lt;blockquote class=&quot;twitter-tweet&quot; lang=&quot;en&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://twitter.com/tpoi&quot;&gt;@tpoi&lt;/a&gt; &lt;a href=&quot;https://twitter.com/ProfLikeSubst&quot;&gt;@ProfLikeSubst&lt;/a&gt; &lt;a href=&quot;https://twitter.com/GenomeDaddy&quot;&gt;@GenomeDaddy&lt;/a&gt; if you have an acceptance letter from an editor, sure put it on there. otherwise, no. &lt;a href=&quot;https://twitter.com/hashtag/inmyworld?src=hash&quot;&gt;#inmyworld&lt;/a&gt;&lt;/p&gt;&amp;mdash; Hope Jahren (@HopeJahren) &lt;a href=&quot;https://twitter.com/HopeJahren/statuses/482608451319644163&quot;&gt;June 27, 2014&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;p&gt;Most of the discussion that followed was about the fact that, unless something
is published in a peer-reviewed journal, it’s moot and should not be accounted
for. There are two assumptions behind that, and both of them are wrong. First,
that you should not trust anything unless it is peer-reviewed. Second, that
the peer-review process is the only thing standing between us and bad science.&lt;/p&gt;

&lt;p&gt;As I suggested later in the discussion, people that don’t trust the results
in a preprint should leave the room during talks based on unpublished
results. These have not been peer-reviewed, either. I half-suspect that
the fact that (in biological sciences) conferences are a more established
medium for the presentation of new scientific results than preprints
(despite being arguably worse for that), people are more willing to trust
12 minutes of un-reviewed talk than 12 pages of an un-reviewed paper by
the same person. Let’s file that under “inertia”, and let’s get back to the
issue of conditionning whether or not we trust a paper on its position in
the editorial pipeline.&lt;/p&gt;

&lt;p&gt;Assuming that published papers can be trusted is making the hypothesis
that peer-review is good at catching bad papers. Additionally, some people
raised the argument that the more &lt;em&gt;reputable&lt;/em&gt; the journal (read: high
impact factor), the more confident we can be in the paper. That is not true
[@bre13]. It is a well established fact that high-impact journals tend to have
a higher retraction rate than low-impact journals. The very places where some
expect to see the most outsanding review process are involved in a “scandal”
every month or so, these days (arsenic DNA, stem cells, and the track record
goes back all the way to the memory of water). But even without considering
the &lt;em&gt;prestige&lt;/em&gt; of a journal, there is this thing with absolute statements
(&lt;em&gt;Peer-reviewed papers count for something&lt;/em&gt;): a counter-example is enough
to make them moot. There have been counter-examples enough that, although a
peer-reviewed paper &lt;em&gt;may be&lt;/em&gt; trusted, you still have to find out for yourself
(by reading it).&lt;/p&gt;

&lt;p&gt;And if the way to know if something, anything, is a good piece of scientific
litterature is to read it, then what is the difference between reading
a pre-print and a reviewed paper? As I said a few times, it’s not that I
don’t trust the peer-review process, it’s that I trust my own critical sense
more. I’m not going to blindly trust a paper because it has been stamped as
good by a journal and two to five people I don’t know, and I’m not going to
distrust a paper because it has not been through the same process. Neglecting
the results of my colleagues, or blindly trusting them because I expect others
to do the critical thinking for me, are I think equally irresponsible. But
Karthik said it best:&lt;/p&gt;

&lt;blockquote class=&quot;twitter-tweet&quot; lang=&quot;en&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://twitter.com/tpoi&quot;&gt;@tpoi&lt;/a&gt; I&amp;#39;ll consider it word of god as soon as two busy reviewers have given it a cursory glance and a 3rd has been a total jerk about it.&lt;/p&gt;&amp;mdash; Karthik Ram (@_inundata) &lt;a href=&quot;https://twitter.com/_inundata/statuses/482629845348651008&quot;&gt;June 27, 2014&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;p&gt;Of course &lt;em&gt;most&lt;/em&gt; of the idea is that peer-review is a filter between us and
bad science. I don’t know how many papers I have reviewed in the last 4 years,
I would be tempted to say about 25+ (not counting the one I reviewed multiple
times). There are two of these I found terrible. Of these two terrible papers,
&lt;em&gt;one&lt;/em&gt; was “wrong”, not even in the sense that they got the bases wrong, but
it was just severely incomplete. And so you don’t start thinking that I’m
a really nice referee that is always happy about everythinAnd so you don’t
start thinking that I’m a really nice referee that is always happy about
everything: I’m not; there are (I think) two instances where I recommended
something good and the paper ended up being rejected. But even though, my own
experience places the probability of a paper being terrible at approximately
0.04. I’m happy to take that chance if it means that I’ll be reading the
cool stuff a year or so before it makes it through the review process.&lt;/p&gt;

&lt;p&gt;But more broadly, there is a very disturbing thing behind the idea that
peer-review is such an essential filter: authors can’t be trusted, and only
referees are to thank for such great science being published (and also for
making us spend another round in review to jump through many many hoops
to satisfy their own personal biases as they yield a ridiculous amount of
power). In addition, people using preprints are in the “whackadoodles” in the
eyes of some, as opposed to people that took the time to formulate a strong
case for preprints, and got it published in a &lt;em&gt;reputable&lt;/em&gt; journal [@des13] –
which, for the record, do not mean you should blindly trust us. But express
your disagreement with an argumentation, not with a “That’s the way it is”
or a cheap insult.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>Open Scientific Software License</title>
     <link href="http://timotheepoisot.fr/2014/06/05/ossl/"/>
    <updated>2014-06-05T00:00:00+12:00</updated>
    <content type="html">&lt;p&gt;I have been doing a bunch of reading on software licenses, because the
question of which one to pick popped up in two projects I currently work
on. There are tools like GitHub’s &lt;em&gt;&lt;a href=&quot;http://choosealicense.com/&quot;&gt;choose a license&lt;/a&gt;&lt;/em&gt;, that compare a
fairly good selection of FOSS (Free and Open Source Software) licenses. And
you can find a lot of (good) advice around the internet that will advise
you to pick a license for your code. See &lt;em&gt;e.g.&lt;/em&gt; &lt;a href=&quot;http://www.software.ac.uk/resources/guides/choosing-open-source-licence&quot;&gt;sofwtare.ac.uk&lt;/a&gt;,
a paper by &lt;a href=&quot;http://www.ploscompbiol.org/article/info%3Adoi%2F10.1371%2Fjournal.pcbi.1002598&quot;&gt;Andrew Morin and colleagues&lt;/a&gt;, and more &lt;a href=&quot;http://www.astrobetter.com/the-whys-and-hows-of-licensing-scientific-code/&quot;&gt;tutorial like
documents&lt;/a&gt;. If you read all of that (and if you produce code,
I think you should), you’ll most likely agree that licensing your work
is important.&lt;/p&gt;

&lt;p&gt;Yet &lt;a href=&quot;http://matt.might.net/articles/crapl/&quot;&gt;as Matt Might points out&lt;/a&gt;, FOSS licenses tailored specifically for
academic use should have other requirements besides no liability and ensuring
sharing of modified code. I spend a sometimes significant amount of my time
writing code to perform various analyzes, or to reproduce results. I think that
this is a research output equally important to publishing the paper (in that
I see very little value in a great methodological paper with no code attached,
for starters); which means that I think it should be credited somehow.&lt;/p&gt;

&lt;p&gt;One thing I have noticed in various discussions over the last few months is
that the fact that (good) software is hard to produce completely eludes some
people. This is perfectly understandable. In part, because we (as computer
users) are &lt;a href=&quot;http://linux.oneandoneis2.org/LNW.htm&quot;&gt;demanding software working intuitively out of the box&lt;/a&gt;. As a
result, most mainstream operating systems (and I’m including several Linux
distributions in that) are focusing on &lt;em&gt;user-friendliness&lt;/em&gt;, so that even
complex things &lt;em&gt;appear&lt;/em&gt; really simple. Given that, really, how hard can
programming a &lt;code&gt;R&lt;/code&gt; package that doesn’t even have a GUI be? Software is a
given right, and (I have checked some of my papers and I must plead guilty
to that too) citing some of the software that made a paper possible is still
not automatic. And to really drive the point home: if someone publishes
a paper using package &lt;code&gt;xyz&lt;/code&gt;, without citing the paper describing &lt;code&gt;xyz&lt;/code&gt;
(or the software itself should there be no paper to cite), that person get
credit while the software creator does not.&lt;/p&gt;

&lt;p&gt;Even the very concept of a software license is focused on people with some
computing experience. It tells things about the use of the code, in the
sense that it tells you whether you can make money with it (not with the
output), whether you can modify the code, and if so, which future licenses
are acceptable. In short, the licensing issue is definitely an important
one in science, but it does not speaks to the end users. Now before you
preemptively burn be at the stake before I propose &lt;a href=&quot;http://xkcd.com/501/&quot;&gt;EULAs&lt;/a&gt; for all
scientific software, hear me out. On a daily basis, what you do with my code
matters to me far less than whether I get credit for having produced the code
in the first place, especially when this code is actually used. I would be
happy to release 90% of everything I write using the &lt;a href=&quot;http://www.wtfpl.net/about/&quot;&gt;WTFPL&lt;/a&gt; (just
to see if I can managed to spell it out in a paper, mostly). So I think we
need &lt;em&gt;some sort&lt;/em&gt; of license between the creator of the software and the users.&lt;/p&gt;

&lt;p&gt;An additional complication (or nuance, rather) is that thanks to increased
computer literacy, a lot of people will be able to either add to, or modify
your software for their own needs. In a sense, software users will contribute
to the improvement of it, &lt;em&gt;as long as they give back&lt;/em&gt; their modifications. It
may not seem like a lot, but a function to read and convert a specific
type of file to use with a particular package &lt;em&gt;is&lt;/em&gt; a potentially worthwhile
addition to it. In this hypothetical scenario, if the “new” format is used
by a lot of people in the field, then this can even improve the adoption
of the software. So where to go from here? An ideal license for scientific
software would (i) ensure that the creator gets credit for its work, (ii)
facilitates the improvement of the software through community contributions,
and (iii) build on existing FOSS licenses.&lt;/p&gt;

&lt;p&gt;I think the modular design of the &lt;em&gt;Creative Commons&lt;/em&gt; licenses is a good
inspiration. Basically, we could write an OSSL (Open Scientific Software
License) with the four following clauses (purely as an intellectual
exercise). Each people would then be free to use any combination (or all)
of the clauses in their particular releases. Ideally, the software would be
released under two different licenses. One of the mainstream FOSS ones for
general code re-use, and the OSSL for use in research.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Open Scientific Software License (OSSL) 0.0.1

This document presents the conditions for the use of, and contribution to,
&quot;XXXX&quot;, hereafter &quot;the software&quot;, by you, hereafter &quot;the user&quot;.

[insert usual warning about no liability]

(AU) AUTHORSHIP -- Developpers of the software do not make any claim to
authorship on projects using the software unless
   
   (1) explicitely invited by the user
   (2) the user requests the software to be altered in any significant
       way for the purpose of a project, in which case authorship for this
       particular project must be negociated between the user and the creator

(CR) CREDIT -- Any scientific production appearing in print (excluding posters
and talks, including electronic only content) using the software must cite
the following item[s]:

   Ref. to the paper or the software itself
   You are expected to keep it relatively short

(DI) DISTRIBUTION -- Outside of academic/research projects, this code is
dual-licensed under the terms XXX license, a copy of which is attached to
this project under the COPYING file.

(EN) ENHANCEMENT -- If the software is modified, expanded, or improved during
the realisation of a study, the person in charge of the modifications is
required to propose them for merging when the results obtained with the
modified software are first published, or earlier.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you use code released under the &lt;code&gt;OSSL/AU-CR&lt;/code&gt;, for example, you can use
the code and modify it without proposing your modifications for merging,
and your only requirement is to cite it. The &lt;code&gt;AU&lt;/code&gt; clause is (in my opinion)
important because it makes the conditions for authorship clear: if you
don’t interact in any way with the software creator, then (of course) he or
she should not be asking for authorship. But if you ask whether or not the
creator could introduce a new functionality, or alter the functioning of the
program in any way, then it’s fair to discuss whether or not this qualifies for
authorship (I argue that it does; I’ve had a very good experience recently of
inviting a co-author to help us figure out his package, and he made excellent
contributions to the paper).&lt;/p&gt;

&lt;p&gt;So in short (that was not short at all!), this is the current state of my
thinking about finding a way to license (open) scientific software that would
respect the right of software creators to get credit for their work. Whether
or not I’ll add this as a license for my own projects is something I have
not decided yet. That will depend in part of the feedback I get…&lt;/p&gt;

</content>
  </entry>
  
  <entry>
     <title>A contribution policy for open manuscripts</title>
     <link href="http://timotheepoisot.fr/2014/05/02/contribution-policy-open-ms/"/>
    <updated>2014-05-02T00:00:00+12:00</updated>
    <content type="html">&lt;p&gt;A few months ago, lab-mate and PhD candidate extraordinaire Philippe
Desjardins-Proulx proposed &lt;a href=&quot;http://phdp.github.io/posts/2013-07-22-scriptopia.html&quot;&gt;scriptoria&lt;/a&gt;, a system to track manuscripts that are
being developed on &lt;em&gt;GitHub&lt;/em&gt; (and other version control systems). It’s a cool
project, because it will allow everyone to access in progress manuscripts,
and allow people to give feedback &lt;em&gt;before&lt;/em&gt; the papers reach the referees. In
the true spirit of open source software collaborative development, it will
also allow anyone to find projects (manuscripts) in which one can make a
valid contribution, fork them, and contribute.&lt;/p&gt;

&lt;p&gt;The thing is, not anyone is OK with people proposing changes to their
manuscript. And because &lt;code&gt;scriptoria&lt;/code&gt; will rely on a set of meta-data, it
is time to start thinking about how we can encode our willingness to see
others contribute to our work. If you find an in-progress manuscript you
might be able to contribute to, before you put any substantial effort in it,
this will tell you the chance that the authors will accept your contribution.&lt;/p&gt;

&lt;p&gt;This scale will serve as a primitive social contract between the authors
and other potential contributors. It will tell whether modifications are
accepted, and how the authors will react if modifications are submitted
and accepted. Note that in all cases, authors are entirely free to reject
contributions they don’t like. Which brings me to the most central point
about how to contribute to an open manuscript: &lt;strong&gt;do not fork it to write
a slightly different paper&lt;/strong&gt;. Seriously, that’s just bad form. And seeing
that we are dealing with text, it’s straight up plagiarism. Be a good sport,
and if your contribution is rejected, raise the issue (eventually) in a
post-publication forum.&lt;/p&gt;

&lt;p&gt;Anyways. What I envision is a linear scale, ranging from &lt;code&gt;0&lt;/code&gt; to &lt;code&gt;3&lt;/code&gt;, with
higher values indicating a less and less open attitude. Here is a brief
summary of the different levels.&lt;/p&gt;

&lt;p&gt;The first degree (which I think should &lt;em&gt;not&lt;/em&gt; be the default) is &lt;code&gt;0&lt;/code&gt;. All
contributions are welcome, and any substantial &lt;em&gt;addition&lt;/em&gt; to the paper (text,
analysis, figure) which is accepted will result in co-authorship. This is my
personal default, but I appreciate that not everyone is comfortable working
this way.&lt;/p&gt;

&lt;p&gt;The second degree (&lt;code&gt;1&lt;/code&gt;) is what I think should be the default. Contributions
are welcome &lt;em&gt;after consultation with the authors&lt;/em&gt;. This can take the form
of opening an issue. State the change you would like to make, how you
plan to make it, and if you have in principle approval, go ahead with your
contribution (fork the repository, then make a pull request). With this mode
of contributions, authors retain a lot of control on their paper.&lt;/p&gt;

&lt;p&gt;The next degree (&lt;code&gt;2&lt;/code&gt;) is where authors only welcome &lt;em&gt;minor&lt;/em&gt;
contributions. Typically, you will not claim co-authorship, because the authors
will only accept small changes (typos, suggestions of better references,
ways to make a paragraph clearer). Most of that can be done in Issues,
rather than by forking the project. If you are writing a “Forum” or similar
type of paper, or if you know excatly what you want to do with your results
and are not open to changes, then this level of openness is good. You will
still encourage users to help on the manuscript, just not in any major way.&lt;/p&gt;

&lt;p&gt;The final degree (&lt;code&gt;3&lt;/code&gt;) is &lt;em&gt;Keep Off!&lt;/em&gt;. The authors want their manuscript to be
online for a variety of reasons (transparency, accountability, early exposure),
but they are not welcome to contributions. This is a valid position. Consider
that any manuscript with this level of openness is &lt;em&gt;read-only&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;There you have it. I don’t know whether these suggestions will be in the
released version of &lt;code&gt;scriptoria&lt;/code&gt;, but I think these are issues we should
be adressing now. The “tradition” for open-source projects is to have a
file called &lt;code&gt;CONTRIBUTING&lt;/code&gt; at the root of the project, in which the ways
of suggesting changes and the “culture” of contribution for a particular
project are outlined. Manuscripts are (hopefully) going to be shorter lived
than most open source software (once the paper is accepted, the repository
is only useful as an archive), so we can go for a simpler system. But being
upfront about whether contributions are welcome is important, and so a
standardized way of saying it would only help.&lt;/p&gt;

</content>
  </entry>
  
  <entry>
     <title>Don't be afraid of Open Access</title>
     <link href="http://timotheepoisot.fr/2014/03/24/open-access-impact/"/>
    <updated>2014-03-24T00:00:00+13:00</updated>
    <content type="html">&lt;blockquote&gt;
  &lt;p&gt;This post is a copy of a response &lt;a href=&quot;http://nature.berkeley.edu/~kram/&quot;&gt;Karthik
Ram&lt;/a&gt; and I wrote to a
recent paper in &lt;em&gt;Trends in Plant Science&lt;/em&gt;, criticizing Open
Access as being risky, especially to early career people. &lt;a href=&quot;http://www.cell.com/trends/plant-science/fulltext/S1360-1385%2814%2900060-0&quot;&gt;Stephen
Curry&lt;/a&gt;
wrote a great reply. Our paper never made it past the editor, so I reproduce
it here.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In a recent Letter, Agrawal (2014) raises four points for which scientists
should be skeptical of the Open Access (OA) movement, two of which we think
are particularly mis-leading.&lt;span class=&quot;margin&quot;&gt;Agrawal (2014) &lt;em&gt;Trends in Plant Science&lt;/em&gt; &lt;a href=&quot;http://dx.doi.org/10.1016/j.tplants.2014.01.005&quot;&gt;DOI&lt;/a&gt;&lt;/span&gt; First, that the impact factor of OA journals
is not higher than the impact factor of non-OA journals. Secondly, that it
is tempting to use a journals impact factor as a proxy for the quality of a
paper, and therefore, high quaility research published in a less prestigious
OA journals runs the risk of being perceived less favorably.&lt;/p&gt;

&lt;p&gt;First, the strong distinction beween OA and non OA journals is a false
dichotomy. OA is a mode of diffusion of scientific literature in which the
authors, or its home institution, buys back the rights of an article to the
publisher, so that the article is free to access. Although some journals apply
an OA licences to their entire content, an increasing number of publishers
are adopting &lt;em&gt;per&lt;/em&gt; article OA options. Even though, the notion that pure-OA
journals have a lower impact is challenged by some. James Pringle of Thomson
ISI (i) recognizes that the relevance of journals when talking about OA is
dubious and (ii) &lt;a href=&quot;http://www.nature.com/nature/focus/accessdebate/19.html&quot;&gt;“prospective authors should not fear publishing in these
journals”&lt;/a&gt;. In
addition, there are numerous studies showing a clear &lt;a href=&quot;http://www.istl.org/10-winter/article2.html&quot;&gt;&lt;em&gt;Open Access citation
advantage&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Second, the fact that OA journals carry less prestige is not a problem with
the OA movement. Measures of journal impact are known to be extremely biased
by a few papers concentrating a few citations, and cannot possibly be used
to estimate the quality of a paper, let alone its &lt;em&gt;likely impact&lt;/em&gt;. Simply
put, the impact factor of a journal is not the expected number of citations
a single paper will receive. If rigor is important to us, then it is not
acceptable to think that a paper is bad because it was published in a less
established journal, or that a paper is good because it was published in a
highly selective journal; the same paper is just as good, and should be just
as impactful, whether published in &lt;em&gt;PLoS One&lt;/em&gt; or &lt;em&gt;Nature&lt;/em&gt;. &lt;span class=&quot;margin&quot;&gt;Fenner (2013) &lt;em&gt;PLoS Biology&lt;/em&gt; &lt;a href=&quot;http://dx.doi.org/10.1371/journal.pbio.1001687&quot;&gt;DOI&lt;/a&gt;&lt;/span&gt; This point strikes
us as a demonstration that the metrics used for evaluations are biased. The
recent years say the development of &lt;em&gt;article-level&lt;/em&gt; metrics (Fenner, 2013),
which provides a way to measure the impact of an article regardless of what
journal it appeared in. We think that rather than pushing against the OA
model, we should have a discussion about how these measures can be used to
objectively evaluate research output.&lt;/p&gt;

&lt;p&gt;Agrawal concludes his paper on the need to find &lt;em&gt;an alternative model
of publishing that suits the primary goals of scientists&lt;/em&gt;. We would like to
make the point that, particularly in ecology and environmental sciences, the
primary goal of research should be to produce fundamental insights that can
be mobilized to solve large scale problems. Making information flow freely
between scientists, policy-makers, and stakeholders is paramount to this
effort. What does not strikes us as a &lt;em&gt;primary&lt;/em&gt; goal, is the maximization
of self-aggrandizing, not to mention arbitrary, measures of impact.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>Teaching isoclines in the two-species competitive logistic model</title>
     <link href="http://timotheepoisot.fr/2014/03/18/teaching-isoclines/"/>
    <updated>2014-03-18T00:00:00+13:00</updated>
    <content type="html">&lt;p&gt;I’ve had a great discussion by email with &lt;a href=&quot;http://www.crystalernst.com/&quot;&gt;Crystal Ernst&lt;/a&gt;, about the
best way to teach isoclines and phase portraits to undergraduates. I had
this discussion countless times over the last two years, and I struggled
when I had to teach that myself. I ended up finding a little twist on the
classical diagrams, and so I decided that it was time to share it.&lt;/p&gt;

&lt;p&gt;Isoclines are usually first introduced when discussing the logistic model with competition between two species, or&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\dot N_1 = N_1 r_1 \left(1 - \frac{\alpha_{11}N_1+\alpha_{12}N_2}{K_1} \right)
&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\dot N_2 = N_2 r_2 \left(1 - \frac{\alpha_{22}N_2+\alpha_{21}N_1}{K_2} \right)
&lt;/script&gt;

&lt;p&gt;It’s an important model, as we can use it as a gateway to talk about
coexistence, local stability, and ecosystem functioning. It is also a perfect
transition between the empirical side of ecology with Gauses’s experiments, and
the more theoretical side of it. I &lt;em&gt;love&lt;/em&gt; this model, especially for teaching.&lt;/p&gt;

&lt;p&gt;There are a number of intersting things to explain to students when introducing
this model. First, each species &lt;script type=&quot;math/tex&quot;&gt; n &lt;/script&gt; perceives its environment
as a “box” with &lt;script type=&quot;math/tex&quot;&gt; K_n &lt;/script&gt; compartments. Left to their own devices,
each “individual” (or units of biomass) will “fill” &lt;script type=&quot;math/tex&quot;&gt;\alpha_{nn}&lt;/script&gt;
compartments, which usually equates unity for a single species model. The
growth of the population is therefore a quest for free compartments to fill,
and when most of the box is filled, the effective growth rate decreases. When
all compartments are filled (&lt;script type=&quot;math/tex&quot;&gt;N_n = K_n&lt;/script&gt;), then the growth rate is
0, and the population is at equilibrium. Cue a discussion on the mechanisms
that would allow us to represent the environment as a box with compartments
(space requirements, having enough to eat, foraging area, and whatever else
you can think of).&lt;/p&gt;

&lt;p&gt;The point with introducing a second species is that there are now
two types of organisms to share the same “box”. And individuals from
species &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; are &lt;em&gt;perceived&lt;/em&gt; by those of species &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;
as taking up &lt;script type=&quot;math/tex&quot;&gt;\alpha_{np}&lt;/script&gt; compartments. The opposite is true,
and we can define values for the intensity of &lt;em&gt;intra&lt;/em&gt; and &lt;em&gt;inter&lt;/em&gt; specific
competition. The question is, then, what are the combinations of values of
&lt;script type=&quot;math/tex&quot;&gt;\mathbf{\alpha}&lt;/script&gt; that allows coexistence, competitive exclusion,
or make the system sensitive to initial conditions?&lt;/p&gt;

&lt;p&gt;It’s at this point that I usually introduce the notion of &lt;em&gt;equilibrium&lt;/em&gt;,
&lt;em&gt;i.e.&lt;/em&gt; a state of the system in which the two populations &lt;em&gt;stop growing&lt;/em&gt;. The
calculus-based solution is to find values of the population densities that
make the two derivatives null at the same time. When you are a facing
a class of undergrads that are not especially enamored with mathematics
(&lt;a href=&quot;https://peerj.com/articles/285/&quot;&gt;potentially a large part of the population&lt;/a&gt;), this approach is not
likely to get them excited. Which is partly why we usually use &lt;em&gt;isoclines&lt;/em&gt;
and &lt;em&gt;phase portraits&lt;/em&gt; to explain how different combinations of intra/inter
specific competition strenghts affect the potential for coexistence.&lt;/p&gt;

&lt;p&gt;The problem is, &lt;strong&gt;explaining how isoclines work is awful&lt;/strong&gt;. I know many
people that are able to predict a &lt;em&gt;lot&lt;/em&gt; of things just by drawing a bunch of
lines on a whiteboard. Did I say many? Well if the discussions I had with
people are to be believe, these are actually the happy few. The rest of us
are left scratching our head in front a cryptic diagram that makes absolutly
no sense. I blame the difficulty to read isoclines as the single reason for
which adaptive dynamics is frwoned upon by many people (that, and the fact
that any method involving third order derivatives is rapidly messy).&lt;/p&gt;

&lt;p&gt;So here is my solution: &lt;strong&gt;use vector fields&lt;/strong&gt;. With a vector field, it’s
virtually impossible to screw up the explanation, because all you have to do
is (i) put your finger somewhere on the diagram and (ii) follow the arows
until the equilibrium that the system would eventually reach. Want to know
how a perturbation affects this equilibrium? Just jiggle your finger a bit,
and go back to step (i). Did you ended up at the same point? Congratulations,
that’s local stability.&lt;/p&gt;

&lt;p&gt;OK, so what’s a vector field? The phase portrait of the system is defined by
two axes, one for the density of species &lt;em&gt;1&lt;/em&gt;, the other for the density of
species &lt;em&gt;2&lt;/em&gt;. The isoclines are the lines going trough the two equilibria for
species &lt;em&gt;1&lt;/em&gt; (&lt;script type=&quot;math/tex&quot;&gt;N_1 = (0; N^\star_1)&lt;/script&gt;), and trough the two equilibria for
species &lt;em&gt;2&lt;/em&gt; (&lt;script type=&quot;math/tex&quot;&gt;N_2 = (0; N^\star_2)&lt;/script&gt;). When you are &lt;em&gt;above&lt;/em&gt; (axis-wise)
your isocline, you converge back to it, and where you are &lt;em&gt;below&lt;/em&gt; your
isocline, well, the same thing happens but in the other direction. At the
intersection of the two isoclines is the &lt;em&gt;two-species equilibrium&lt;/em&gt;, whose
stability and accessibility tells whether coexistence is possible or not.&lt;/p&gt;

&lt;p&gt;The important thing to keep in mind is that both species will push the system
in (possibly) different directions at the same time. So there can be some funky
behavior happening, and species can cross their isoclines (this happens in the
predator-prey Lotka-Volterra model, among many others). So what matters is the
&lt;em&gt;combined&lt;/em&gt; changed in the system, which we can model through the addition of
two vectors in the &lt;script type=&quot;math/tex&quot;&gt;N_1,N_2&lt;/script&gt; plane. The first vector (which describes
the amount of change in the &lt;em&gt;vertical&lt;/em&gt; direction is &lt;script type=&quot;math/tex&quot;&gt;\vec{N_2}&lt;/script&gt;. The
norm of this vector is simply &lt;script type=&quot;math/tex&quot;&gt;\frac{d}{dt}N_2&lt;/script&gt;. The second vector, in
the &lt;em&gt;horizontal&lt;/em&gt; direction, is similarly &lt;script type=&quot;math/tex&quot;&gt;\vec{N_1}&lt;/script&gt;. The resulting
vector describing the overall change in the system is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\vec{S} = \vec{N_1} + \vec{N_2}
&lt;/script&gt;

&lt;p&gt;Because &lt;script type=&quot;math/tex&quot;&gt;\vec{N_1}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\vec{N_2}&lt;/script&gt; are orthogonal (and
because of a couple of other assumptions, such as the space defined by the
population densities being Euclidean), it’s easy to measure the Euclidean
norm of &lt;script type=&quot;math/tex&quot;&gt;\vec{S}&lt;/script&gt;, which is to say the total amount of change in
the system, as being&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\lVert \vec{S} \rVert = \sqrt{\lVert \vec{N_1} \rVert ^2 + \lVert \vec{N_2} \rVert ^2 }
&lt;/script&gt;

&lt;p&gt;At any given coordinate &lt;script type=&quot;math/tex&quot;&gt;(N_1, N_2)&lt;/script&gt;, the system will move towards
&lt;script type=&quot;math/tex&quot;&gt;(N_1 + \dot{N_1}, N_2 + \dot{N_2})&lt;/script&gt;. With both infos in hand, we
can do two things. First, we can plot the instantaneous rate of change of the
system (the absolute value of the norm of &lt;script type=&quot;math/tex&quot;&gt;\vec{S}&lt;/script&gt;) over space,
to see in which combinations of populations densities are trigerring rapid
changes. Second, we can plot the direction of change using arrows &lt;em&gt;over&lt;/em&gt;
the previous graph, to see in which direction the system is moving. The
really good thing with this method is that we only have to tell students
that we describe the cumulative change of populations on the two axes,
and leave aside all of the maths (as straightforward as they may be).&lt;/p&gt;

&lt;p&gt;So now, we can produce a graph like the one below:&lt;/p&gt;

&lt;p class=&quot;left&quot;&gt;&lt;img src=&quot;/images/isoclines.png&quot; alt=&quot;Is1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When the color is close to red, the system is moving extremely fast. When the
color is close to yellow, the system is slowing down. Each arrow gives the
direction in which the system is changing, so we can see that the outcome
of competition in this case will depend on the initial conditions. It’s
also easy to start from anywhere on the plan, and follow the direction of
the arrows. You can also zoom in, to see how the system behaves when near
the equilibrium, and investigate the effects of a small disturbance. In
this case, it’s clear that the system will move away from the equilibrium,
which is a good way to introduce the notion of local stability (after that,
it’s eigenvalues all the way down!).&lt;/p&gt;

&lt;p class=&quot;right&quot;&gt;&lt;img src=&quot;/images/isoclines_stable.png&quot; alt=&quot;Is2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The same visualization with a stable coexistence is also extremely clear. You
can easily see that all of the arrows converge towards the two-species
equilibrium, which in that case is stable. I’m not going to show the systematic
exclusion scenario, but it’s working in the same way.&lt;/p&gt;

&lt;p&gt;The good thing with this method is that (i) it’s easy to explain what isoclines
are and how to use them to see how the system will change over time, and (ii)
as it’s extremely visual, it does not rely on the students understanding the
maths &lt;em&gt;before&lt;/em&gt; they understand their ecological meaning. I have no data to
back this up, but I think it’s easier to understand the maths once you can
see the output in a visual way.&lt;/p&gt;

&lt;p&gt;The code I used is available as a &lt;a href=&quot;https://gist.github.com/tpoisot/9632093&quot;&gt;gist&lt;/a&gt;. It’s not optimized &lt;em&gt;at all&lt;/em&gt;
but it runs fast enough that optimization is not really an issue. You can
modify the &lt;code&gt;params&lt;/code&gt; array to fix values of &lt;em&gt;r&lt;/em&gt;, &lt;em&gt;K&lt;/em&gt;, and the &lt;em&gt;per capita&lt;/em&gt;
competition rates, and the code will do the rest.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Here we are. This is the way I currently explain isoclines, and the outcome of
competition in the two-species logistic growth model (and a bunch of others,
actually). It helps &lt;em&gt;me&lt;/em&gt; by making it virtually impossible to fail at the
explanation, because I can just follow the arrows. And I think it helps &lt;em&gt;the
students&lt;/em&gt; because (i) it gives a visual explanation of the phenomenon that
requires no mathematical understanding, and (ii) it’s an intuitive way to
start thinking about all the conclusions we can draw using this model.&lt;/p&gt;

&lt;p&gt;Now, if any of you have an alternative method to teach that, I’m very
interested. I think that the behavior of this model can be tricky to
explain. Yet it touches upon so many core principles in community ecology
that it’s super important that we get it right.&lt;/p&gt;

</content>
  </entry>
  
  <entry>
     <title>Is a citation a missed publication?</title>
     <link href="http://timotheepoisot.fr/2014/03/07/missed-publications-citations/"/>
    <updated>2014-03-07T00:00:00+13:00</updated>
    <content type="html">&lt;p&gt;I had a very interesting discussion/argument on twitter this afternoon, about
(basically) why data sharing is evil. Or to put it in a more balanced way,
why sharing data is a net loss of potential papers for people. Because 140
characters are not enough to make a coherent argument, I will summarize my
position here. The key disagreement with coercive data sharing policies is that
journals are requiring people to give away their data, and that any one can use
these datasets to publish papers without putting in the effort of collecting
data, so that is a loss of potential publications for the data creators. It’s
probably one of my most despicable qualities, but I don’t mind having heated
arguments over &lt;em&gt;ideas&lt;/em&gt;. Which most definely happened this afternoon…&lt;/p&gt;

&lt;p&gt;The point is, creating data &lt;em&gt;is&lt;/em&gt; hard work. I know that because, well, I
generated some on my own, through either field (my left buttock still bears
the scar of the time I sat on a sea urchin), and lab work. I know how many
hours go into making a dataset. All of these datasets are publicly available.&lt;/p&gt;

&lt;p&gt;These last years, I’ve been moving towards (mostly) numerical/computational
research. One important output of my research (other than results and papers)
is software. I know how many hours go into making good software. Coming up
with the &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1111/ele.12002/abstract&quot;&gt;measure of ecological networks beta-diversity&lt;/a&gt; was a
solid year worth of my time. Reviewing measures of ecological specificity,
and coming up with one that &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2011.00174.x/abstract&quot;&gt;behave the way we felt it had to&lt;/a&gt; was
approximately the same amount of time.&lt;/p&gt;

&lt;p&gt;That’s by no mean an optimal comparison, but it will serve: it takes an
equivalent effort to put together a dataset and a software package. Keep also
in mind that, just as some datasets keep on growing from year to year, most
high-profile pieces of ecological software are permanently being developped
and expanded. Terry McGlynn rightly pointed out that spending a summer in
the field carries a personnal cost (as in, not seeing your family), and that
doesn’t happen in software developments (though it’s often best to avoid
being in the general vincinity of anyone &lt;em&gt;debugging&lt;/em&gt; a program).&lt;/p&gt;

&lt;p&gt;Now, I won’t re-state my view on open data. You can read about it &lt;a href=&quot;http://library.queensu.ca/ojs/index.php/IEE/article/view/4632&quot;&gt;here&lt;/a&gt;
(in open access), but in science as in other domains, I’m leaning towards
the &lt;em&gt;From each according to its capacities, to each according to its needs&lt;/em&gt;
end of the spectrum (which does not mean that credit should not be given,
and I will discuss this later on).&lt;/p&gt;

&lt;p&gt;So, back to the argument. When people re-use your data, it represents a net
loss of one paper for your lab. By the same logic, everytime someone uses a
software, the developers are loosing a paper. I’m a lucky one, since so far
I’ve only “missed” a few dozen papers (and counting, since I’ve reviewed a
few other papers using some of these packages). Developpers of the big-name
R packages in ecology and evolution, or bioinformatics packages, really have
it bad, because they “missed” a few thousands of papers. But I don’t think
any software developer will see a citation to its package as a missed paper,
because it is assumed that the number of citations is the measure of impact
we need.&lt;/p&gt;

&lt;p&gt;On the other hand, many people would assume that contributing a dataset is a
reason to require authorship. I’ve had two projects fall flat last year because
using the datasets would have required adding another five co-authors. These
datasets have been last used over five years ago, and it’s unlikely the people
creating them would have done whatever we are were thinking of doing (luckily,
it all ends well, since we’ve since found open data…). And you don’t have
to look far to find people that were in the same position. And it’s always
the same argument: by publishing with my data, you’re robbing me of one paper.&lt;/p&gt;

&lt;p&gt;And this points to a very big, fundamental issue: if we all operate on the
assumption that a citation is OK to credit methodological work, but authorship
is the only appropriate credit for data collection, it reflects the fact
that methodological development is &lt;em&gt;less real&lt;/em&gt; than empirical work. That
it’s easier, that it’s less desserving of being recognized. That it’s less
&lt;em&gt;hard to do&lt;/em&gt;. In other words, that data are scarce, but methods are plenty. I
don’t think that’s true.&lt;/p&gt;

&lt;p&gt;This is the interesting point in the discussion. Many of the people I talked
to would likely find preposterous the ideas of granting co-authorship to
developers of the software they use, yet would ask to be included in papers
using the data they produced. Hence my complaints about a double standard:
methods developers should be satisfied by citations, data creators can
expect authorship.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;The problem of &lt;em&gt;getting credit&lt;/em&gt;, nonetheless, is a valid one. The underlying
issue is the perception that authorship is the &lt;em&gt;only&lt;/em&gt; type of credit you can
receive. Citations of datasets is going a long way to fix this, but it is
true that data creation is not always recognized as a research output. This
is one of the points &lt;a href=&quot;http://library.queensu.ca/ojs/index.php/IEE/article/view/4632&quot;&gt;we discuss in our open data paper&lt;/a&gt;. Yet there
are interesting opportunities to exploit. Manon (a former student) and I
&lt;a href=&quot;http://www.ecologicalprocesses.com/content/2/1/13&quot;&gt;share authorship of the paper on natural bacteria-phage networks&lt;/a&gt;,
because even though we were in charge of different sides of putting the
paper together, we had an equal contribution to it overall. Yet Manon is
lead author of the &lt;a href=&quot;http://figshare.com/articles/Phage_bacteria_networks_isolated_in_soil/696102&quot;&gt;dataset&lt;/a&gt;, because she was in charge of sampling
and isolation. So should someone publish something using this dataset (which
is going to happen soon if I’m not mistaken), it’s normal that she get the
credit for this part of the work.&lt;/p&gt;

&lt;p&gt;But the point is, we’re not going to publish anything else with this
dataset. Not that it has told everything there it can, but we’ve moved on to
other things. And so any new paper using it, is &lt;em&gt;not&lt;/em&gt; a missed opportunity. It
carries no cost for me, because chances are, I would never have written
this new paper in the first place. And the important question is, what are
the chances that someone will write the same paper you would have, with your
data? My point of view (and I understand that not everyone will share it) is:
low enough that I can share some data I intend to go back to at some point,
because they can still be used to generate interesting research &lt;em&gt;even though
I’m not the one responsible&lt;/em&gt;. That, and I’m working on the assumption that
a very small subset of people are interested/like-minded in writing the type
of papers I write; and most of them are good collaborators already; and you
don’t scoop your close colleagues. And I think it’s true for a lot of people.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;So a citation to your dataset, in my mind, is not a missed publication. Rather,
it’s the opposite. Data which are not shared are a net loss of potential
impact. They could be generating citations. They could be used by people to
get interested in what you do. One of the most important piece of advice
that was ever given to me, and which is why I’m so into open science now,
is &lt;em&gt;Anything that people can’t see, doesn’t count&lt;/em&gt;. I’d much rather see
whatever few datasets I was involved in creating be used, rather than
collecting dust somewhere.&lt;/p&gt;

&lt;p&gt;Are there situations in which people will use your data, and you won’t be on
the paper? Definitely. But there are also situations in which putting your
work out there will result in invitations to contribute to a paper? Yes. And
it goes both ways. I’ve been invited to contribute to papers relying on my
methods, I’ve been inviting people to contribute to papers because their data
are central in what I’m trying to do. This is one of the key assumptions I
make: most people are nice and good natured, no one is out there to steal
your papers or your work, don’t be afraid.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;To summarize my key points: expecting citations in exchange for software and
authorship in exchange for data sends a really negative message to use code
monkeys (erm, researchers interested in methodological developments). It’s
worth considering whether someone can write the same paper as you, and if
the answer is “Unlikely”, then it’s probably safe to release your data. And
finally, try to see the world through rose-tinted glasses; despite pressure
to publish, and well, pressure in general, people are nice, love nothing
more than working together to create beautiful interesting science, and the
total is more than the sum of its individualistic parts - or so I believe.&lt;/p&gt;

</content>
  </entry>
  
  <entry>
     <title>Ecologists needs to focus more on patterns</title>
     <link href="http://timotheepoisot.fr/2014/03/05/ecology-needs-more-patterns/"/>
    <updated>2014-03-05T00:00:00+13:00</updated>
    <content type="html">&lt;p&gt;Ecology is the study, which is to say the identification, description, and
reporting, of patterns. Patterns are simply defined as there being something
rather than nothing, which is to say something that can represented as a
histogram or a scatterplot, or even just called a pattern because this helps
relate it to previously described patterns.&lt;/p&gt;

&lt;p&gt;Sub-fields of ecology are primarily defined by which types of
patterns they look for. Functional ecologists are interested in traits
distribution patterns. Macroecologists look for species distribution
patterns. Biogeographers do that too, but they focus on slightly
different patterns of species distributions. Food web ecologists looks for
network structure patterns. Community ecologists are interested in niche
differentiation patterns. Some ecologists, allegedly, are into the study of
mechanism; this is not as un-orthodox as it seems, for what are mechanisms,
if not processes giving rise to patterns? Their study is therefore, if not
entirely excusable, understandable as an interesting intellectual passtime
for those unlucky enough not to have access to big databases.&lt;/p&gt;

&lt;p class=&quot;left&quot;&gt;&lt;img src=&quot;/images/satyr.jpg&quot; alt=&quot;Satire&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But I digress. There is an issue we should collectively adress, and it relates
to the search for meta-patterns. For example, several species distribution
patterns are similar, and so we ought to recognize this observation for what
it is: there is a pattern of species distribution patterns. The same holds for
other types of patterns. The next breakthrough in ecology will probably come
from the identification of these meta-patterns. My personnal opinion is that
this breakthrough has been delayed because so many things that are patterns
are called something else, thus preventing unification of the vocabulary.&lt;/p&gt;

&lt;p&gt;I would even say that our remarkable self-restraint in using the p-word
is bringing much technical confusion to the litterature. Although this is
surely of interest to statisticians, most of us should not be too concerned
by whether the reported pattern is a &lt;em&gt;distribution&lt;/em&gt;, a &lt;em&gt;relationship&lt;/em&gt;, or
something else entirely. We would greatly simplify our writing by calling
all of these by their proper denomination: &lt;em&gt;significant patterns&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;This would, as I proposed above, bring conceptual unification to ecology
by allowing us to gloss over the tedious mathematical details. For example,
the search for patterns of species distributions patterns now becomes the much
more ecologier search for significant patterns of significant species patterns
patterns. And of course, as it is timely to dismiss the use of &lt;em&gt;p&lt;/em&gt;-values as
an old tradition slowly going away, “significant” is always assumed because
we are obviously above doing unsignificant research. No longer will one feel
excluded because the precise nuance of each term is sometimes difficult to
grasp. To some extent, we are about to prove Lawton wrong. Community ecology
is not a mess. We made it a mess by refusing to recognize that everything
is a pattern, and much like in the Tower of Babel analogy, we made our field
tumble and fall because we used a meaningless variety of highly specific terms.&lt;/p&gt;

&lt;p&gt;But conceptual unification of the field through the elimination of superfluous
words (words such as, for instance, superfluous) serves a very limited purpose
if it won’t allow to create bridges with other fields. Evolutionary ecology,
because it studies the rise of patterns through evolution, can be re-branded
as the study of the emergence of complex patterns. It is admitted that all
patterns are complex, as if they were not, being smart people with diplomas,
we would have found about them already.&lt;/p&gt;

&lt;p&gt;So there it is; ecology is built upon patterns, and in the future, it should
be patterns all the way down. The divide between theoreticians and empiricists
can, at this point, only be bridged by having a common language, in which
anything worth mentionning is refered to as a pattern. Some would call this
language poorer. They would be wrong. Pauperisation is when each word only
holds a small amount of meaning; this drives the division of ecology in
obscure sub-fields unable to communicate. By refocusing our research efforts
on patterns, and patterns of patterns, we will be able to re-unify our field.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;A final (and serious) note&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In case this was not obvious by the overall absurdity, and the picture
of an eminently Rococo &lt;em&gt;Satyr&lt;/em&gt; (get it?), this is meant to be a satirical
piece. In case you wonder, yes, it felt good to write it, I got rid of so much
frustration. Now for the serious part. I have a feeling that the p-word is
used to mean a &lt;em&gt;lot&lt;/em&gt; of things. I played a game at a meeting the other day:
some of the speakers clocked in at over 1.5 ppm, which is to say “patterns
per minute”. It would have been the most dangerous drinking game. Removing
stop-words, &lt;em&gt;i.e.&lt;/em&gt; extremely frequent words regardless of the topic discussed,
I’m sure &lt;em&gt;pattern&lt;/em&gt; was &lt;strong&gt;the&lt;/strong&gt; most frequently used term.&lt;/p&gt;

&lt;p&gt;From my own observations, &lt;em&gt;pattern&lt;/em&gt; is used to mean either &lt;em&gt;distribution&lt;/em&gt;
(in the statistical sense), &lt;em&gt;relationship&lt;/em&gt; (often statistical, sometimes
mechanistic), and other times either to describe something that we can’t
quite qualify. In some cases, &lt;em&gt;patterns&lt;/em&gt; is superfluous (also, I don’t think
we should get rid of superfluous), and can be removed entirely without loss
of meaning - I would argue that &lt;em&gt;species distribution pattern&lt;/em&gt; is a perfect
example of the later. Actually, there is at least one valid sentence in
my first paragraph: &lt;em&gt;pattern&lt;/em&gt; means something, rather than nothing; but it
fails to provide any information about the &lt;em&gt;nature&lt;/em&gt; of this thing.&lt;/p&gt;

&lt;p&gt;Mark Twain said that young writers should write &lt;em&gt;damn&lt;/em&gt; every time they would
have written &lt;em&gt;very&lt;/em&gt;; the editor would then remove &lt;em&gt;damn&lt;/em&gt;, and the text will
not be diminished. For a few months, I had a macro replacing &lt;em&gt;pattern&lt;/em&gt; by
&lt;em&gt;stuff&lt;/em&gt;, which I think is equally informative. As a result, I stopped using
&lt;em&gt;pattern&lt;/em&gt; (almost) entirely, and I think it improved my writing. Notably,
it forced me to pick the right word, instead of letting the reader figure
out what I meant as an exercise.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>Of the value of datasets and methods in open science</title>
     <link href="http://timotheepoisot.fr/2014/03/04/open-datasets-methods/"/>
    <updated>2014-03-04T00:00:00+13:00</updated>
    <content type="html">&lt;p&gt;The interesting thing about predictions is that sometimes, they come true. Last
week, I was discussing the need to do advocacy about open data; specifically,
the need to do it fast, because one day, a big journal will release a new
policy, and we are going to see the closest thing to mass hysteria that the
academic world can do. If I was so good at predicting species interactions,
I’d be a much happier ecologist.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;PLoS&lt;/em&gt; journals &lt;a href=&quot;http://blogs.plos.org/everyone/2014/02/24/plos-new-data-policy-public-access-data/&quot;&gt;announced a new data release policy&lt;/a&gt;, which is now
effective. People &lt;a href=&quot;http://rxnm.wordpress.com/2014/02/25/fan-fiction/&quot;&gt;freaked out&lt;/a&gt;. Some effectively stated that data sharing encourages scientific &lt;a href=&quot;http://smallpondscience.com/2014/03/03/i-own-my-data-until-i-dont/&quot;&gt;freeloading&lt;/a&gt;, and I only link to the later because there is an interesting point made in the comments:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I suspect that you’re fully aware that the sharing of R programming has
some fundamental differences with the sharing of original scientific data. The
existence of R itself is a medium for community sharing. It’s like the NEON
or LTER but for data analysis, designed as a public resource. I didn’t design
my experiments for their data to be a public resource, except for the findings.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;See, that’s where I lost it. I’m a methods developer. Some of my papers
I like the most are built on the idea of finding the best way to measure
something, or comparing and understanding different measures, and wrapping
things up in a &lt;code&gt;R&lt;/code&gt; package or &lt;code&gt;python&lt;/code&gt; script, which are made public, and
that I spend an insane amount of time troubleshooting through email because
I care about enabling people to use what I do. Yet in two sentences, my work
is robbed of all its meaning because, well, of course it ought to be public,
it’s public service and people developing methods only do so to serve people
generating data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;I agree with that&lt;/strong&gt;. The most enjoyable thing for me is to face an empirical
problem that reveals a gap in our methodology, and then filling a notepad
with sketches and equations until I come up with a new way of measuring old
things. And I’m an ecologist. My interest is in understanding ecological
systems, and I choose to do so with a heavy component of methodological
development. I’m glad when people show interest in what I’ve done, which
is why I spend time replying to emails on technical questions, writing code
for people, and trying to understand why things behave the way they do.&lt;/p&gt;

&lt;p&gt;Here’s what I don’t agree with: the sense of entitlement. The above excerpt
(and most of the post it comes from) is, I think, the opinion of a minority,
but it should be vocally opposed nonetheless. I don’t believe I am entitled
to other peoples’ data. I don’t know anyone who does. Yet some people feel
that they are entitled to new methods, because there are apparently on one
side nerds with glasses and computers (I plead guilty), and on the other
side “real” ecologists with muddy boots and field trips. And saying that
methods are supposed to be given away for free, while data should be released
whenever you damn well please send a very clear message about which side is
the best one, about which side is privileged. So I don’t take lightly to my
work being belittled as community service, because I would never say that
all empiricists are good for is taking data for me to develop new methods with.&lt;/p&gt;

&lt;p&gt;Not only is that attitude counterproductive, it’s re-inforcing the
divide between theory and empiricism in biology at large, and ecology
in particular. I think it’s high time that we recognize that datasets,
methods, ideas, and what have you, are all ingredients in a research
paper. Initiatives like &lt;em&gt;ImpactStory&lt;/em&gt; are helping in that direction. I’m
convinced that dialogue between everyone, and meaningful collaborations
between theoretical and empirical scientists are the way to go, and will
produce better science. Ultimately, it will also help each of these aspects
of doing research being recognized, as I discussed in my talk at the &lt;a href=&quot;http://figshare.com/articles/Open_science_Make_your_work_known_/877095&quot;&gt;QCBS
panel on open ecology&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So in short: &lt;strong&gt;research would gain to be open&lt;/strong&gt;. I want to believe that no one
is out there to &lt;em&gt;steal&lt;/em&gt; the work of others. Re-use is probably the highest form
of recognition you can get, because it means that what you have &lt;em&gt;done&lt;/em&gt; (not
what you have written about it) allowed other people to build something new. And we’re all doing different things, but also complementary things. As ecologists, we &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1034/j.1600-0706.2000.910101.x/abstract&quot;&gt;ought to know that it should be beneficial for productivity&lt;/a&gt;…&lt;/p&gt;

</content>
  </entry>
  
  <entry>
     <title>Responsibility and sensationalism in high-profile papers</title>
     <link href="http://timotheepoisot.fr/2014/01/25/responsibility-high-impact/"/>
    <updated>2014-01-25T00:00:00+13:00</updated>
    <content type="html">&lt;p&gt;Last week, we had an inter-lab journal club during which we discussed
a very polemic paper in ecology. The discussion quickly turned into the
responsibility of people at the various levels of the scientific publishing
process, and the risks associated with counter-mainstream results appearing
in high-profile journals. Most of what follows is my recollection of the
different arguments we raised for and against self-censorship in scientific
publishing. Which paper we discussed is not relevant for the rest of this
post, so I’ll try to summarize the issue in extremely broad terms. There is a
well-known relationship between concept &lt;em&gt;B&lt;/em&gt; and property &lt;em&gt;C&lt;/em&gt;. Changes in &lt;em&gt;B&lt;/em&gt;
are expected, under some circumstances, to result in changes in &lt;em&gt;C&lt;/em&gt;. &lt;em&gt;B&lt;/em&gt;
is a complex thing with several layers to it. A set of studies showed no
variation of &lt;em&gt;A&lt;/em&gt;, which is a component of &lt;em&gt;B&lt;/em&gt;, and so it seems to question
the relationship between &lt;em&gt;B&lt;/em&gt; and &lt;em&gt;C&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;For different reasons, whether there is (i) a change in &lt;em&gt;A&lt;/em&gt;, and (ii)
a link between &lt;em&gt;B&lt;/em&gt; and &lt;em&gt;C&lt;/em&gt;, is a pretty big deal when it comes to policy
making. “Proving” that the relationship is not general, or not important,
or even wrong, is susceptible to have real-world consequences. In addition,
it would be a big deal for theoretical science as well, because it will mean
that what we thought (and still think) of an important concept in ecology,
is wrong and need to be re-built. But it would also mean that, as far as
policy makers are concerned, we can persist in several “business as usual”
practices; after all, there are now data to back them up!&lt;/p&gt;

&lt;p&gt;So, should we practice self-censorship? Absolutely not. The “cost” of making
all information public has been particularly discussed in the context of
bio-terrorism; understanding the risks requires results to be published, but
publishing results can increase &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/books/NBK98397/&quot;&gt;the ease to produce bioweapons&lt;/a&gt;. Some
argue that &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1353/hcr.2007.0046/abstract&quot;&gt;not all results should be published&lt;/a&gt;, but determining
which should, and which should not, is a tricky issue. Some ecological results
have very strong relevance for policy-making (and lobbies), and there is a
risk that some will have a political reading of some papers.&lt;/p&gt;

&lt;p&gt;The question is, therefore, who is responsible for “customer service”&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;
once a new result appears? First thing first, the authors. I have the
feeling that a perverse effect of the publish-or-perish mentality is a
quest for sensationalism (which seems to be confirmed by this &lt;a href=&quot;http://nsaunders.wordpress.com/2013/07/16/interestingly-the-sentence-adverbs-of-pubmed-central/&quot;&gt;analysis of
adverbs used in PMC papers&lt;/a&gt;, where more abstracts emphasize interest,
surprise, and novelty). And when we (scientists) read a paper making bold
claims, we are able to rationalize them because it’s not the first paper we
are reading. We are able to weigh one result against dozens of others. But
my very personal and humble opinion is that, when the result we report is
likely to make a splash, we have to consider whether the way we express it
is likely to be mistaken by the public.&lt;/p&gt;

&lt;p&gt;Second, editors have a role to play. There is a well documented relationship
between &lt;a href=&quot;http://retractionwatch.com/2011/08/11/is-it-time-for-a-retraction-index/&quot;&gt;impact factor and likelihood of retractation&lt;/a&gt;, and a tendency
for the media to cover &lt;a href=&quot;http://www.plosone.org/article/info:doi%2F10.1371%2Fjournal.pone.0085355&quot;&gt;not the best conducted studies&lt;/a&gt; (perhaps,
has we hypothesized, because most science is normal, and normal science is
very rarely cool for people outside the field). It would make sense that the
boxes with comments from the editor, that seem to appear in more and more
journals, be dedicated to putting sensational papers into the broader context.&lt;/p&gt;

&lt;p&gt;And finally, science journalists. One of the things that never fails
to amaze/annoy me is that, when a vastly mediatised paper ends up being
retracted, there is almost never any indication of that in the media (our
short attention span be damned). Which means that unless you specifically
follow people discussing the issue, do you know that bacteria are still
unable to use arsenic in their DNA? I get that saying “Nope, turned out
we were pretty much right about how life works” is unlikely to make the
headlines (unless you want to, you know, &lt;em&gt;inform&lt;/em&gt;). As this post &lt;a href=&quot;http://scienceblogs.com/notrocketscience/2009/07/04/does-science-journalism-falter-or-flourish-under-embargo/&quot;&gt;really
clearly states&lt;/a&gt;, the ability to react &lt;em&gt;right now&lt;/em&gt; on a big story is
important for the career of journalists. It lead to (in my mind) absurd
situations in which journalists can prepare their copy before the majority
of the scientific community knows the results.&lt;/p&gt;

&lt;p&gt;As a final note, I have nothing against sensationalism. If anything (if
it is backed by facts and sound interpretation), sensational ideas and
results are a great way to make people interested in science. Telling
a story about the population dynamics of an emblematic animal makes
for better communication than the subtleties of a stochastic, spatially
explicit generalized Lotka-Volterra model (somehow). But everything even
remotely close to &lt;a href=&quot;http://en.wikipedia.org/wiki/Post-normal_science&quot;&gt;post-normal science&lt;/a&gt; should be written carefully,
contextualized carefully, and reported on carefully.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;I forgot who came up with the “customer service of scientific results” idea, but I find it brilliant… &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
  </entry>
  
  <entry>
     <title>Should we tell students that communities don't exist?</title>
     <link href="http://timotheepoisot.fr/2014/01/21/teaching-ecological-communities/"/>
    <updated>2014-01-21T00:00:00+13:00</updated>
    <content type="html">&lt;p&gt;Literally one of the first things I told my students in the first lecture
of the &lt;em&gt;Population and Community Dynamics&lt;/em&gt; class I gave last winter was
along the lines of &lt;em&gt;an ecological community is the pool of all species
occuring at the same place and at the same time (and the interactions between
them)&lt;/em&gt;. This is more or a less a synthesis of what you can find in the first
pages of Peter Morin’s textbook, for example; and so far, I was relatively
satisfied by this definition. Then a few weeks ago, I read the @lea14 paper in
&lt;em&gt;Ecology&lt;/em&gt;, which uses co-occurrence data from marine species to question the
“classical” definition of a community. Their most important finding is that,
when clustering species in a large spatial dataset, there are groups of highly
correlated species. The interesting thing is that these groups are not unique
to any particular location, and are not necessarily found together. Or in
short, “communities” are a mix between the two definitions of “whatever is
found locally” and “the most characteristic species of an assemblage”.&lt;/p&gt;

&lt;p&gt;As I was writing this post, &lt;a href=&quot;http://evol-eco.blogspot.ca/2014/01/a-multiplicity-of-communities-for.html&quot;&gt;Caroline Tucker&lt;/a&gt; wrote a very interesting
piece on the &lt;em&gt;multiplicity of ecological communities&lt;/em&gt;. Which you should read,
because it covers a lot of the ground I was planning on covering (and I had to
delete a few paragraphs). Let’s just say I am glad that the first time I get
scooped is for a blog post and not for a paper. So just go read Caroline’s
piece, but then come back, because I will try to put another twist on the
subject. Specifically, should we adapt our message when teaching?&lt;/p&gt;

&lt;p&gt;The end of Caroline’s piece is that we should not try to have a
one-size-fits-none definition of what an ecological community is. I thought
it was the only significant weakness of Mark Vellend’s otherwise excellent
synthesis [@vel10] from a few years ago: a definition of a community which
is as simple as the one I gave in the introduction of this post. Defining
communities this way has one distinct advantage: it is straightforward to
go from community to meta-communities (&lt;em&gt;a bunch of communities connected by
the dispersal of at least one of their species&lt;/em&gt;). But the important point
is that the world is most likely not discrete, and the elegant framework of
meta-everything is probably not going to hold in its entirety in continuous
landscapes.&lt;/p&gt;

&lt;p&gt;So looking back, I am thinking that the nice tale we are telling students is
(i) probably not relevant at the local scale, because it considers that the
qaudrat we work on is a perfectly isolated island, and (ii) probably not
relevant at the regional scale either, because it does not expands nicely to
a continuous spatial context. What should we tell then? I think it would be
an interesting discussion, to present the “simple” definition of a community
to students, and get them to understand when it breaks. How do you define the
area you consider to define a community? What if the environment is always
moving (like lagoons, intertidal zones, …)? What if you started walking,
from your sampling point, in the same direction; when do you leave the zone
“defined” by your community?&lt;/p&gt;

&lt;p&gt;Another interesting approach would be to compare this definition to classical
large scale experimental systems. Looking at the list of the &lt;em&gt;greatest&lt;/em&gt;
ecological experiments proposed by &lt;a href=&quot;http://dynamicecology.wordpress.com/2014/01/13/whats-the-greatest-ecology-experiment-in-history/&quot;&gt;Jeremy Fox and in the comments&lt;/a&gt;,
or just showing pictures of Huffaker’s oranges, or the Adirondack lakes, on
any microcosm setup, should lead to the realization that most of these systems
conform to the discrete theory (counterpoint: &lt;em&gt;Barro Colorado Island&lt;/em&gt;). Coming
back to the paper by Leaper &lt;em&gt;et al.&lt;/em&gt;, it is actually hardly surprising that
marine biologists are once more pushing the ideas that discrete communities
may make very little sense: dispersion is cheap and easy in the oceans, and for
some functional groups, it is entirely passive [@mcm12 propose nice evidence that this mode of dispersal will “assemble” consistent communities]. In short, there is a tremendous opportunity to engage with students on the
relationship between &lt;em&gt;what&lt;/em&gt; we study, &lt;em&gt;how&lt;/em&gt; we study it, and how it can
&lt;em&gt;shape&lt;/em&gt; the theoretical background we develop to make sense of observations.&lt;/p&gt;

</content>
  </entry>
  
  <entry>
     <title>Semantic versioning for scientific software</title>
     <link href="http://timotheepoisot.fr/2014/01/20/semantic-versioning/"/>
    <updated>2014-01-20T00:00:00+13:00</updated>
    <content type="html">&lt;p&gt;I have recently discovered the concept of &lt;a href=&quot;http://semver.org/&quot;&gt;semantic versioning&lt;/a&gt;,
introduced by Tom Preston-Werner. In short, semantic versioning is a
way to know whether two versions of a program will behave in the same
way. A program conforming to this specification has a version number of
&lt;code&gt;major&lt;/code&gt;.&lt;code&gt;minor&lt;/code&gt;.&lt;code&gt;patch&lt;/code&gt;, and comparing two version numbers will tell you
exactly what to expect in terms of compatibility. Versions with different
&lt;code&gt;major&lt;/code&gt; numbers are not supposed to be compatible. A version with a higher
&lt;code&gt;minor&lt;/code&gt; number will have additional, backwards compatible functions. Different
levels of &lt;code&gt;patch&lt;/code&gt; will correspond to issues solving and so forth.&lt;/p&gt;

&lt;p&gt;Semantic versioning is intended for programs declaring a public API, as
all &lt;code&gt;R&lt;/code&gt; packages do (the various functions and data formats). Any package,
module, etc., declaring functions that will be used in user-written scripts
are within the scope of semantic versioning. A script that you wrote using
v. &lt;code&gt;1.2.3&lt;/code&gt; will work when &lt;code&gt;1.3.1&lt;/code&gt; or &lt;code&gt;1.2.4&lt;/code&gt; are released, but &lt;em&gt;not&lt;/em&gt; when
&lt;code&gt;2.0.0&lt;/code&gt; is. An important point is that, unless &lt;code&gt;major&lt;/code&gt; is &lt;code&gt;1&lt;/code&gt;, the software
is considered unstable: the API can change very rapidly, and so it should
not be used for production.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://yihui.name/en/2013/06/r-package-versioning/&quot;&gt;Yihui Xie&lt;/a&gt; proposes a slightly different set of rules, specifically
aimed at &lt;code&gt;R&lt;/code&gt; packages. When the &lt;code&gt;minor&lt;/code&gt; version increases, release the new
version of your package to &lt;code&gt;CRAN&lt;/code&gt; (as &lt;code&gt;patch&lt;/code&gt; versions are not supposed to
introduce new features anyway). People can just save time by referring to the
&lt;code&gt;major&lt;/code&gt;.&lt;code&gt;minor&lt;/code&gt; version of the package. The way semantic versioning works,
you will only submit &lt;code&gt;major&lt;/code&gt;.&lt;code&gt;minor&lt;/code&gt;.&lt;code&gt;0&lt;/code&gt; versions to &lt;code&gt;CRAN&lt;/code&gt; (because each
time a number increases, all numbers to the right are set to &lt;code&gt;0&lt;/code&gt;). And so
any version with &lt;code&gt;patch&lt;/code&gt; higher than &lt;code&gt;0&lt;/code&gt; is a &lt;em&gt;development&lt;/em&gt; version.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;What is the relevance of that for people not contributing to software
development? I think using semantic versioning is useful if we &lt;em&gt;report&lt;/em&gt;
version numbers in the papers in the same way. If you distribute supplementary
files with your paper, such as &lt;code&gt;R&lt;/code&gt; scripts, don’t give the package name,
but also the version of the package you used. This can be viewed using
&lt;code&gt;sessionInfo()&lt;/code&gt;. If you use package &lt;code&gt;foo&lt;/code&gt;, v. &lt;code&gt;1.3.0&lt;/code&gt;, and the maintainer of
package &lt;code&gt;foo&lt;/code&gt; uses semantic versioning, if someone tries to use your script
a few years later with &lt;code&gt;foo&lt;/code&gt; v. &lt;code&gt;2.1.0&lt;/code&gt;, it is &lt;em&gt;possible&lt;/em&gt; that things will
break. If you clearly state which packages versions are used, it will be easier
for users to install the correct version of the packages to run your analyses.&lt;/p&gt;

&lt;p&gt;Correct reporting of software versions is important (and I’ve been guilty of
not using top-notch practices myself), but if an increasing number of people
start using semantic versioning, it will &lt;em&gt;help&lt;/em&gt; re-use of code. In short,
we should always give the full version number when mentioning a package
or software.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;As a final note, this also require the those of us producing code adopt
better development practices. In &lt;em&gt;GitHub&lt;/em&gt;, it is easy to tag releases,
so that all different versions are available over time. And because the
ultimate goal of semantic versioning is to ensure backwards compatibility,
we need to be sure that new fixes or additions do not break it. This can
be done by writing thorough test suites. Unless a new addition results in
previously passing tests failing, it can be considered safe.&lt;/p&gt;

</content>
  </entry>
  
  <entry>
     <title>Python wrapper for mangal</title>
     <link href="http://timotheepoisot.fr/2014/01/19/pymangal/"/>
    <updated>2014-01-19T00:00:00+13:00</updated>
    <content type="html">&lt;p&gt;I use &lt;code&gt;python&lt;/code&gt; more than I use &lt;code&gt;R&lt;/code&gt; for my own job. &lt;code&gt;R&lt;/code&gt; is good to share
things with people, but it’s a little bit slower, and orders of magnitude
quirkier, than &lt;code&gt;python&lt;/code&gt;. So I decided to write a &lt;code&gt;python&lt;/code&gt; wrapper for the
&lt;code&gt;mangal&lt;/code&gt; API, which you can &lt;a href=&quot;https://github.com/mangal-wg/pymangal&quot;&gt;find on &lt;em&gt;GitHub&lt;/em&gt;&lt;/a&gt;. It works with
&lt;code&gt;python&lt;/code&gt; 2.6 and 2.7 (at least the test suite passes on these two versions).&lt;/p&gt;

&lt;p&gt;The goal is &lt;em&gt;not&lt;/em&gt; to release something as full-featured as the &lt;code&gt;rmangal&lt;/code&gt;
package (though the core abilities will be here). Rather, I wanted a tool that
is “pythonic”, easy to use, and programmed extremely defensively so that it
can be used for automated analyses. There is a minimal &lt;a href=&quot;http://pymangal.readthedocs.org/en/latest/&quot;&gt;documentation&lt;/a&gt;
online, which will hopefully get better over time as I understand the
&lt;code&gt;sphinx&lt;/code&gt; markup.&lt;/p&gt;

&lt;p&gt;The syntax is relatively similar to that of the &lt;code&gt;rmangal&lt;/code&gt; package. Everything starts by the instansiation of a &lt;code&gt;mangal()&lt;/code&gt; class.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; import pymangal as pm
&amp;gt;&amp;gt;&amp;gt; api = pm.mangal(url=&#39;http://localhost:8000&#39;)
&amp;gt;&amp;gt;&amp;gt; api.resources
[u&#39;interaction&#39;, u&#39;network&#39;, u&#39;reference&#39;, u&#39;taxa&#39;, u&#39;trait&#39;, u&#39;dataset&#39;, u&#39;environment&#39;, u&#39;item&#39;, u&#39;user&#39;, u&#39;population&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The methods implemented by the &lt;code&gt;mangal&lt;/code&gt; class are &lt;code&gt;List&lt;/code&gt;, &lt;code&gt;Get&lt;/code&gt;, and &lt;code&gt;Post&lt;/code&gt;
(&lt;code&gt;Patch&lt;/code&gt; is coming soon, and &lt;code&gt;Post&lt;/code&gt; support is sketchy at best). They work
almost in the same way as their &lt;code&gt;R&lt;/code&gt; counterparts. For example, getting a
list of all networks can be done with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; all_net = api.List(&#39;network&#39;)
&amp;gt;&amp;gt;&amp;gt; len(all_net)
17
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The only significant difference is that, by default, the &lt;code&gt;python&lt;/code&gt; version will only return the first 20 results. If you want more than 20  results, you need to use &lt;code&gt;autopage=True&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; twenty_taxa = api.List(&#39;taxa&#39;)
&amp;gt;&amp;gt;&amp;gt; len(twenty_taxa)
20
&amp;gt;&amp;gt;&amp;gt; all_taxa = api.List(&#39;taxa&#39;, autopage=True)
&amp;gt;&amp;gt;&amp;gt; len(all_taxa)
38
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;em&gt;big&lt;/em&gt; advantage of the &lt;code&gt;python&lt;/code&gt; version is that, when uploading data, there
is a validation of the way they are formatted. Internally, there is a function
taking care of reading the API scheme, and returning it as a &lt;code&gt;json&lt;/code&gt; schema,
against which data are checked. It’s probably simpler with a demonstration…&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; print json.dumps(api.schemes[&#39;trait&#39;], indent=2)
## I have cut some of the elements for the sake of brevity
{
   &quot;$schema&quot;: &quot;http://json-schema.org/draft-04/schema#&quot;, 
   &quot;required&quot;: [
      &quot;value&quot;, 
      &quot;owner&quot;, 
      &quot;name&quot;
   ], 
   &quot;type&quot;: &quot;object&quot;, 
   &quot;properties&quot;: {
      &quot;name&quot;: {
         &quot;type&quot;: &quot;string&quot;, 
         &quot;description&quot;: &quot;The name of the measured trait&quot;
      }, 
      &quot;value&quot;: {
         &quot;type&quot;: &quot;string&quot;, 
         &quot;description&quot;: &quot;The value of the trait&quot;
      }, 
      &quot;units&quot;: {
         &quot;type&quot;: &quot;string&quot;, 
         &quot;description&quot;: &quot;Units in which the trait was measured&quot;
      }
   }, 
   &quot;title&quot;: &quot;Autogenerated JSON schema&quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The data are validated &lt;em&gt;automatically&lt;/em&gt;, so you don’t have to bother about checking them yourself. For example, let’s try with a badly formatted, and with a correctly formatted, taxa &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; bad_taxa = {&#39;name&#39;: &#39;Vulpes lagopus&#39;, &#39;vernacular&#39;: &#39;arctic fox&#39;, &#39;eol&#39;: 1053894, &#39;status&#39;: &#39;valid&#39;}
# &quot;valid&quot; is not an acceptable value for &quot;status&quot; (&quot;confirmed&quot; is)
&amp;gt;&amp;gt;&amp;gt; ok_taxa = {&#39;name&#39;: &#39;Vulpes lagopus&#39;, &#39;vernacular&#39;: &#39;arctic fox&#39;, &#39;eol&#39;: 1053894, &#39;status&#39;: &#39;confirmed&#39;}

&amp;gt;&amp;gt;&amp;gt; arctic_fox = 
&amp;gt;&amp;gt;&amp;gt; arctic_fox = api.Post(&#39;taxa&#39;, bad_taxa)
# Skipping some lines...
jsonschema.exceptions.ValidationError: &#39;valid&#39; is not one of [u&#39;confirmed&#39;, u&#39;trophic species&#39;, u&#39;morphospecies&#39;, u&#39;nomen dubium&#39;, u&#39;nomen oblitum&#39;, u&#39;nomen nudum&#39;, u&#39;nomen novum&#39;, u&#39;nomen conservandum&#39;, u&#39;species inquirenda&#39;]

Failed validating &#39;enum&#39; in schema[&#39;properties&#39;][u&#39;status&#39;]:
{&#39;description&#39;: u&#39;The taxonomic status of the taxa.&#39;,
   &#39;enum&#39;: [u&#39;confirmed&#39;,
   u&#39;trophic species&#39;,
   u&#39;morphospecies&#39;,
   u&#39;nomen dubium&#39;,
   u&#39;nomen oblitum&#39;,
   u&#39;nomen nudum&#39;,
   u&#39;nomen novum&#39;,
   u&#39;nomen conservandum&#39;,
   u&#39;species inquirenda&#39;],
&#39;type&#39;: u&#39;string&#39;}

On instance[u&#39;status&#39;]:
&#39;valid&#39;

&amp;gt;&amp;gt;&amp;gt; arctic_fox = api.Post(&#39;taxa&#39;, ok_taxa)
&amp;gt;&amp;gt;&amp;gt; arctic_fox[&#39;id&#39;]
103
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that the &lt;code&gt;validate&lt;/code&gt; function of the &lt;code&gt;jsonschema&lt;/code&gt; will tell you what is
wrong with the data, and show you the relevant excerpt from the schema. This
really helps finding out what it wrong with the data you try to upload.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;As a final note: I am using &lt;em&gt;&lt;a href=&quot;https://coveralls.io/r/mangal-wg/pymangal&quot;&gt;Coveralls.io&lt;/a&gt;&lt;/em&gt; for the first time,
and I am extremely impressed. In short, &lt;em&gt;coveralls&lt;/em&gt; will read the results
of the &lt;code&gt;coverage&lt;/code&gt; command, and tell you the proportion of your code with
corresponding test cases. &lt;em&gt;Coveralls&lt;/em&gt; is integrated with &lt;em&gt;TravisCI&lt;/em&gt;, a
continuous integration engine runnign the test suite each time I commit
changes to the &lt;em&gt;GitHub&lt;/em&gt; repository (so I have confirmation that the new
versions pass  the tests). And finally, the documentation is auto-generated
using &lt;em&gt;Read The Docs&lt;/em&gt;, which extracts the informations directly from the
code. These are really impressive tools to work with, and they kind of force
good practices upon you – which is a good thing.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;As usual, posting and patching requires that you have a username and password. There will be a way to register through the &lt;code&gt;python&lt;/code&gt; package in the future. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
  </entry>
  
  <entry>
     <title>These aren't the networks you're looking for</title>
     <link href="http://timotheepoisot.fr/2014/01/14/filtering-in-mangal/"/>
    <updated>2014-01-14T00:00:00+13:00</updated>
    <content type="html">&lt;p&gt;The development of the &lt;code&gt;rmangal&lt;/code&gt; package is making good progress. I just
finised implementing a way to filter results, which will be extremely useful
when the number of taxa and networks will start growing. Before we get into
how filtering is done, I’d like to take the opportunity to bore everyone
with technical details. &lt;code&gt;mangal&lt;/code&gt; (the API) relies on &lt;code&gt;tastypie&lt;/code&gt;, which is a
&lt;code&gt;django&lt;/code&gt; plugin to use the data models as API resources (bored yet?). The
&lt;code&gt;tastypie&lt;/code&gt; developers had the good taste to allow &lt;code&gt;django&lt;/code&gt;’s ORM filter as
URL parameters to filter results. Or in short, there is a filtering solution
working out of the box.&lt;/p&gt;

&lt;p&gt;The filtering is realtively easy to do, as it follows the common pattern:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;?field__relation=target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For example, to look for taxa whose name starts with &lt;em&gt;Lamellodiscus&lt;/em&gt;, we
just need to append:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;?name__startswith=Lamellodiscus
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;the-basics-of-filtering&quot;&gt;The basics of filtering&lt;/h1&gt;

&lt;p&gt;So how does it works in the &lt;code&gt;R&lt;/code&gt; package? There is a &lt;code&gt;mangalSearch&lt;/code&gt; function,
which takes as arguments the &lt;code&gt;api&lt;/code&gt;, the type of resource to look for (the
list of resources can be now be viewed with &lt;code&gt;api$resources&lt;/code&gt;), and a list of
filters. The filters are themselves &lt;code&gt;list&lt;/code&gt; objects, with three properties:
&lt;code&gt;field&lt;/code&gt;, &lt;code&gt;relation&lt;/code&gt; (I give an overview of these below), and &lt;code&gt;target&lt;/code&gt;. Let
me illustrate:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;library(rmangal)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Loading required package: rjson
## Loading required package: httr
## Loading required package: igraph
## Loading required package: stringr
## Loading required package: cheddar
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;api &amp;lt;- mangalapi(&quot;http://localhost:8000/&quot;)
Amphiprion &amp;lt;- mangalSearch(api, &quot;taxa&quot;, list(list(field = &quot;name&quot;, relation = &quot;startswith&quot;, 
    target = &quot;Amphiprion&quot;)))
laply(Amphiprion, function(x) x$name)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &quot;Amphiprion clarkii&quot;      &quot;Amphiprion perideraion&quot; 
## [3] &quot;Amphiprion ocellaris&quot;    &quot;Amphiprion sandaracinos&quot;
## [5] &quot;Amphiprion melanopus&quot;    &quot;Amphiprion polymnus&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It may look like a convoluted way to do a simple thing, &lt;em&gt;but&lt;/em&gt; the interest
is that you can combine filters to have more precise requests. Let’s say we
are really interested in a small portion of the pacific ocean, and want to
know if there are any networks inside. We will define two different filters:
one for latitude, and one for longitude.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;filter_lat = list(field = &quot;latitude&quot;, relation = &quot;range&quot;, target = c(1.756, 
    1.813))
filter_lon = list(field = &quot;longitude&quot;, relation = &quot;range&quot;, target = c(124.76, 
    124.808))
Networks &amp;lt;- mangalSearch(api, &quot;network&quot;, list(filter_lat, filter_lon))
laply(Networks, function(x) x$name)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &quot;Batu Kapal&quot;  &quot;Jalan Masuk&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;types-of-relations&quot;&gt;Types of relations&lt;/h1&gt;

&lt;p&gt;There are ten different values for the &lt;code&gt;relation&lt;/code&gt; type. &lt;code&gt;starswith&lt;/code&gt; and
&lt;code&gt;endswith&lt;/code&gt; will match the beginnign and end of the value. &lt;code&gt;exact&lt;/code&gt; will match
the entirety of the field. &lt;code&gt;contains&lt;/code&gt; will look for &lt;code&gt;target&lt;/code&gt; somewhere in
the field. &lt;code&gt;in&lt;/code&gt; will return objects that have &lt;code&gt;target&lt;/code&gt; in the values of a
multi-valued field (example of use: with the identifier of an interactio,
find the network it belongs to). &lt;code&gt;range&lt;/code&gt; will return all objects that have
values in the &lt;code&gt;target&lt;/code&gt; range. And finally, &lt;code&gt;gte&lt;/code&gt;, &lt;code&gt;gt&lt;/code&gt;, &lt;code&gt;lte&lt;/code&gt; and &lt;code&gt;lt&lt;/code&gt;,
handle superior/inferior (or equal) relationships.&lt;/p&gt;

&lt;h1 id=&quot;more-advanced-usage&quot;&gt;More advanced usage&lt;/h1&gt;

&lt;p&gt;If you look at the code of &lt;code&gt;mangalSearch&lt;/code&gt;, you’ll see that it passes
an addition argument (&lt;code&gt;filtering&lt;/code&gt;) to &lt;code&gt;mangalList&lt;/code&gt;. This allows you
to handle more complicated filtering schemes. In the filtering pattern
&lt;code&gt;field__relation=target&lt;/code&gt;, &lt;code&gt;field&lt;/code&gt; can actually be a &lt;em&gt;path&lt;/em&gt; across multiple
objects. Let’s illustrate with an example.&lt;/p&gt;

&lt;p&gt;We want to retrieve all interactions involving &lt;code&gt;taxa&lt;/code&gt; whose name starts with
&lt;code&gt;Amphiprion&lt;/code&gt;. The &lt;em&gt;path&lt;/em&gt; from &lt;code&gt;interaction&lt;/code&gt; to &lt;code&gt;taxa&lt;/code&gt; goes through the field &lt;code&gt;taxa_to&lt;/code&gt;/&lt;code&gt;taxa_from&lt;/code&gt; in interaction. So what we want to do is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;?taxa_from__name__startswith=Amphiprion
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can add that as the &lt;code&gt;filtering&lt;/code&gt; argument of &lt;code&gt;mangalList&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;Interactions &amp;lt;- rmangal:::mangalList(api, &quot;interaction&quot;, filtering = &quot;taxa_from__name__startswith=Amphiprion&quot;)
laply(Interactions, function(x) x$id)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##  [1] &quot;93&quot;  &quot;92&quot;  &quot;87&quot;  &quot;86&quot;  &quot;85&quot;  &quot;82&quot;  &quot;81&quot;  &quot;76&quot;  &quot;75&quot;  &quot;69&quot;  &quot;68&quot; 
## [12] &quot;67&quot;  &quot;59&quot;  &quot;58&quot;  &quot;53&quot;  &quot;52&quot;  &quot;51&quot;  &quot;50&quot;  &quot;45&quot;  &quot;44&quot;  &quot;23&quot;  &quot;24&quot; 
## [23] &quot;25&quot;  &quot;26&quot;  &quot;27&quot;  &quot;28&quot;  &quot;29&quot;  &quot;30&quot;  &quot;31&quot;  &quot;32&quot;  &quot;33&quot;  &quot;34&quot;  &quot;35&quot; 
## [34] &quot;36&quot;  &quot;37&quot;  &quot;38&quot;  &quot;39&quot;  &quot;41&quot;  &quot;42&quot;  &quot;43&quot;  &quot;44&quot;  &quot;45&quot;  &quot;46&quot;  &quot;47&quot; 
## [45] &quot;48&quot;  &quot;49&quot;  &quot;50&quot;  &quot;51&quot;  &quot;52&quot;  &quot;53&quot;  &quot;54&quot;  &quot;56&quot;  &quot;58&quot;  &quot;59&quot;  &quot;60&quot; 
## [56] &quot;61&quot;  &quot;62&quot;  &quot;64&quot;  &quot;65&quot;  &quot;66&quot;  &quot;67&quot;  &quot;68&quot;  &quot;69&quot;  &quot;71&quot;  &quot;72&quot;  &quot;73&quot; 
## [67] &quot;75&quot;  &quot;76&quot;  &quot;77&quot;  &quot;78&quot;  &quot;79&quot;  &quot;81&quot;  &quot;82&quot;  &quot;83&quot;  &quot;84&quot;  &quot;85&quot;  &quot;86&quot; 
## [78] &quot;87&quot;  &quot;88&quot;  &quot;89&quot;  &quot;90&quot;  &quot;91&quot;  &quot;92&quot;  &quot;93&quot;  &quot;95&quot;  &quot;96&quot;  &quot;97&quot;  &quot;98&quot; 
## [89] &quot;99&quot;  &quot;100&quot; &quot;101&quot; &quot;102&quot; &quot;103&quot; &quot;104&quot; &quot;105&quot; &quot;106&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because &lt;code&gt;mangalList&lt;/code&gt; is not exported, you have to use the &lt;code&gt;:::&lt;/code&gt; operator
to explicitely call it. Note that these functions may not work (quite yet)
with the public database, because we will update the API later this week.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>Animate networks with igraph</title>
     <link href="http://timotheepoisot.fr/2014/01/11/animate-networks-with-igraph/"/>
    <updated>2014-01-11T00:00:00+13:00</updated>
    <content type="html">&lt;p&gt;The &lt;code&gt;igraph&lt;/code&gt; package in &lt;code&gt;R&lt;/code&gt; is a good solution to manipulate networks. A
few days ago, I had to produce an animated figure for a paper, and I really
wanted to use &lt;code&gt;igraph&lt;/code&gt; to do it. As it turns out, it’s not overly complicated
(you need to have &lt;code&gt;imagemagick&lt;/code&gt; installed, though.&lt;/p&gt;

&lt;p&gt;So as to provide a quick visualisation, let’s start with a simple dynamical
model of a diamond food web (one primary producer, two consumers, one top
predator, classic stuff). The “usual” way to represent the dynamics of biomass
is to have four time series. The “captivating” way (good for talks!) is to
animate the network, with each node size being proportional to its biomass
at any time.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;library(igraph)
library(simecol)

# A simple LV diamond food web model
glv &amp;lt;- function(time, init, parms) {
    N &amp;lt;- init
    with(as.list(parms), {
        d1 = N[1] * (rp - (a1p * N[2] + a2p * N[3]))
        d2 = N[2] * (rc + a1p * N[1] - ap1 * N[4])
        d3 = N[3] * (rc + a2p * N[1] - ap2 * N[4])
        d4 = N[4] * (rt + ap1 * N[3] + ap1 * N[2])
        return(list(c(d1, d2, d3, d4)))
    })
}

# Some default parameters
params &amp;lt;- c(rp = 1.1, rc = -0.2, rt = -0.6, a1p = 0.8, a2p = 0.4, ap1 = 0.21, 
    ap2 = 0.12)
init &amp;lt;- c(10, 4, 3, 5)
times &amp;lt;- seq(from = 0, to = 50, length = 300)

# And the simulations...
output &amp;lt;- ode(init, times, func = glv, parms = params)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nevermind that the parameters aren’t realistic at all, it’s not what matters
for this example. At this point, we have a simulation running for 50 timesteps,
with a total of 300 points. Now, let’s build a graph:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;adjacency &amp;lt;- matrix(c(0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0), 4, 4)
diamond &amp;lt;- graph.adjacency(adjacency)
layout &amp;lt;- layout.fruchterman.reingold(diamond)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can start building a loop to create all networks for each time. The trick is
that &lt;code&gt;igraph&lt;/code&gt; accepts vectors for allmost all edges and vertex attributes. It
can be used to change the color, size, label, etc. An important point is that,
by default, the layout of each plot is scaled and center. But in our case,
it’s important that it’s fixed for all plots, so we’ll just need to add
&lt;code&gt;rescale = FALSE&lt;/code&gt;, and give explicit &lt;code&gt;xlim&lt;/code&gt; and &lt;code&gt;ylim&lt;/code&gt;; this way, the center
of each vertex is always at the same position.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;# We&#39;ll make sure that the maximal biomass is not too big
MaxBio &amp;lt;- max(output[, -1])
png(file = &quot;biomass_%04d.png&quot;, width = 300, height = 300)

for (i in c(1:nrow(output))) {
    Biomass &amp;lt;- output[i, -1]
    plot(diamond, layout = layout, vertex.size = 10 * Biomass, rescale = FALSE, 
        xlim = range(layout[, 1]), ylim = range(layout[, 2]))
}
dev.off()

# The we use ImageMagick to convert all images to a gif file
system(&quot;convert -delay 5 biomass_*.png animation_biomass.gif&quot;)
# And remove the images
file.remove(list.files(pattern = &quot;biomass_&quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And here is the result:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/rfig/animation_biomass.gif&quot; alt=&quot;Animation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Clearly it’s not extremely good looking, but it can help show the dynamics
of networks. An interesting thing is that you can also &lt;em&gt;hide&lt;/em&gt; some edges or
vertices, so it’s possible to show temporal dynamics.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>Examples of mangal in action</title>
     <link href="http://timotheepoisot.fr/2014/01/09/mangal-in-action/"/>
    <updated>2014-01-09T00:00:00+13:00</updated>
    <content type="html">&lt;p&gt;In the two previous posts (see &lt;a href=&quot;http://timotheepoisot.fr/2014/01/07/mangal-database-networks/&quot;&gt;Part 1&lt;/a&gt; and &lt;a href=&quot;http://timotheepoisot.fr/2014/01/08/mangal-add-data/&quot;&gt;Part 2&lt;/a&gt;), I’ve presented the basics of using &lt;code&gt;mangal&lt;/code&gt; and the &lt;code&gt;rmangal&lt;/code&gt; package to access, deposit, and edit data about ecological networks. The final post in the series is going to be slightly more fun: I’ll illustrate some use cases of things you can potentially do with the database. We will start from the representation of a network in space, then move on to measuring beta-diversity in the dataset, and end with some more classical species-links relationships.&lt;/p&gt;

&lt;h1 id=&quot;setting-things-up&quot;&gt;Setting things up&lt;/h1&gt;

&lt;p&gt;We’ll need to pull the latest version of &lt;code&gt;rmangal&lt;/code&gt;, as usual. In addition, we’ll need a few other packages.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;options(stringsAsFactors = FALSE)
if (getOption(&quot;unzip&quot;) == &quot;&quot;) options(unzip = &quot;unzip&quot;)
library(devtools)
install_github(&quot;rmangal&quot;, &quot;mangal-wg&quot;)
install_github(&quot;betalink&quot;, &quot;tpoisot&quot;)
library(rmangal)
# Let&#39;s connect to the database
URL &amp;lt;- &quot;http://localhost:8000&quot;
api &amp;lt;- rmangal::mangalapi(URL)
library(betalink)
# Tools for spatial analyses
library(sp)
library(RgoogleMaps)
library(RColorBrewer)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;span class=&quot;margin&quot;&gt;Ricciardi et al. (2010) &lt;em&gt;Env Biol Fishes&lt;/em&gt; &lt;a href=&quot;http://dx.doi.org/10.1007/s10641-010-9606-0&quot;&gt;DOI&lt;/a&gt;&lt;/span&gt;I will do everything using a dataset released by Ricciardi and colleagues. The &lt;a href=&quot;http://www.nceas.ucsb.edu/interactionweb/html/ricciardi-et-al-2010.html&quot;&gt;original data&lt;/a&gt; are on the &lt;em&gt;Interaction Web DataBase&lt;/em&gt;. This dataset describes 16 anemonefish/fishes mutualistic networks in Indonesia. It’s an extremely cool dataset, because (i) there are a lot of sites in a small space, (ii) a lot of species are in common across sites, and (iii) anemonefishes. So I’ve uploaded these data in my local copy of the database, and I’m going to illustrate a few uses-cases. The only “manual” operation was determining the spatial coordinates in each networks, based on the map presented in the original paper.&lt;/p&gt;

&lt;h1 id=&quot;use-case-1-spatialized-network&quot;&gt;Use-case 1: spatialized network&lt;/h1&gt;

&lt;p&gt;&lt;span class=&quot;margin&quot;&gt;Bascompte (2009) &lt;em&gt;Science&lt;/em&gt; &lt;a href=&quot;http://dx.doi.org/10.1126/science.1170749&quot;&gt;DOI&lt;/a&gt;&lt;/span&gt;There is a really cool figure in one of Jordi Bascompte’s paper, in which a network is plotted in space, and the position of each species is the center of mass of its area. This is an interesting way to plot networks when spatial information is available, so let’s do just that.&lt;/p&gt;

&lt;p&gt;We start by getting the dataset:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;Dataset &amp;lt;- getDataset(api, 8)
Dataset$networks
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##  [1] &quot;32&quot; &quot;33&quot; &quot;18&quot; &quot;19&quot; &quot;20&quot; &quot;21&quot; &quot;22&quot; &quot;23&quot; &quot;24&quot; &quot;25&quot; &quot;26&quot; &quot;27&quot; &quot;28&quot; &quot;29&quot;
## [15] &quot;30&quot; &quot;31&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, this dataset has a lot of networks. So we’ll do to things. First, we will get a list of all network objects, because they have the metadata we need:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;Networks &amp;lt;- alply(Dataset$networks, 1, function(x) getNetwork(api, x))
Networks[[1]]$latitude
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 1.651
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, we will call the &lt;code&gt;network_as_graph&lt;/code&gt; function to get all the interactions, and associated informations on the taxa. We will also convert these graphs into matrices, because that will be easier for some of the next steps.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;Graphs &amp;lt;- llply(Networks, function(x) network_as_graph(api, x$id))
names(Graphs) &amp;lt;- laply(Networks, function(x) x$name)
Matrices &amp;lt;- llply(Graphs, get.adjacency, sparse = FALSE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let’s now get a table with the sites spatial coordinates:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;Coord &amp;lt;- ldply(Networks, function(x) c(name = x$name, lon = x$longitude, lat = x$latitude))
rownames(Coord) &amp;lt;- Coord$name
Coord &amp;lt;- Coord[, c(&quot;lat&quot;, &quot;lon&quot;)]
Coord$lat &amp;lt;- as.numeric(Coord$lat)
Coord$lon &amp;lt;- as.numeric(Coord$lon)
head(Coord)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##                 lat   lon
## Tanjung Kopi  1.651 124.7
## Tanjung Pisok 1.578 124.8
## bahowo        1.585 124.8
## Kampung Bajo  1.608 124.9
## Batu Kapal    1.788 124.8
## Bualo         1.616 124.7
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Good!&lt;/p&gt;

&lt;p&gt;Almost final steps, let’s (i) get the list of species in each site, (ii) make a community matrix from this, and (iii) calculate the center of mass of each species based on its occurence in each of the 16 locations. &lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;# Get the list of species
sp_by_site &amp;lt;- llply(Graphs, function(x) unlist(V(x)$name))
sp_list &amp;lt;- unique(unlist(sp_by_site))

# Species-by-site matrix
M &amp;lt;- matrix(0, ncol = length(sp_list), nrow = length(sp_by_site))
colnames(M) &amp;lt;- sp_list
rownames(M) &amp;lt;- names(sp_by_site)
for (site in c(1:length(sp_by_site))) M[names(sp_by_site)[site], sp_by_site[[site]]] = 1

# Get the center of mass for each species
sp_center &amp;lt;- adply(M, 2, function(x) colMeans(Coord[names(x)[x &amp;gt; 0], ]))
rownames(sp_center) &amp;lt;- sp_center[, 1]
sp_center &amp;lt;- sp_center[, -1]
head(sp_center)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##                           lat   lon
## Entacmaea quadricolor   1.599 124.8
## Heteractis aurora       1.677 124.8
## Heteractis crispa       1.614 124.8
## Stichodactyla mertensii 1.625 124.8
## Heteractis magnifica    1.601 124.8
## Amphiprion melanopus    1.620 124.7
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, we use the &lt;code&gt;metaweb&lt;/code&gt; function from &lt;code&gt;betalink&lt;/code&gt; to get the regional network, as an &lt;code&gt;igraph&lt;/code&gt; graph:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;Mw &amp;lt;- graph.adjacency(metaweb(Matrices)$web)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And now, we’ll use the &lt;code&gt;RgoogleMaps&lt;/code&gt; package to plot a map, and overlay the spatialized network on top:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;# But first we need this function to convert spatial coordinates
ll_to_gm &amp;lt;- function(Map, ll) {
    # Lat. conversion
    lat &amp;lt;- ll$lat
    lat &amp;lt;- lat - Map$lat.center
    lat &amp;lt;- lat/(Map$BBOX$ur[, &quot;lat&quot;] - Map$BBOX$ll[, &quot;lat&quot;])
    lat &amp;lt;- lat * 640
    # Lat. conversion
    lon &amp;lt;- ll$lon
    lon &amp;lt;- lon - Map$lon.center
    lon &amp;lt;- lon/(Map$BBOX$ur[, &quot;lon&quot;] - Map$BBOX$ll[, &quot;lon&quot;])
    lon &amp;lt;- lon * 640
    ## Return matrix
    return(cbind(lon, lat))
}
# Plot a map
center_point &amp;lt;- colMeans(sp_center)
Map &amp;lt;- GetMap(center = center_point, zoom = 11, SCALE = 1)
par(pty = &quot;s&quot;)
colors &amp;lt;- c(brewer.pal(9, &quot;Set1&quot;), brewer.pal(8, &quot;Set2&quot;))
PlotOnStaticMap(Map)
plot(Mw, layout = jitter(ll_to_gm(Map, sp_center), amount = 10), rescale = FALSE, 
    add = TRUE, vertex.size = 600, vertex.label = NA, vertex.color = colors, 
    edge.arrow.size = 0.25, edge.color = 1)
legend(&quot;bottomleft&quot;, fill = colors, legend = V(Mw)$name, inset = 0.02, cex = 0.7, 
    bty = &quot;n&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/rfig/mangal_make_map_network.png&quot; alt=&quot;plot of chunk mangal_make_map_network&quot; /&gt; &lt;/p&gt;

&lt;p&gt;And here we are! It might not be the best-looking graph ever, but I was fairly surprised that it was so easy to produce.&lt;/p&gt;

&lt;h1 id=&quot;use-case-2-network-beta-diversity&quot;&gt;Use-case 2: network β-diversity&lt;/h1&gt;

&lt;p&gt;For use-case number two, I will use the functions in &lt;code&gt;betalink&lt;/code&gt; to do a quick illustration of how network β-diversity depends on the distance between two networks. In the first use case, we have created a &lt;code&gt;Matrices&lt;/code&gt; objects, which is a list of matrices. So we can simply do:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;bdiv &amp;lt;- betalink.dist(Matrices)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We’ll add the spatial distance between sites, using a function from the &lt;code&gt;sp&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;GeoDist &amp;lt;- spDists(as.matrix(Coord), longlat = TRUE)
colnames(GeoDist) &amp;lt;- rownames(GeoDist) &amp;lt;- rownames(Coord)
bdiv$Space &amp;lt;- as.dist(GeoDist)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And now, some plots:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;par(mfcol = c(1, 2))
with(bdiv, {
    plot(Space, WN, pch = 19, xlab = &quot;Distance (in km.)&quot;, ylab = &quot;Network dissim. (all species)&quot;)
    plot(Space, OS, pch = 19, xlab = &quot;Distance (in km.)&quot;, ylab = &quot;Network dissim. (common species)&quot;)
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/rfig/betadiv_rmangal_plots.png&quot; alt=&quot;plot of chunk betadiv_rmangal_plots&quot; /&gt; &lt;/p&gt;

&lt;p&gt;And here is a cool new mini-result: the dissimilarity of networks increases with distance overal, but not when you only account for the species which are shared between two locations.&lt;/p&gt;

&lt;h1 id=&quot;use-case-3-link-species-relationships&quot;&gt;Use-case 3: link-species relationships&lt;/h1&gt;

&lt;p&gt;Let’s end with a more classical example: in this system, what is the relationship between number of species (S), and number of interactions (L)? This information is super easy to get with a little bit of &lt;code&gt;plyr&lt;/code&gt; magic:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;LS &amp;lt;- ldply(Graphs, function(x) c(S = length(V(x)), L = length(E(x))))
head(LS)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##             .id  S  L
## 1  Tanjung Kopi 10  8
## 2 Tanjung Pisok  9  7
## 3        bahowo 11 10
## 4  Kampung Bajo  6  4
## 5    Batu Kapal  5  4
## 6         Bualo  9  8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The only thing left is to plot this table:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;plot(LS[, c(2, 3)], log = &quot;xy&quot;, xlab = &quot;Species&quot;, ylab = &quot;Links&quot;, pch = 19)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/rfig/mangal_demo_ls_png.png&quot; alt=&quot;plot of chunk mangal_demo_ls.png&quot; /&gt; &lt;/p&gt;

&lt;p&gt;We find the expect positive, log/log relationship. And it only required three lines of code!&lt;/p&gt;

&lt;h1 id=&quot;to-conclude&quot;&gt;To conclude&lt;/h1&gt;

&lt;p&gt;These three use cases are (I think) good examples of what can be done with the &lt;code&gt;rmangal&lt;/code&gt; package. I’ll refine them a little bit to add in the paper. If you have nay any suggestions, that would be greatly appreciated as well!&lt;/p&gt;

</content>
  </entry>
  
  <entry>
     <title>Uploading data into mangal</title>
     <link href="http://timotheepoisot.fr/2014/01/08/mangal-add-data/"/>
    <updated>2014-01-08T00:00:00+13:00</updated>
    <content type="html">&lt;p&gt;In the first part, I introduced &lt;code&gt;mangal&lt;/code&gt;, the database and associated &lt;code&gt;R&lt;/code&gt; package to interact with ecological networks. In the second part, I’ll give an overview of how to upload and curate your data. In the last part, we used functions starting with &lt;code&gt;get&lt;/code&gt; and &lt;code&gt;list&lt;/code&gt;. This time, we will use the &lt;code&gt;add&lt;/code&gt; and &lt;code&gt;patch&lt;/code&gt; functions, as they allow to change resouces on the server. There are a few important informations to have before we start.&lt;/p&gt;

&lt;p&gt;First, working on the server is &lt;em&gt;for real&lt;/em&gt;, so please be careful. But take the opportunity of the testing phase to try to break the database in new, innovative ways! Second, the licence under which the data are released is the &lt;em&gt;CC-0&lt;/em&gt; waiver. In short, when data are uploaded in the database, they are frelly available, for all to see and use. Almost all data sharing services use this license for data. Finally, the database is &lt;em&gt;add-only&lt;/em&gt;, which means that you can’t remove data (but I can, in case things go exceptionally wrong).&lt;/p&gt;

&lt;p&gt;In this post, we’ll see (i) how to create an account on the database, (ii)
how to create and populate a dataset, and (iii) how to modify the interactions
after sending them. You are of coure invited to try similar things. Note
that the database won’t accept objects with duplicated names, so if you try
to re-upload some of the taxa in this example, it most likely won’t work.&lt;/p&gt;

&lt;h1 id=&quot;setting-things-up&quot;&gt;Setting things up&lt;/h1&gt;

&lt;p&gt;As always, we’ll start by getting the latest release of &lt;code&gt;rmangal&lt;/code&gt;. And we’ll also get the latest release of &lt;code&gt;taxize&lt;/code&gt;, because there is a really neat trick I want to show you (in short: automated curation).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;options(stringsAsFactors = FALSE)
if (getOption(&quot;unzip&quot;) == &quot;&quot;) options(unzip = &quot;unzip&quot;)
library(devtools)
install_github(&quot;rmangal&quot;, &quot;mangal-wg&quot;)
library(rmangal)
install_github(&quot;taxize&quot;, &quot;ropensci&quot;)
library(taxize)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;signing-up&quot;&gt;Signing-up&lt;/h1&gt;

&lt;p&gt;Uploading and curating data require that you are logged in. The reason is simple: each object in the database has an &lt;code&gt;owner&lt;/code&gt; property, and this owner becomes you every time you modify an object. Signing up can be done directly from &lt;code&gt;R&lt;/code&gt;: &lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;api &amp;lt;- mangalapi()
me &amp;lt;- signUp(api, &quot;my_user_name&quot;, &quot;my_password&quot;)
# This line will log you with your newly acquired identifiers!
api &amp;lt;- mangalapi(usr = me$username, pwd = me$password)
me
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## $email
## [1] &quot;&quot;
## 
## $first_name
## [1] &quot;&quot;
## 
## $id
## [1] 3
## 
## $last_name
## [1] &quot;&quot;
## 
## $password
## [1] &quot;my_password&quot;
## 
## $username
## [1] &quot;my_user_name&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Before we start, let’s add some personal informations to your profile:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;me$first_name &amp;lt;- &quot;Fake&quot;
me$last_name &amp;lt;- &quot;People&quot;
me$email &amp;lt;- &quot;me@fake.people&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we just need to update this information on the server:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;me &amp;lt;- patchUser(api, me)
me
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## $email
## [1] &quot;me@fake.people&quot;
## 
## $first_name
## [1] &quot;Fake&quot;
## 
## $id
## [1] 3
## 
## $last_name
## [1] &quot;People&quot;
## 
## $password
## [1] &quot;my_password&quot;
## 
## $username
## [1] &quot;my_user_name&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next time you want to work on data, you’ll just need to start your session by&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;api &amp;lt;- mangalapi(URL, usr = &quot;my_username&quot;, pwd = &quot;my_password&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;putting-data-on-the-database&quot;&gt;Putting data on the database&lt;/h1&gt;

&lt;p&gt;In the previous part, I may have mentionned that the usual way to get a network is to start at the &lt;code&gt;dataset&lt;/code&gt; level, then go all the way down to the &lt;code&gt;taxa&lt;/code&gt;. When uploading data, you need to follow the opposite direction. Let’s say that we want to send data about what (probably) happens in the &lt;em&gt;Isle Royale National Park&lt;/em&gt;: wolves eat mooses, mooses eat balsam fir.&lt;/p&gt;

&lt;p&gt;As for &lt;code&gt;get&lt;/code&gt; and &lt;code&gt;list&lt;/code&gt;, the functions to modify data follow a single naming convention: either &lt;code&gt;patch&lt;/code&gt; or &lt;code&gt;add&lt;/code&gt;, and the name of the resource with its first letter capitalized.&lt;/p&gt;

&lt;h2 id=&quot;creating-taxa&quot;&gt;Creating taxa&lt;/h2&gt;

&lt;p&gt;The first thing we need to do is create a few &lt;code&gt;taxa&lt;/code&gt; objects. In case you don’t remember how a &lt;code&gt;taxa&lt;/code&gt; is formatted, you can call&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;whatIs(api, &quot;taxa&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##         field                                        help    type  null
## 1        bold             The BOLD identifier of the taxa integer  TRUE
## 2 description             A short description of the taxa  string  TRUE
## 3        gbif             The GBIF identifier of the taxa integer  TRUE
## 5        itis             The ITIS identifier of the taxa integer  TRUE
## 6        name             The scientific name of the taxa  string FALSE
## 7        ncbi    The NCBI Taxonomy identifier of the taxa integer  TRUE
## 9  vernacular The vernacular name of the taxa, in English  string FALSE
##   unique values
## 1   TRUE       
## 2  FALSE       
## 3   TRUE       
## 5   TRUE       
## 6   TRUE       
## 7   TRUE       
## 9  FALSE
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So we’ll want to create three taxa:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;moose &amp;lt;- list(name = &quot;Alces americanus&quot;, vernacular = &quot;American moose&quot;)
wolf &amp;lt;- list(name = &quot;Canis lupus&quot;, vernacular = &quot;Gray wolf&quot;)
balsam &amp;lt;- list(name = &quot;Abies balsamea&quot;, vernacular = &quot;Balsam fir&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, let’s put them in the database:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;wolf &amp;lt;- addTaxa(api, wolf)
moose &amp;lt;- addTaxa(api, moose)
balsam &amp;lt;- addTaxa(api, balsam)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can check that our taxa are indeed in the database with&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;getTaxa(api, wolf$id)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## $bold
## NULL
## 
## $description
## NULL
## 
## $gbif
## NULL
## 
## $id
## [1] &quot;20&quot;
## 
## $itis
## NULL
## 
## $name
## [1] &quot;Canis lupus&quot;
## 
## $ncbi
## NULL
## 
## $owner
## [1] &quot;my_user_name&quot;
## 
## $vernacular
## [1] &quot;Gray wolf&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;creating-interactions&quot;&gt;Creating interactions&lt;/h2&gt;

&lt;p&gt;We will now create interactions between these taxa. Once more, have a look at &lt;code&gt;whatIs(api, &#39;interaction&#39;)&lt;/code&gt;, and in particular pay attention to the &lt;code&gt;values&lt;/code&gt; column of the &lt;code&gt;ecotype&lt;/code&gt; field. It tells you that &lt;code&gt;ecotype&lt;/code&gt; (the type of ecological interaction betwee two organisms), can only be one of:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;strsplit(subset(whatIs(api, &quot;interaction&quot;), field == &quot;ecotype&quot;)$values, &quot;, &quot;)[[1]]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##  [1] &quot;predation&quot;                 &quot;ectoparasitism&quot;           
##  [3] &quot;endoparasitism&quot;            &quot;intra-cellular parasitism&quot;
##  [5] &quot;parasitoidism&quot;             &quot;mycoheterotrophy&quot;         
##  [7] &quot;antixenosis&quot;               &quot;teletoxy&quot;                 
##  [9] &quot;amensalism&quot;                &quot;antibiosis&quot;               
## [11] &quot;allelopathy&quot;               &quot;competition&quot;              
## [13] &quot;facilitation&quot;              &quot;refuge creation&quot;          
## [15] &quot;inquilinism&quot;               &quot;phoresis&quot;                 
## [17] &quot;epibiosis&quot;                 &quot;pollination&quot;              
## [19] &quot;mutualistic symbiosis&quot;     &quot;zoochory&quot;                 
## [21] &quot;mutual protection&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that this list of terms will probably increase in the future. In any case, we have enough informations to start writing our interactions:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;w_e_m &amp;lt;- list(taxa_from = wolf, taxa_to = moose, ecotype = &quot;predation&quot;)
m_e_b &amp;lt;- list(taxa_from = moose, taxa_to = balsam, ecotype = &quot;herbivory&quot;)
w_e_m &amp;lt;- addInteraction(api, w_e_m)
m_e_b &amp;lt;- addInteraction(api, m_e_b)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Congratulations! Note that within the &lt;code&gt;R&lt;/code&gt; package, it’s perfectly acceptable to pass the whole &lt;code&gt;taxa&lt;/code&gt; object (but if you want to interact with the API, you need to pass only the URI, and if you want to interact directly with the API, chances are you knew that already…).&lt;/p&gt;

&lt;h2 id=&quot;wrapping-things-up&quot;&gt;Wrapping things up&lt;/h2&gt;

&lt;p&gt;At this point, we’re almost done. We simply need to wrap our interactions in a &lt;code&gt;network&lt;/code&gt; object:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;isle_royale &amp;lt;- addNetwork(api, list(name = &quot;Isle Royale National Park&quot;, description = &quot;Or at least a simplified version of it&quot;, 
    interactions = list(w_e_m, m_e_b), metaweb = TRUE))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;metaweb&lt;/code&gt; attribute comes from our &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/22994257&quot;&gt;paper on network beta-diversity&lt;/a&gt;. If you are reporting regionally observed or infered interactions, then it is &lt;code&gt;TRUE&lt;/code&gt;. if you have been sitting in the field looking at stuff, then it’s &lt;code&gt;FALSE&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;And finally, even if we only have one network, we will publish it as a dataset:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;ir_dataset &amp;lt;- addDataset(api, list(name = &quot;North-American Terrestrial food webs&quot;, 
    networks = list(isle_royale)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And we’re done!&lt;/p&gt;

&lt;h1 id=&quot;modifying-data&quot;&gt;Modifying data&lt;/h1&gt;

&lt;p&gt;In this part, we will go through the different ways to alter data already in the database.&lt;/p&gt;

&lt;h2 id=&quot;adding-some-attributes&quot;&gt;Adding some attributes&lt;/h2&gt;

&lt;p&gt;Now, let’s add some informations to our data. First, we have told almost nothing about where our network is in space. We’ll pull it from the database, and add the &lt;code&gt;latitude&lt;/code&gt; and &lt;code&gt;longitude&lt;/code&gt; attribute:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;isle_royale &amp;lt;- getNetwork(api, isle_royale$id)
isle_royale$latitude &amp;lt;- 48.015
isle_royale$longitude &amp;lt;- -88.831
isle_royale &amp;lt;- patchNetwork(api, isle_royale)
isle_royale
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## $date
## NULL
## 
## $description
## [1] &quot;Or at least a simplified version of it&quot;
## 
## $environment
## list()
## 
## $id
## [1] &quot;3&quot;
## 
## $interactions
## [1] &quot;18&quot; &quot;19&quot;
## 
## $latitude
## [1] &quot;48.015&quot;
## 
## $longitude
## [1] &quot;-88.831&quot;
## 
## $metaweb
## [1] TRUE
## 
## $name
## [1] &quot;Isle Royale National Park&quot;
## 
## $owner
## [1] &quot;my_user_name&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because the &lt;code&gt;ir_dataset&lt;/code&gt; dataset contains a &lt;em&gt;reference&lt;/em&gt; to the object we just modified, there is no need to alter it in any way.&lt;/p&gt;

&lt;p&gt;At this point, you can have a look at &lt;code&gt;http://mangal.uqar.ca/data/&lt;/code&gt;, and on the map, there should be a little dot somewhere between Ontario and Wisconsin, representing the network.&lt;/p&gt;

&lt;h2 id=&quot;adding-new-relations&quot;&gt;Adding new relations&lt;/h2&gt;

&lt;p&gt;Let’s say that we want to provide a short bibliography along with the data. An important paper on this system is &lt;em&gt;The Rise and Fall of Isle Royale Wolves&lt;/em&gt;, by Peterson &amp;amp; Page. We know the DOI of this article (&lt;code&gt;10.2307/1381751&lt;/code&gt;), so we’ll just add a reference to the dataset. Datasets have &lt;code&gt;papers&lt;/code&gt; and &lt;code&gt;data&lt;/code&gt; fields, that point to &lt;code&gt;reference&lt;/code&gt; objects. First, we will create a reference (again, look at &lt;code&gt;whatIs(api, &#39;reference&#39;)&lt;/code&gt; to see the possible field (and beware, there is a typo, the &lt;code&gt;jstorid&lt;/code&gt; field is called &lt;code&gt;jsonid&lt;/code&gt;, I will fix that really soon). &lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;peterson_n_page &amp;lt;- addReference(api, list(doi = &quot;10.2307/1381751&quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, we need to add the reference to the &lt;code&gt;papers&lt;/code&gt; field of the &lt;code&gt;dataset&lt;/code&gt; object:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;ir_dataset$papers &amp;lt;- list(peterson_n_page)
ir_dataset &amp;lt;- patchDataset(api, ir_dataset)
ir_dataset
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## $data
## list()
## 
## $description
## NULL
## 
## $id
## [1] &quot;2&quot;
## 
## $name
## [1] &quot;North-American Terrestrial food webs&quot;
## 
## $networks
## [1] &quot;3&quot;
## 
## $owner
## [1] &quot;my_user_name&quot;
## 
## $papers
## [1] &quot;1&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And if you go to &lt;code&gt;http://mangal.uqar.ca/data/dataset/&amp;lt;id&amp;gt;/&lt;/code&gt; (where &lt;code&gt;&amp;lt;id&amp;gt;&lt;/code&gt; is whatever number is in the &lt;code&gt;id&lt;/code&gt; field of the dataset), you will see a list of the references.&lt;/p&gt;

&lt;h2 id=&quot;using-the-power-of-open-science-for-good&quot;&gt;Using the power of open science for good&lt;/h2&gt;

&lt;p&gt;When describing the taxa, we only gave the latin and vernacular names. It’s good, but if we want to really take advantage of all the great tools we have, we will need a little more. One solution is to do a bit of googling, and just copy/paste the taxonomic identifiers and patch the taxa this way. Another solution is to use &lt;code&gt;taxize&lt;/code&gt;. &lt;code&gt;taxize&lt;/code&gt; has a function to get the NCBI identifiers from the latin name, which is perfect for our case:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;for (tax in list(wolf, moose, balsam)) {
    if (is.null(tax$ncbi)) {
        identifier &amp;lt;- get_uid(tax$name, ask = FALSE)
        if (!is.na(identifier)) {
            tax$ncbi &amp;lt;- identifier[1]
            patchTaxa(api, tax)
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## 
## Retrieving data for taxon &#39;Canis lupus&#39;
## 
## 
## Retrieving data for taxon &#39;Alces americanus&#39;
## 
## 
## Retrieving data for taxon &#39;Abies balsamea&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let’s get the whole network and see that the NCBI identifiers have been added:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;g &amp;lt;- network_as_graph(api, isle_royale$id)
unlist(V(g)$ncbi)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 999462  90345   9612
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There is probably more and more we can do using combination of packages, I’ll try to show some possibilities in use-cases.&lt;/p&gt;

&lt;h1 id=&quot;what-now&quot;&gt;What now?&lt;/h1&gt;

&lt;p&gt;As before, keep in mind that the test database will be periodically wiped
clean, so don’t use it to do some actual data deposition (yet). But it would
be really cool for you to try writing scripts to upload/modify your datasets,
and tell me if anything goes wrong. Meanwhile, I’m working on preparing
use cases (including one to automatically upload your data on &lt;em&gt;figshare&lt;/em&gt;
and returning the DOI), which I’ll publish (hopefully) later this week,
or early next week. As always, use the &lt;a href=&quot;https://github.com/mangal-wg/rmangal/issues?state=open&quot;&gt;GitHub page&lt;/a&gt; to report issues.&lt;/p&gt;

</content>
  </entry>
  
</feed>
