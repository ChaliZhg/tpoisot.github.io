<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title>Tim Poisot :: Updates</title>
  <link href="http://timotheepoisot.fr/atom.xml" rel="self"/>
  <link href="http://timotheepoisot.fr/"/>
  <updated>2014-01-01T18:25:38-05:00</updated>
  <id>Tim Poisot</id>
  <author>
    <name>Tim Poisot</name>
    <email></email>
  </author>
  
  <entry>
     <title>Using Vim as a writing environment</title>
     <link href="http://timotheepoisot.fr/2014/01/01/vim-writing-environment/"/>
    <updated>2014-01-01T00:00:00-05:00</updated>
    <content type="html">&lt;p&gt;Happy new year, everyone! And to make sure it starts in a good way, let me
bring you the gift of productivity! Or specifically, how to set-up &lt;code&gt;Vim&lt;/code&gt;
in a way that will transform it into an impressively efficient writing
environment. &lt;code&gt;Vim&lt;/code&gt; is a really good code editor, there is no questioning
that. But writing code and writing prose are entirely different exercises,
and some tools you need to efficiently write code are only getting in the
way when you write prose. But &lt;code&gt;Vim&lt;/code&gt; can handle that, and I&#39;m going to share
my configuration with you.&lt;/p&gt;

&lt;h2&gt;The font&lt;/h2&gt;

&lt;p&gt;I care &lt;em&gt;way too much&lt;/em&gt; about fonts and typography. And as I spend several
hours each days looking at a terminal, picking a good font for writing is
important. What makes a font good will vary from person to person, but there
are a few common elements. I want glyphs that are easy to differentiate (&lt;code&gt;0&lt;/code&gt;
&lt;em&gt;vs.&lt;/em&gt; &lt;code&gt;O&lt;/code&gt;, &lt;code&gt;l&lt;/code&gt; &lt;em&gt;vs.&lt;/em&gt; &lt;code&gt;1&lt;/code&gt;, for example), and a font with a good negative
space, so that reading long paragraphs of texts is not eye-straining. When
spending a few hours working on a paper, it can make all the difference in
the world to have a good, easily legible font. I use the &lt;a href=&quot;http://www.google.com/fonts/specimen/Cousine&quot;&gt;Cousine&lt;/a&gt; family,
which can be freely downloaded. It works well as all sizes, and has a good
inventory of glyphs.&lt;/p&gt;

&lt;h2&gt;The color scheme&lt;/h2&gt;

&lt;p&gt;No surprise, I use &lt;a href=&quot;http://ethanschoonover.com/solarized&quot;&gt;Solarized&lt;/a&gt; (light). It has a good contrast, and because
of the beige background, you don&#39;t feel like you are staring at a light
bulb. And the &lt;code&gt;solarized-vim&lt;/code&gt; version comes with quite a very exhaustive
syntax set, so everything (i) works and (ii) looks great out of the box.&lt;/p&gt;

&lt;h2&gt;The plugins&lt;/h2&gt;

&lt;p&gt;Although you can get my entire &lt;a href=&quot;https://github.com/tpoisot/dotfiles/blob/master/vimrc&quot;&gt;&lt;code&gt;.vimrc&lt;/code&gt; file&lt;/a&gt; from &lt;em&gt;GitHub&lt;/em&gt;, I
will just walk you through the most important plugins. I manage them using
&lt;a href=&quot;https://github.com/gmarik/vundle&quot;&gt;vundle&lt;/a&gt;, and the ones I &lt;em&gt;absolutely need&lt;/em&gt; to do some serious writing are:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;vim&quot;&gt;Bundle &lt;span class=&quot;s1&quot;&gt;&amp;#39;tpope/vim-markdown&amp;#39;&lt;/span&gt;
Bundle &lt;span class=&quot;s1&quot;&gt;&amp;#39;mikewest/vimroom&amp;#39;&lt;/span&gt;
Bundle &lt;span class=&quot;s1&quot;&gt;&amp;#39;vim-pandoc/vim-pandoc&amp;#39;&lt;/span&gt;
Bundle &lt;span class=&quot;s1&quot;&gt;&amp;#39;altercation/vim-colors-solarized&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;First, &lt;code&gt;vim-markdown&lt;/code&gt; and &lt;code&gt;vim-pandoc&lt;/code&gt; are extremely powerful extensions for
(you guessed it), &lt;code&gt;markdown&lt;/code&gt; and &lt;code&gt;pandoc&lt;/code&gt;. As I used these for &lt;em&gt;everything&lt;/em&gt;
these days, having good plugins is a requirement. They introduce things like
autocompletion of citations from a &lt;code&gt;bibtex&lt;/code&gt; file, and things like &lt;em&gt;replacing
LaTeX greek letters by the actual greek letter&lt;/em&gt;, which means that when you
are looking at a paragraph, it shows no markup, but the formatting. It&#39;s
really, really amazing when you want to &lt;em&gt;read&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The most recent addition to this collection is &lt;code&gt;vimroom&lt;/code&gt;. This plugins
attempts to replicate the look and feel of &lt;em&gt;WriteRoom&lt;/em&gt;, one of the many, many
&quot;distraction-free&quot; writing softwares. When editing a document, &lt;code&gt;&amp;lt;leader&amp;gt;V&lt;/code&gt;
will center the text on the screen, with ample white-space on each sides,
and remove every piece of clutter from the &quot;interface&quot;. It means that, if
you have a large screen, you don&#39;t need to turn your head all the way to
the left to read what you are working on.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;So that&#39;s it! It&#39;s not much work, and it makes for a really pleasant writing
experience in &lt;code&gt;Vim&lt;/code&gt;. And it&#39;s effortless to switch from &quot;code&quot; to &quot;prose&quot;
mode, making &lt;code&gt;Vim&lt;/code&gt; a very versatile and productive tool.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>Software papers and open source licenses</title>
     <link href="http://timotheepoisot.fr/2013/12/19/reviewing-software-papers/"/>
    <updated>2013-12-19T00:00:00-05:00</updated>
    <content type="html">&lt;p&gt;I have been reviewing a lot more software / methodological papers these
days, which led me to think about open &lt;em&gt;vs.&lt;/em&gt; closed software (even more
than usual). By &lt;em&gt;closed&lt;/em&gt;, I mean any software that would require me to pay
to be able to reproduce the method, either directly, or through a licence
costing money to the university. Think &lt;em&gt;Mathematica&lt;/em&gt;, &lt;em&gt;matlab&lt;/em&gt;, or any other
commercial tools. Despite what &lt;a href=&quot;http://www.quantumforest.com/2013/12/should-i-reject-a-manuscript-because-the-analyses-werent-done-using-open-source-software/&quot;&gt;some&lt;/a&gt; would have you believe, the way
analyses are done in a &quot;traditional&quot; research paper (&lt;q&gt;Here is my experiment,
here are the analyses I have been doing, and here are my conclusions&lt;/q&gt;)
is an entirely different piece of work. When I review such papers, I care
very little about the software used to obtain the results. And of course,
it&#39;s difficult to reproduce a field survey or a large lab experiment, and
no amount of free software will change that.&lt;/p&gt;

&lt;p&gt;But software papers, &lt;em&gt;i.e.&lt;/em&gt; either papers presenting a new software,
or relying heavily on computer code (&lt;em&gt;i.e.&lt;/em&gt; most simulation studies),
are defined largely by their computability. So let&#39;s jump straight to the
conclusion: would I reject such a paper because the code relies on closed
software? &lt;strong&gt;No&lt;/strong&gt;, for two reasons. The first reason is highly personal:
I would not accept to review it in the first place. My condition to review
a software paper is to be able to understand and evaluate the code, so if
I can&#39;t run the software or understand what it does, I won&#39;t review the
paper. The second reason is that, ultimately, the important thing in a
software paper is the method described, and so as long as I can reproduce
what the authors propose, and it is a useful addition to the methodological
toolkit, there is no reason to reject the paper. An additional reason is that
I am a co-author on at least two (&lt;em&gt;in prep.&lt;/em&gt;) papers using closed software,
and I won&#39;t hold others to a higher standard than the one I hold myself to...&lt;/p&gt;

&lt;p&gt;That being said, there are extremely strong arguments to be made in favor
of using Free and Open-Source Software (FOSS) whenever possible. For one
thing, and although it&#39;s easy to forget that coming from a &lt;em&gt;rich&lt;/em&gt; university,
not everyone can afford to pay a software license. Tools that are relevant
for conservation may be unavailalbe to NGOs if they need to pay to use
them. Even in the academic world, researchers from developing countries,
may have a hard time justifying spending thousands of dollars on software
when there are free alternatives available. My (highly personal) point of
view is that using FOSS software is part of the collective responsibility
to lower the access fee to science.&lt;/p&gt;

&lt;p&gt;Now that we know that &lt;a href=&quot;https://peerj.com/articles/175/&quot;&gt;open data&lt;/a&gt; is associated with an increased
citation rate, I wonder whether the same is true of FOSS-using papers. I
can think a couple reasons for why it should be the case. Describing a new
method is much like explaining a recipe. But I&#39;m much less likely to cook
true &lt;em&gt;crÃªpes&lt;/em&gt; from Britanny, which require a bulky and expensive &lt;em&gt;billlig&lt;/em&gt;,
than I am to cook the regular version that I can do at home with my frying
pan. Wow, that is a bad metaphor, it almost reads like FOSS is the cheap
knock-off version of science. Never mind... In any case, offering users
a way to apply an analysis at no cost is a good incentive to using your
method. And now that almost anyone knows and uses R (especially students),
you can reach ~100% of your field this way.&lt;/p&gt;

&lt;p&gt;But what should we do when no FOSS software can do the particular analysis
one wants to do? This is a valid and difficult question. It&#39;s obvious
that what matters most is to have the most solid result one can get using
the currently available tools. But on the other hand, contributing new
software is an activity in itself, that is starting to be recognized as
such. Porting a good method into FOSS is probably going to get you some
recognition withing your community. &lt;em&gt;ImpactStory&lt;/em&gt; let you track the impact
of software releases through &lt;em&gt;e.g.&lt;/em&gt; &lt;em&gt;GitHub&lt;/em&gt;. For example, the &lt;em&gt;ImpactStory&lt;/em&gt;
page &lt;a href=&quot;http://impactstory.org/timpoisot/product/puktfnm5rj97v383zwo6smlp&quot;&gt;for the &lt;code&gt;digitize&lt;/code&gt; package&lt;/a&gt; I wrote in one afternoon because I
wanted to know whether it (it: extracting data from published scatterplots)
could be done, show that people found this package useful. While it&#39;s true that
writing useful code is not as appreciated as writing papers that get cited, the
development of new metrics will probably make it increasingly rewarding. And
the less people have to pay, the more likely they are to use your code.&lt;/p&gt;

&lt;p&gt;At the very least, if your code requires proprietary software to run,
pick a &lt;a href=&quot;http://choosealicense.com/&quot;&gt;FOSS license&lt;/a&gt;. There are tools (like &lt;em&gt;Choose a license&lt;/em&gt;,
linked just before) that will allow you to select licenses according to your
needs. Even if people can&#39;t &lt;em&gt;run&lt;/em&gt; the software, they can see how it works,
and translate it in another language it they need it. Starting in January,
all &lt;a href=&quot;http://www.britishecologicalsociety.org/publications/&quot;&gt;BES journals&lt;/a&gt; will require that &lt;em&gt;data&lt;/em&gt; are available, free of
charge and in a repository, when the paper is accepted - and the goal is
clear: allow reprodubilityand re-use. Perhaps the same type of initiatives
should be progressively introduced for code. And this is the part where, as
referees, we can start making a small difference: by suggesting appropriate
FOSS licenses if the authors have not done so. It will help software spread,
and the most methods we have at our disposal, the most likely we have to
find the right one to solve a problem.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>What's hot on PeerJ?</title>
     <link href="http://timotheepoisot.fr/2013/12/17/what-peerj/"/>
    <updated>2013-12-17T00:00:00-05:00</updated>
    <content type="html">&lt;p&gt;Lat week, I mentionned that ecology is the leading discipline on &lt;a href=&quot;https://peerj.com/preprints/&quot;&gt;&lt;em&gt;PeerJ&lt;/em&gt;
preprints&lt;/a&gt;, as attested by the number of submissions. Which led me to
wondering which topics were actually &lt;em&gt;hot&lt;/em&gt; on the preprint server. &lt;em&gt;PeerJ&lt;/em&gt; do
not have an API, but there URL scheme is not too complicated, and most of the
relevant informations are in the webpage body. So I wrote a very short python
script to get the first 158 preprints, download them, and get the abstract.&lt;/p&gt;

&lt;p&gt;The code goes like this:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;requests&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;bs4&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BeautifulSoup&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;BaseURL&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;https://peerj.com/preprints/&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;StopAt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;158&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;StartAt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Corpus&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Fulltext&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;&amp;quot;&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preprint_id&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StartAt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StopAt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;preprint_page&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BaseURL&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preprint_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;/&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preprint_page&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;status_code&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;preprint_text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preprint_page&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;preprint_soup&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BeautifulSoup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preprint_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;abstract&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preprint_soup&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;meta&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;description&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;Fulltext&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot; &amp;quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;abstract&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;Corpus&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;abstract&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Fulltext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;There is actually a one-liner version of this code, that exsist solely to
show that you can do horrible twisted things with python. I&#39;m not showing
it. But the point is: this code will return a list of all abstracts, pasted
together in a single sentence. Which I copied, and used in &lt;a href=&quot;http://www.wordle.net&quot;&gt;wordle&lt;/a&gt; to get
the following graphic:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/w-peerj-abs.png&quot; title=&quot;With abstracts&quot; alt=&quot;With abstracts&quot; /&gt;
So apparently, &lt;code&gt;data&lt;/code&gt; are big, followed by &lt;code&gt;sharks&lt;/code&gt; and &lt;code&gt;whales&lt;/code&gt; (actually
whale sharks), but suriprisingly, there are no big ecological keywords
that are very prominent - which goes well with the idea that &lt;em&gt;PeerJ&lt;/em&gt;
is multi-disciplinary. By changing the &lt;code&gt;BaseURL&lt;/code&gt; variable in my code,
it was easy to get the same picture but for the &lt;em&gt;articles&lt;/em&gt;, &lt;em&gt;i.e.&lt;/em&gt; the
contributions that went through peer-review. According to the front page,
Ecology and Biodiversity are still the leading disciplines there.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/w-peerj-pap.png&quot; title=&quot;With papers&quot; alt=&quot;With papers&quot; /&gt;
It seems to be the case, with &lt;code&gt;species&lt;/code&gt; being the most important word, followed
(still) by &lt;code&gt;data&lt;/code&gt;. Interestingly, clinical and molecular keywords are more
prominent in the published papers than in the preprints. But in any case,
&lt;em&gt;PeerJ&lt;/em&gt; is definitely ecology-friendly, which hopefully will mean that more
and more people will submit there.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>Data-sharing paper online</title>
     <link href="http://timotheepoisot.fr/2013/12/05/data-sharing-paper/"/>
    <updated>2013-12-05T00:00:00-05:00</updated>
    <content type="html">&lt;p&gt;Our &lt;a href=&quot;http://library.queensu.ca/ojs/index.php/IEE/article/view/4632&quot;&gt;paper&lt;/a&gt; &lt;em&gt;Towards a sustainable ecological science&lt;/em&gt; is now online at
&lt;em&gt;Ideas in Ecology and Evolution&lt;/em&gt;. Thanks are due to Karthik Ram for the
invitation. This is going to be a very exciting special issue. If you want
to know more about the paper, the best thing to do is most likely to read it
(it&#39;s open access). But in short, we (I, Ross Mounce, and Dominique Gravel),
try to make a point of all the benefits of sharing data in ecology. There
are some parts about licenses, and some parts about &lt;code&gt;JSON&lt;/code&gt; (because of course
there are).&lt;/p&gt;

&lt;p&gt;But besides the content of the paper (that made me think &lt;em&gt;a lot&lt;/em&gt; about data
sharing), the way we wrote it was extremely fun. Everything was &lt;a href=&quot;https://github.com/tpoisot/DataSharingPaper/network&quot;&gt;done on
&lt;em&gt;GitHub&lt;/em&gt;&lt;/a&gt;, in &lt;em&gt;mardkwon&lt;/em&gt;,and  we&#39;ve used Issues and all the tricks in
the book to keep track of what needed to be done. It&#39;s also the first paper
in which my twitter handle is listed in the contacts, and I&#39;ll probably do
that for every paper now.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>Why JSON is my go-to data format</title>
     <link href="http://timotheepoisot.fr/2013/12/02/json-goto-format/"/>
    <updated>2013-12-02T00:00:00-05:00</updated>
    <content type="html">&lt;p&gt;Since this summer, I work with models that generate massive amounts
of output. This weekend batch of simulations resulted in 20+ Gig of raw
data. This particular model outputs very variable data. The number of
lines/columns (were I working in a tabular format) would be unpredictable,
vary across simulations, and generally be a little bit too massive to work
with (simply) in &lt;code&gt;R&lt;/code&gt;. But I&#39;ve been using &lt;code&gt;JSON&lt;/code&gt; for a while now, and it
makes working with these types of data (not the &quot;several Gig&quot; types, the
&quot;highly heterogeneous&quot; type) easy.&lt;/p&gt;

&lt;p&gt;I love &lt;code&gt;JSON&lt;/code&gt; because it can be validated. Running a command like &lt;code&gt;jsonlint
file.json&lt;/code&gt; will either return me the pretty-printed version of the file,
or an error telling me where the file is not conforming to the &lt;code&gt;JSON&lt;/code&gt;
specification. Or in other words, when I read something in memory, I&#39;m
confident it is indeed a correctly formated file.&lt;/p&gt;

&lt;p&gt;But wait, there&#39;s more! In &lt;code&gt;JSON&lt;/code&gt;, you can define &lt;em&gt;schemes&lt;/em&gt;, or a &lt;code&gt;JSON&lt;/code&gt;
file telling you (or a validator) what other &lt;code&gt;JSON&lt;/code&gt; files should look
like. Which means that it&#39;s possible to check that a particular file is
correctly formatted &lt;em&gt;with regard to a previously described data format&lt;/em&gt;. I
use the &lt;code&gt;jsonschema&lt;/code&gt; python module to do that:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;json&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;jsonschema&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;validate&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Read the JSON scheme file&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;scheme.json&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Read the JSON output file&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;output.json&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;op&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Validate&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;validate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;Not valid&amp;quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;Valid&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And because a&lt;code&gt;JSON&lt;/code&gt; scheme has &lt;a href=&quot;http://json-schema.org/&quot;&gt;field for description of each element&lt;/a&gt;,
your output is essentially self-described. Someone with no prior knowledge
of how you organized your data can take your results, check that they conform
to the specification, and see what each element of the output file represents.&lt;/p&gt;

&lt;p&gt;Although I haven&#39;t bothered to write a scheme (yet), you can see how &lt;code&gt;JSON&lt;/code&gt;
can contain a lot of heterogeneous informations in an easy to read format
&lt;a href=&quot;https://github.com/tpoisot/manna&quot;&gt;on the &lt;code&gt;manna&lt;/code&gt; index page&lt;/a&gt;: the output files give informations about
the species, and for each time steps, the number of individuals, and each
individual interaction.&lt;/p&gt;

&lt;p&gt;While it&#39;s true this information is not extremely complicated, it&#39;s still
simpler to have it in this form, rather than as a text file. But &lt;code&gt;JSON&lt;/code&gt;
truly shines when &lt;em&gt;reading&lt;/em&gt; the data. In the above example, if the &lt;code&gt;op&lt;/code&gt;
object contains a list of species, each having a body size called &lt;code&gt;bs&lt;/code&gt;,
then we can get the mean body size with&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;m_bs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;bs&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sp&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;R&lt;/code&gt; can also read &lt;code&gt;JSON&lt;/code&gt; well with the &lt;code&gt;rjson&lt;/code&gt; package. This returns a &lt;code&gt;list&lt;/code&gt;
representation of the &lt;code&gt;JSON&lt;/code&gt; file, so it&#39;s easy to manipulate &lt;code&gt;JSON&lt;/code&gt; objects
with &lt;code&gt;l*ply&lt;/code&gt; functions in &lt;code&gt;plyr&lt;/code&gt;. So now, most of my models work in the same
way. I write one &lt;code&gt;JSON&lt;/code&gt; file for each simulation (or variations thereof)
into an &lt;code&gt;output&lt;/code&gt; folder. Then I read through each file with &lt;code&gt;python&lt;/code&gt; and/or
&lt;code&gt;R&lt;/code&gt;, and I have a great time doing it!&lt;/p&gt;

&lt;p&gt;The main drawback of &lt;code&gt;JSON&lt;/code&gt; (&lt;a href=&quot;http://www.alcides-mp.com/?p=1&quot;&gt;as pointed out here&lt;/a&gt; are that it must be
read in memory entirely before it&#39;s used (and it gets parsed at this time
too). Or in other words, it can be slow. But it&#39;s a good thing! I found out
that it forces me to (i) aim for the most concise representation possible,
and (ii) split the output in chunks when needed. These chunks can be read
in parallel later, and re-assembled, so I don&#39;t try to load files of a few
hundreds Mb at times.&lt;/p&gt;

&lt;p&gt;So now, go try &lt;code&gt;JSON&lt;/code&gt; for yourself. In the &lt;code&gt;manna&lt;/code&gt; program linked above, there
are examples with (admittedly badly written) &lt;code&gt;R&lt;/code&gt; files to read and manipulate
&lt;code&gt;JSON&lt;/code&gt; outputs. And keep in mind that when you&#39;ll be coming back to your
output files in six months, you&#39;ll be glad to have a verbose format and a
scheme describing it to understand what is going on...!&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>The pros and cons of using R packages for teaching</title>
     <link href="http://timotheepoisot.fr/2013/11/27/r-packages-teaching/"/>
    <updated>2013-11-27T00:00:00-05:00</updated>
    <content type="html">&lt;p&gt;R packages are one of the best tools we have. It&#39;s a library of 1000s
of functions ready to be used, all wrapped in a single common language,
and it&#39;s free. I wrote some, and used even more. When I review a paper,
upon seeing &quot;we used function &lt;code&gt;x&lt;/code&gt; in the version 2.3 of package &lt;code&gt;y&lt;/code&gt;&quot;,
I know a lot about what the authors have been doing (and I can check the
help of this function for references and additional details). And obviously,
training students about which packages are widely used in a particular field
is an important task, because it will ensure that they will speak the same
language as other scientists in the field.&lt;/p&gt;

&lt;p&gt;Because packages (often) represent the &lt;em&gt;state of the art&lt;/em&gt; in a particular
field, or because they offer a unified interface to a lot of different methods
(&lt;code&gt;simecol&lt;/code&gt; is a good example of that), it&#39;s important that students know how
to use them (and most importantly, which packages to use). To some extent,
packages are just another tool, like PCR or statistics. Knowing how to use
them (properly) saves time, and opens new possibilities for analyses. And
let&#39;s just be realistic, everyone uses packages. All the time. So yes, of
course, the students should be familiar with the most important in their field.&lt;/p&gt;

&lt;p&gt;An important point is that we also start to see the development of &lt;em&gt;package
ecosystems&lt;/em&gt;. The &lt;a href=&quot;http://ropensci.org/packages/index.html&quot;&gt;&lt;em&gt;rOpenSci&lt;/em&gt;&lt;/a&gt; project is an example of that. They propose a
lot of different packages whose common purpose is to interact with databases
and API for science, but the real power is offered by the ability to integrate
several of these packages &lt;a href=&quot;http://ropensci.org/usecases/index.html&quot;&gt;in a single analysis&lt;/a&gt;. Want to get the list of
species in a country, and check which are invasive, then plot that onto
their phylogenetic tree? It can be done. Another well known example is the
&lt;code&gt;plyr&lt;/code&gt;/&lt;code&gt;reshape2&lt;/code&gt;/&lt;code&gt;ggplot2&lt;/code&gt; combo, which is (seriously) the only reason I
still have to start &lt;code&gt;R&lt;/code&gt; at least once a day.&lt;/p&gt;

&lt;p&gt;So on one hand, packages are really good, because they save time and
allow to do an analysis which (hopefully) conforms with the current set of
&quot;best practices&quot; in the field. On the other hand, when you start working
&lt;em&gt;only&lt;/em&gt; with what the packages have to offer, you can severely limit your
creativity. There are questions that will require new code to be written,
and you&#39;ll most likely hit obstacles along the ways. That, too, should be
something the students encounter during their training.&lt;/p&gt;

&lt;p&gt;And this is when the limits of &lt;code&gt;R&lt;/code&gt; start to show. As John Cook said (&lt;a href=&quot;http://readwrite.com/2013/11/25/python-displacing-r-as-the-programming-language-for-data-science&quot;&gt;source&lt;/a&gt;),
&quot;I find it more helpful to think of R as having a programming language than
being a programming language&quot;. I would never have put that so eloquently,
and it is entirely ture. Over the last two months, I gave a graduate training
class on &lt;em&gt;Algorithms and programming for environmental sciences&lt;/em&gt;, in which
I tried to discuss how to make &quot;good&quot; (robust, user-friendly, stupid-proof)
programs. Regardless of what the students learned, I definitely realized
that &lt;code&gt;R&lt;/code&gt; is a &lt;em&gt;huge mess&lt;/em&gt; when you try to work outside of packages, or
do something else than statistics. Interestingly, even Hadley Whickam&#39;s
forthcoming &lt;a href=&quot;http://adv-r.had.co.nz/&quot;&gt;&lt;em&gt;Advanced R development&lt;/em&gt;&lt;/a&gt; starts by saying that it will
show how &lt;em&gt;what seems horrible is merely &quot;not that bad&quot;&lt;/em&gt; (go read the book,
by the way, it&#39;s great). And so understanding &lt;em&gt;what&lt;/em&gt; to do often comes second
to understanding &lt;em&gt;how to do it&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The (many, many) quirks of &lt;code&gt;R&lt;/code&gt; nonwithstanding, working with a package
and doing something new are two different approaches. The former involves
looking at the examples, and reading the documentatin to see what each
function does. The later requires a working knowledge of the language, and
some notions of algorithmic. Yet, judging from my observations, these two
different exercices are often (and let&#39;s not forget statistics) presented
together, under the general denomination of &lt;code&gt;R&lt;/code&gt;. And when, afterwards, students
complain of &quot;not getting &lt;code&gt;R&lt;/code&gt;&quot;, it&#39;s hard to tell whether what they don&#39;t
get is what exactly a &lt;code&gt;while&lt;/code&gt; loop does (which is not a &lt;code&gt;R&lt;/code&gt;-specific issue),
or why a &lt;code&gt;data.frame&lt;/code&gt; is also a &lt;code&gt;list&lt;/code&gt; and a &lt;code&gt;matrix&lt;/code&gt; (which is a reasonable
question, and quite specific to &lt;code&gt;R&lt;/code&gt;); you may know how to do statistics with
&lt;code&gt;R&lt;/code&gt;, or use &lt;code&gt;vegan&lt;/code&gt; or &lt;code&gt;picante&lt;/code&gt;, but not know &lt;code&gt;R&lt;/code&gt; (or the other way around).&lt;/p&gt;

&lt;p&gt;The point I&#39;m slowly getting at is that you&#39;ll most likely end up in a
situation which is not covered by &lt;em&gt;any&lt;/em&gt; package available. Or, as a student,
you&#39;ll have to modify a code for a group project, which means re-writing
some functions, or changing the data structure. Part of the training should
prepare students to do this kind of things. Rather than using a pre-made
function to calculate a Shannon&#39;s index, or even find the maximal value in
an array, let&#39;s walk students through the writing of this function.&lt;/p&gt;

&lt;p&gt;So, to conclude, while it&#39;s clear that I have a love/hate relationship
with &lt;code&gt;R&lt;/code&gt;, I think it&#39;s still an invaluable tool for teaching, especially in
undergraduate courses. &lt;code&gt;python&lt;/code&gt; might be as easy and more coherent, but less
people use it (for now...), and &lt;code&gt;R&lt;/code&gt; is still the &lt;em&gt;lingua franca&lt;/em&gt;. And a part
of the success of &lt;code&gt;R&lt;/code&gt; is the diversity of available packages. But relying
only on packages creates this weird black box situation, in which students
will know what the function does, but not necessarily &lt;em&gt;how&lt;/em&gt; it does it; and
knowing how stuff works is key when you have to create your own stuff. So
&lt;strong&gt;pros&lt;/strong&gt;, packages are awesome because 90% of what you want to do is already
programmed. &lt;strong&gt;Cons&lt;/strong&gt;, the remaining 10% will be extremely hard for you to
get if you don&#39;t get your hands dirty once in a while, and write things
from scratch.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>Spatially explicit metapopulation model in R</title>
     <link href="http://timotheepoisot.fr/2013/11/25/metapopulation-model-r/"/>
    <updated>2013-11-25T00:00:00-05:00</updated>
    <content type="html">&lt;p&gt;One of my favorite tools to work with networks is &lt;a href=&quot;http://networkx.github.io/&quot;&gt;the &lt;code&gt;networkx&lt;/code&gt; python
package&lt;/a&gt; (if only because lists comprehensions make so much sense when you
work with graphs). But for teaching purposes (especially in ecology), having
purely &lt;code&gt;R&lt;/code&gt;-based tools is better. The students know the language (at least
enough to get around most problems in a few hours, and so they can focus on
the theory rather than the tedious details of implementation. So I started
looking for good graph analysis tools in R.&lt;/p&gt;

&lt;p&gt;I&#39;ve read good thing (well, &lt;a href=&quot;http://assemblingnetwork.wordpress.com/tutorials/network-basics/&quot;&gt;good-looking code&lt;/a&gt;, actually) about the
&lt;code&gt;igraph&lt;/code&gt; package, so I decided to try it (good surprise: there are both &lt;code&gt;R&lt;/code&gt;,
&lt;code&gt;python&lt;/code&gt; and &lt;code&gt;C&lt;/code&gt; implementations. My first two projects when I&#39;m doing graph
things are (i) how fast can I implement Williams &amp;amp; Martinez &lt;em&gt;niche model&lt;/em&gt;, and
(ii) how fast can I implement a spatially explicit metapopulation model. I have
folders in my computers with various versions of both in various languages.&lt;/p&gt;

&lt;p&gt;So I decided to work on the later project. The idea is to use &lt;code&gt;igraph&lt;/code&gt; to
generate a random geometric graph, and use that as the landscape over which the
metapopulation is simulated. The code is &lt;a href=&quot;https://gist.github.com/tpoisot/7547954&quot;&gt;available as a gist&lt;/a&gt;. Coming
from &lt;code&gt;networkx&lt;/code&gt;, it took some time getting used to the different approach. But
overall, &lt;code&gt;igraph&lt;/code&gt; convinced me, and I&#39;ll most definitely use it when teaching
about networks in the future.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/rmetapop.png&quot; alt=&quot;Fig1&quot; /&gt;
As a side note, the code allows to generate 3D spatial graphs. It&#39;s entirely
useless, but it&#39;s kinda cool!&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>Collaborating with markdown and bibtex</title>
     <link href="http://timotheepoisot.fr/2013/11/10/shared-bibtex-file-markdown/"/>
    <updated>2013-11-10T00:00:00-05:00</updated>
    <content type="html">&lt;p&gt;The single most annoying issue when using &lt;code&gt;pandoc&lt;/code&gt; to work collaboratively on
papers is that it might be difficult for other people to compile the paper if
they do not use your bibtex file. And if there is ony thing I avoid like the
plague, it&#39;s having several bibtex files all over the place. And I&#39;m can&#39;t
be bothered to keep &quot;Collections&quot; or separate folders in Zotero. So clearly,
I had to code my way out of this one.&lt;/p&gt;

&lt;p&gt;The good thing of using &lt;code&gt;pandoc&lt;/code&gt; is that the citation syntax is remarkably
simple: &lt;code&gt;@key&lt;/code&gt;. It means that using a simple &lt;code&gt;grep&lt;/code&gt; command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;bash&quot;&gt;grep @&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;-:_a-zA-Z0-9&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;* ms.md -oh --color&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;never | sort  | uniq -u | sed &lt;span class=&quot;s1&quot;&gt;&amp;#39;s/@//g&amp;#39;&lt;/span&gt; &amp;gt; bib.keys
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;, I can generate &lt;code&gt;bib.keys&lt;/code&gt;, a list with all the keys encountered in
&lt;code&gt;ms.md&lt;/code&gt;. The keys will be present only once, and sorted. The first few lines
of one such file are&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text&quot;&gt;allesina_competitive_2011
angilletta_temperature_2004
araujo_using_2011
baiser_geographic_2012
baskerville_spatial_2011
bluthgen_what_2008
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, I need to take each of these keys, read my big &lt;code&gt;library.bib&lt;/code&gt; file, and
extract only the entries that are cited in the document. So I installed a
&lt;a href=&quot;https://github.com/sciunto/python-bibtexparser&quot;&gt;bibtex parser for python&lt;/a&gt;, and started doing exactly that. The amazing
thing is, this only takes four (interesting) lines:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;## Step 1 - read the key list&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rstrip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kl&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;## Step 2 - read the library file&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;refs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BibTexParser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bib_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_entry_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;## Step 3 - extract the used entries&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;used_refs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;refs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iteritems&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;## Step 4 - convert the dicts back into bibtex&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;refs_as_bib&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dict2bib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;used_refs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iteritems&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uk&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;uk&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;uk&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;used_refs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()]&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;## Step 5 - write the output file&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;codecs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;utf-8-sig&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;writelines&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;refs_as_bib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This code will print a list of all the keys in the &lt;code&gt;bib.keys&lt;/code&gt; that have
&lt;em&gt;not&lt;/em&gt; been matched to an entry in the main library file. If all went well,
this list should be empty. I&#39;ve uploaded the whole file as a &lt;a href=&quot;https://gist.github.com/tpoisot/7406955&quot;&gt;gist&lt;/a&gt;, if you
intend to use it.&lt;/p&gt;

&lt;p&gt;And of course, you can nicely wrap things up in a &lt;code&gt;makefile&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;makefile&quot;&gt;&lt;span class=&quot;nv&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; python2
&lt;span class=&quot;nv&quot;&gt;refs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; refs.bib
&lt;span class=&quot;nv&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; ms.md
&lt;span class=&quot;nv&quot;&gt;library&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; /path/to/main/bibtex/file.bib

&lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;refs&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt;: bib.keys
   &lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;python&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt; extractbib.py bib.keys &lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;library&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;refs&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt;

bib.keys: 
   grep @&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;-:_a-zA-Z0-9&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;* &lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;text&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt; -oh --color&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;never | sort  | uniq -u | sed &lt;span class=&quot;s1&quot;&gt;&amp;#39;s/@//g&amp;#39;&lt;/span&gt; &amp;gt; bib.keys
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I&#39;m glad that I finally have a solution to this problem. Of course, a
multi-author version is not difficult to do (just have each author write
its own bibtex file, and put them all together before running &lt;code&gt;pandoc&lt;/code&gt;). It
also means that my &lt;code&gt;pandoc&lt;/code&gt;-using papers are going to finally be entirely
reproducible, as I&#39;ll distribute the references list in the &lt;em&gt;github&lt;/em&gt;
repository.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>Notes on test-driven development (in Python)</title>
     <link href="http://timotheepoisot.fr/2013/11/04/test-driven-python/"/>
    <updated>2013-11-04T00:00:00-05:00</updated>
    <content type="html">&lt;p&gt;I&#39;m looking into the methodology of &lt;em&gt;test-driven development&lt;/em&gt;, that is when
you write your tests &lt;em&gt;before&lt;/em&gt; writing your code. The advertised benefit is
that you know in advance the behavior of your code, and it will make it easier
to implement and debug. This &lt;a href=&quot;http://www.onlamp.com/pub/a/python/2004/12/02/tdd_pyunit.html&quot;&gt;paper at O&#39;Reilly&lt;/a&gt; is a good introduction
(although the exact functions of &lt;code&gt;unittest&lt;/code&gt; it uses are now deprecated). To
get started, I&#39;ll work on a simple script that takes a series of numerical
values, and divide them by their max, to get the equivalent series of values
but in [0;1]. Let&#39;s start with the basics:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;unittest&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# for testing&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# for other things&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The first function I&#39;d like to write will be called &lt;code&gt;scale&lt;/code&gt;, and it will take a
(&lt;code&gt;numpy&lt;/code&gt;) array of numerical values, and return the same array divided by its
maximal value. So my first &lt;em&gt;unit test&lt;/em&gt; will test that the function performs
as needed:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;scaleTest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unittest&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TextCase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;testOnKnownResult&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;KnownInput&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;Scaled&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;KnownInput&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;assertTrue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Scaled&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Scaled&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;There are a number of stupid-proofing steps required. First, the function
cannot work when all values in the array are 0, or if there are negative
values, so I&#39;ll write a test for that. I want the function &lt;code&gt;scale&lt;/code&gt; to raise a
&lt;code&gt;ValueError&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;testNegative&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;WithNeg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
   &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;assertRaises&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ne&quot;&gt;ValueError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;WithNeg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;testNull&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;OnlyNull&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
   &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;assertRaises&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ne&quot;&gt;ValueError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OnlyNull&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now I have one test to check that the expected behavior is indeed obtained,
and two tests to check that the input values are correct. Things are starting
to look good! The one final thing that I want to enforce is the type of
inputs. Specifically, I want &lt;code&gt;numpy&lt;/code&gt; arrays of numeric types. If this is
not the case, the &lt;code&gt;scale&lt;/code&gt; function will raise a &lt;code&gt;TypeError&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;testType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;WithChar&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;0.0&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
   &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;assertRaises&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;ne&quot;&gt;TypeError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;WithCar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;At this point, even though I haven&#39;t yet wrote a single line of the &lt;code&gt;scale&lt;/code&gt;
function, I know quite well how it should work! Importantly, I know the
type of data that will come in this function. This is something I like when
writing code: I need to understand which data go where, and how they are
formatted and transformed along the way. I try to make students think about
this when giving courses. What kind of data go in? What kind of data come
out? This approach forces to determine when and how the function will break,
and it&#39;s great to have a very clear idea of the data types.&lt;/p&gt;

&lt;p&gt;Now is the time to program the actual function. It will take one argument,
and return another array (the original one is not modified). Let&#39;s write
the simplest possible form of this function:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In most situations, I would have been happy writing the function this way. Let&#39;s integrate it with the test suite, and see how it goes:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;#! /usr/bin/python2&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;unittest&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;## scaleTest goes here&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;## scale goes here&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;unittest&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here is the output of running this file:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text&quot;&gt;======================================================================
FAIL: testNegative (__main__.scaleTest)
----------------------------------------------------------------------
Traceback (most recent call last):
File &amp;quot;lv.py&amp;quot;, line 21, in testNegative
self.assertRaises(ValueError, scale, WithNeg)
AssertionError: ValueError not raised
======================================================================
FAIL: testNull (__main__.scaleTest)
----------------------------------------------------------------------
Traceback (most recent call last):
File &amp;quot;lv.py&amp;quot;, line 18, in testNull
self.assertRaises(ValueError, scale, WithNeg)
AssertionError: ValueError not raised
----------------------------------------------------------------------
Ran 4 tests in 0.001s
FAILED (failures=2)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;There are &lt;em&gt;only&lt;/em&gt; two failed tests. This is because &lt;code&gt;testOnKnownResult&lt;/code&gt; is
expected to succeed, and &lt;code&gt;testType&lt;/code&gt; will catch the &lt;code&gt;TypeError&lt;/code&gt; raised by
&lt;code&gt;np.max&lt;/code&gt; if there are arguments of the bad type. So now I just need to test
that the argument are not negative, or not only zeroes:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
   &lt;span class=&quot;c&quot;&gt;# We test that values are positive&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;ne&quot;&gt;ValueError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;Values of p cannot be smaller than 0&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;c&quot;&gt;# We test that values are not all null&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;ne&quot;&gt;ValueError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;Values of p cannot be all equal to 0&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;c&quot;&gt;# Finally we return the result&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;With these two checks, we can now re-run the tests:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;text&quot;&gt;....
----------------------------------------------------------------------
Ran 4 tests in 0.001s
OK
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Mission accomplished! It requires some discipline to work this way, but I
actually enjoyed going through this example. I really like the fact that
this approach forces to ask question about the structure of the code first,
and write the actual code later.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>Why should Ecology be open?</title>
     <link href="http://timotheepoisot.fr/2013/11/03/open-ecology/"/>
    <updated>2013-11-03T00:00:00-04:00</updated>
    <content type="html">&lt;p&gt;There are a lot of extremely good arguments to defend the fact that &lt;a href=&quot;http://www.technology.org/2013/10/04/science-even-open/&quot;&gt;&lt;em&gt;Science&lt;/em&gt;
(as a whole) should be more open&lt;/a&gt;. To summarize them very roughly: it&#39;s the
ethical thing to do as it allows everyone to access information, it&#39;s easier
for scientists to access information, it&#39;s faster than the traditional
peer-review system when you need to get your work noticed, and it&#39;s &lt;em&gt;much less
expensive&lt;/em&gt; than closed-source science. I could also elaborate on transparency,
accountability, and the refusal to put a wall around knowledge for a while
as well. In short, I&#39;m yet to find an argument that would convince me that
open science is a bad thing.&lt;/p&gt;

&lt;p&gt;What we call &lt;em&gt;open science&lt;/em&gt; obviously varies from people to people, but the
common thread is that it is a &lt;em&gt;set of practices aiming a lowering the
technological, financial, and legal barrier to the accessibility of scientific
materials, methods, processes, and outputs&lt;/em&gt;. This includes a diversity of
practices such as open manuscript writing, the use of preprints, data sharing,
in addition to open access publishing and the use of free/open source
software. Most of these practices can be applied to nearly all fields
of science, as proven by the &lt;a href=&quot;http://openwetware.org/wiki/Main_Page&quot;&gt;&lt;em&gt;OpenWetWare&lt;/em&gt; wiki&lt;/a&gt;, that caters
specifically to experimental biology and biochemistry.&lt;/p&gt;

&lt;p&gt;An interesting question that keeps popping up whenever I discuss &lt;em&gt;Open
ecology&lt;/em&gt;, or &lt;em&gt;Open biodiversity&lt;/em&gt;, is &quot;Why should it be different in ecology?&quot;.
Or expressed in another way, why should ecology pose particular challenges as
far as open science is concerned? It is a very valid question, and in
preparation for the &lt;em&gt;Open biodiversity&lt;/em&gt; panel that will take place at the QCBS
meeting in a month and a half, I thought it was time to bring some elements to
answer it.&lt;/p&gt;

&lt;p&gt;Let&#39;s start by recognizing that there are (broadly) two types of ecologists.
The &quot;empirical&quot; ecologists (including the microcosm people) rely on empirical
observations, and extensive field surveys, to address their questions. The
&quot;theoretical&quot; ecologists, on the other hand, integrate different data sources,
or build models, to understand &quot;general&quot; questions. There are obviously
people working all along this continuum (I don&#39;t consider myself a &quot;pure&quot;
theoretician, for example), and empiricists contributing some of the
most important ecological &lt;em&gt;theories&lt;/em&gt;, but we can all agree that leaning
towards one end of the continuum implies having a different set of
practices. And to make things worse, the communication between the two
groups is not as good as it should. While it&#39;s easy (I would like to say
&lt;em&gt;natural&lt;/em&gt;) for theoreticians to have an entirely open workflow (put
your code on &lt;em&gt;GitHub&lt;/em&gt;, use it to write your manuscripts, push your
data on &lt;em&gt;figshare&lt;/em&gt; using the API, and &lt;em&gt;voilÃ !&lt;/em&gt;), it&#39;s slightly more
complicated for empiricists. While it&#39;s true that anyone can make data
public, you can&#39;t reasonably release your field site under a &lt;em&gt;Creative
Commons&lt;/em&gt; licence (because it doesn&#39;t make any sense...).&lt;/p&gt;

&lt;p&gt;There are two points at which both worlds meet, however: data and algorithms.
Even for the most local and system-centered question, there is a large quantity
of required data. And because data are essentially multivariate, sometimes
incomplete, collected with unbalanced designs, and generally subject to the
contingencies of the &lt;a href=&quot;http://dna-protein.blogspot.com/2012/03/harvard-law-of-biology.html&quot;&gt;Harvard Law of Biology&lt;/a&gt;, these is often a need for
elegant (read: complicated) numerical methods to make sense of them (but this
is easy to solve, code should always be made open source, in
non-proprietary languages). This is probably the unique challenge of
&lt;em&gt;open ecology&lt;/em&gt;: we produce a lot of data, we need a lot of data, but there are
so many peculiarities attached to datasets that sharing them is by nature
a difficult task. Molecular biologists do not have this problem. The &lt;code&gt;fasta&lt;/code&gt;
format is simple because the biological reality of what it represents
(sequences) is simple too (or at least, it is easy to represent the building
blocks). And so it seems almost natural than sequences databases are so
prominent: there is no obstacle to data sharing because anyone can use
a &lt;code&gt;fasta&lt;/code&gt; file after two minutes of explanation of the format. I cannot remember
a single case when I managed to become entirely comfortable with the structure
of an Excel file in less than an afternoon.&lt;/p&gt;

&lt;p&gt;Open ecology will probably be much like all other forms of open science, but
the data heterogeneity challenge is especially problematic. There are certain
ways to solve it, though.&lt;/p&gt;

&lt;p&gt;First, we need &lt;strong&gt;strict data specifications&lt;/strong&gt;. If several groups work on the
same questions in similar systems, it would make sense that the data are
formatted in a common way. A good opportunity to draft these specifications is
large working groups, in which people sharing a common interest would think
about the &quot;core&quot; and &quot;satellite&quot; properties of a dataset. This will also
speed-up the development of data repositories with APIs. My personal favorite
for the development of data specifications is &lt;code&gt;JSON&lt;/code&gt; schemes, but really any
kind will do. Data are highly structured information, and it makes sense that
they are used and shared in a highly structured way. It is quite obvious that
there will not a be a single &lt;em&gt;ecological ontology&lt;/em&gt;, but if we managed to get it
right for a few dozens of high quality ones, it will be a strong enhancement
over the current data format of &quot;However I felt that day&quot; (I&#39;m guilty of this
one, of course).&lt;/p&gt;

&lt;p&gt;Second, we need &lt;strong&gt;stricter data review&lt;/strong&gt;. Once the data specification has been
published, it is important that its use is enforced by referees and editors, so
as to make sure that data will be re-usable. This is different from evaluating
the quality of data (data quality is only measurable in the light of the
specific question they are used for). What I have in mind is more along
the lines of a checklist with a few points: is the data conforming to the
specification (this step can be automated with a &lt;code&gt;jsonlint&lt;/code&gt;-like tool), is the
data easy to access, and are enough data released to make the dataset re-usable?&lt;/p&gt;

&lt;p&gt;Third, and finally, we need to &lt;strong&gt;un-install Excel and similar software&lt;/strong&gt;. What
we need instead, is a &lt;code&gt;pandoc&lt;/code&gt; for data. &lt;code&gt;pandoc&lt;/code&gt; is a tool to convert text
formats into other text formats. It&#39;s awesome. And with strict data
specifications, we should be able to write one for ecology. This will allow use
to store data in their correct format (&lt;em&gt;i.e.&lt;/em&gt;, conforming to the data
specification), but use them in another format when we need them in
another format.&lt;/p&gt;

&lt;p&gt;All of this will require a fair amount of community coordination, and changes
in the training and teaching of ecologists, but it&#39;s all for the greater good.
Some outstanding ecological problems will only be solved when a critical mass
of data is reached, and it would be extremely disappointing to realize that
these data were existing, but not available due to poor practices. So in short,
this is the challenge specific to open ecology: &lt;strong&gt;making sense of highly
heterogeneous and local data, and mobilizing them to address global and
general questions&lt;/strong&gt;.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>The (French) paradox of Closed Open Access</title>
     <link href="http://timotheepoisot.fr/2013/11/02/my-science-work/"/>
    <updated>2013-11-02T00:00:00-04:00</updated>
    <content type="html">&lt;p&gt;The OpenAccessWeek in France generated a &lt;em&gt;huge&lt;/em&gt; backlash, mostly owing to the
identity of the organizer. This edition was organized by the for profit
&quot;scientific social network&quot; &lt;em&gt;MyScienceWork&lt;/em&gt; (I&#39;m not going to link them). &lt;em&gt;My
Science Work&lt;/em&gt; (&lt;em&gt;MSW&lt;/em&gt; henceforth) is a social network in which people can...
well, you know what a social network is. This one is yet another social network
for scientists, much like Academia.edu and ResearchGate. I don&#39;t care all that
much about these tools.&lt;/p&gt;

&lt;p&gt;Let&#39;s summarize the situation. The French OpenAccesWeek was organized by &lt;em&gt;My
Science Work&lt;/em&gt;, and got backed up by several public organisms involved in
research: some labs, some universities, other related institutions. During this
operation, the credibility of &lt;em&gt;MSW&lt;/em&gt; got a huge boost in the French OA
community, in large part because of the fact that high-profile institutions
lent their credibility to the OpenAccessWeek.&lt;/p&gt;

&lt;p&gt;During the OpenAccessWeek itself, the critics started to emerge. &lt;a href=&quot;http://translate.google.com/translate?hl=fr&amp;amp;sl=fr&amp;amp;tl=en&amp;amp;u=http%3A%2F%2Fblog.stephanepouyllau.org%2F709&quot;&gt;StÃ©phane
Pouyllau &lt;/a&gt; started talking about &lt;em&gt;private open access&lt;/em&gt;. Shortly
thereafter, &lt;a href=&quot;http://translate.google.com/translate?hl=fr&amp;amp;sl=fr&amp;amp;tl=en&amp;amp;u=http%3A%2F%2Fpenserclasser.wordpress.com%2F2013%2F11%2F02%2Fopenaccessweek_france%2F&quot;&gt;Elifsu Sabuncu&lt;/a&gt; produced a great synthesis of the most important
reactions on twitter and in blogs (both links are through Google translate,
which gives a fairly broken but acceptable translation). What are these
critics about?&lt;/p&gt;

&lt;p&gt;Well, for a &quot;Frontrunner in Open Access&quot;, &lt;em&gt;MSW&lt;/em&gt; is surprisingly closed. Their
website lists over 28 millions academic papers in Open Access. I tried to
access some of mine, and when I clicked on the &quot;Download link&quot;, I got
redirected to a login/signup page. Does it seems like a &lt;em&gt;huge&lt;/em&gt; problem to you?
Because it is. When you put papers behind a wall, you are not advancing Open
Access. You are using the OA label to advance your business, and you are not
more worthy than a predatory OA publisher. If your goal is to provide a service
to the scientists, let us download the papers without signing-up for yet
another website we&#39;ll never visit again.&lt;/p&gt;

&lt;p&gt;And it&#39;s not like the links to the original publications is easy to spot,
either. It&#39;s nowhere to be found on the pages. You have no choice but to
signup for their &quot;service&quot;, if you want to get access to the paper (or,
well, you can realize that this is bullshit and go find the PDF
somewhere else). When some authors took offense with the practice,
the heads of &lt;em&gt;MSW&lt;/em&gt; proposed to remove their articles from the platform.
They were also quite active in insulting some of the most vocal critics on
their blogs, which is not a smart move. So it seems that &lt;em&gt;MSW&lt;/em&gt; has no will
to change their system, and is instead trying to silence the critics (you
cannot silence French people - criticism is an &lt;em&gt;art de vivre&lt;/em&gt; for
us!).&lt;/p&gt;

&lt;p&gt;If it were just yet another social network for scientists, I&#39;m sure no one
would have cared about their &quot;signup-required&quot; policy. But &lt;em&gt;MSW&lt;/em&gt; was, for
a week, the face of French Open Access. The closed, greedy, critics-insulting
face of French Open Access. So here is your explanation of the huge backslash.
People advocating &lt;em&gt;open&lt;/em&gt; access took it badly when a business tried to trick
people into thinking they were advancing OA, but was instead freeloading on OA
initiatives to make money. This is extremely worrisome. Predatory publishing is
bad press enough for OA, we really don&#39;t need new forms of predation.&lt;/p&gt;

&lt;p&gt;And the next step, after the name calling and the finger pointing will have
ended, is to have a good productive debate about &lt;em&gt;open&lt;/em&gt; access. Access is not
&lt;em&gt;open&lt;/em&gt; if you have to pay for it. And the fact that the price you pay for
a paper at &lt;em&gt;MSW&lt;/em&gt; is not monetary makes no matter: you pay with your
information, with your facebook profile, with your email address. When every
piece of information about you can be turned into a profit, &lt;strong&gt;every wall is
a paywall&lt;/strong&gt;. Parasitic behaviors, in which some use the OA label to get
credibility, but exploit OA to increase their userbase and their profit, must
be denounced &lt;em&gt;en masse&lt;/em&gt; by the community. And just for this reason, even so I&#39;m
disappointed that the Open Access Week was organized by &lt;em&gt;MSW&lt;/em&gt;, I&#39;m glad to see
that there is such a massive discussion about the ethics of putting OA papers
behind (pay)walls.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>Mapping ecological concepts using twitter</title>
     <link href="http://timotheepoisot.fr/2013/10/18/mapping-ecological-concepts/"/>
    <updated>2013-10-18T00:00:00-04:00</updated>
    <content type="html">&lt;p&gt;I&#39;ve been setting up a Twitter account for the &lt;a href=&quot;http://twitter.com/sfecologie/&quot;&gt;French Ecological
Society&lt;/a&gt;, and while we currently use it as a tool to relay the various
informations we want to share, it&#39;s also a great way to engage with people. So
the question that immediately came to mind is: what kind of topics are
associated with ecology on twitter? My own twitter list is a poor indicator of
the &quot;general public&quot; as far as ecology is concerned, so I had to look for
another solution.&lt;/p&gt;

&lt;p&gt;Twitter offers a series of &lt;em&gt;Streaming API&lt;/em&gt;, which allows to &quot;listen&quot; to the
conversation, by tracking some keywords, or users, or geographic location. So
I downloaded the excellent &lt;a href=&quot;https://github.com/geduldig/TwitterAPI&quot;&gt;&lt;code&gt;TwitterAPI&lt;/code&gt; &lt;/a&gt; module for &lt;code&gt;python&lt;/code&gt;, and
tracked a series of keywords. After some trial and error, I settled on
&lt;code&gt;biodiversity&lt;/code&gt;, &lt;code&gt;biological diversity&lt;/code&gt;, &lt;code&gt;ecology&lt;/code&gt;, &lt;code&gt;species richness&lt;/code&gt;,
&lt;code&gt;ecosystem services&lt;/code&gt;, &lt;code&gt;ecosystem functioning&lt;/code&gt;, and &lt;code&gt;ecological&lt;/code&gt;. I left out
&lt;code&gt;conservation&lt;/code&gt; and &lt;code&gt;ecosystem&lt;/code&gt;, because they had a really high rate of false
positives (in engineering and software, respectively).&lt;/p&gt;

&lt;p&gt;The streaming API, in a nutshell, will return a new line everytime a tweet that
matches the search parameters is posted. Not all tweets are sampled, but it&#39;s
approximately a random sampling, so more than good enough for a side-project.
I left the script running for a few hours at a time, which gave me a database
of over 1000 tweets, about 600 of which were retained. I&#39;ve removed all tweets
using a alphabet that is not latin, and there were quite a bit of these.&lt;/p&gt;

&lt;p&gt;Once this was done, I just read each line in &lt;code&gt;python&lt;/code&gt; (the Twitter API have the
good taste of returning everything as &lt;code&gt;json&lt;/code&gt; strings), selected the text,
and cleaned it. I simple (i) converted everything to lower case, (ii)
removed every non-ascii characters, (iii) removed punctuations, and (iv)
removed the hasthag (&lt;code&gt;#&lt;/code&gt;) symbol. Then I extracted all of the unique words
in each tweet, and let the fun begin!&lt;/p&gt;

&lt;p&gt;One of the things I know how to do is network analysis. So I settled on the
following procedure to find connections between words. All of the unique words
are nodes in a network, and two nodes are connected whenever they appear in the
same tweet. That makes for a &lt;em&gt;lot&lt;/em&gt; of connections, but it&#39;s not a problem
because I will eliminate a lot of them later. At this stage, I have a network
with 2600 nodes, and over 30000 undirected edges. Obviously (i) words appearing
only once in the dataset have little interest, and (ii) words like &lt;code&gt;the&lt;/code&gt; and
&lt;code&gt;in&lt;/code&gt; have a much higher chance of being in the same tweet than &lt;code&gt;amazonia&lt;/code&gt; and
&lt;code&gt;tits&lt;/code&gt; (I checked the original tweet, and it was not about ornithology).&lt;/p&gt;

&lt;p&gt;Before I go into how I accounted for that, some additional cleaning steps: all
usernames (&lt;code&gt;@&lt;/code&gt;), urls (&lt;code&gt;htt&lt;/code&gt;), and twitter-specific idioms (&lt;code&gt;RT&lt;/code&gt;, &lt;code&gt;MT&lt;/code&gt;, &lt;code&gt;ff&lt;/code&gt;)
were excluded. I also deleted all of the small connex components (&lt;em&gt;i.e.&lt;/em&gt;
small groups of words that were not connected with the rest of the
dataset). Then, the most arbitrary part of the work began. I removed
&lt;em&gt;all&lt;/em&gt; the nodes with a betweenness centrality of 0. The betweenness
centrality is the frequency at which a node appears in the shortest paths of
the graph, so I assumed that words that were not part of the shortest
connexions were not really important. Most of the words were excluded using
this proccess, and the network felt to a much more manageable 149 nodes and
2509 edges.&lt;/p&gt;

&lt;p&gt;Part of these were still words like &lt;code&gt;the&lt;/code&gt;, &lt;code&gt;at&lt;/code&gt;, and so forth. I&#39;ll eventually
use TF-IDF to select relevant words from each tweet, but remember, this is
a quick-and-dirty analysis. So I simply looked at mylist of nodes, and removed
every words that were not relevant (this is rather arbitrary too, but in doubt
I decided to be inclusive). The final network had 58 nodes, and 250 edges.&lt;/p&gt;

&lt;p&gt;Here are the top most important ones, based on their eigenvector centrality:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:left;&quot;&gt; Word          &lt;/th&gt;
&lt;th style=&quot;text-align:center;&quot;&gt; Centrality &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; ecology       &lt;/td&gt;
&lt;td style=&quot;text-align:center;&quot;&gt; 1          &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; biodiversity  &lt;/td&gt;
&lt;td style=&quot;text-align:center;&quot;&gt; 0.94       &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; ecological    &lt;/td&gt;
&lt;td style=&quot;text-align:center;&quot;&gt; 0.70       &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; human         &lt;/td&gt;
&lt;td style=&quot;text-align:center;&quot;&gt; 0.45       &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; environment   &lt;/td&gt;
&lt;td style=&quot;text-align:center;&quot;&gt; 0.43       &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; loss          &lt;/td&gt;
&lt;td style=&quot;text-align:center;&quot;&gt; 0.40       &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; wetland       &lt;/td&gt;
&lt;td style=&quot;text-align:center;&quot;&gt; 0.36       &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; environmental &lt;/td&gt;
&lt;td style=&quot;text-align:center;&quot;&gt; 0.35       &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; forest        &lt;/td&gt;
&lt;td style=&quot;text-align:center;&quot;&gt; 0.34       &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; wildlife      &lt;/td&gt;
&lt;td style=&quot;text-align:center;&quot;&gt; 0.33       &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;If you want to see the other words, you can &lt;a href=&quot;http://dx.doi.org/10.6084/m9.figshare.827286&quot;&gt;get the &lt;code&gt;graphml&lt;/code&gt; file from
&lt;em&gt;figshare&lt;/em&gt;&lt;/a&gt;. So here is the recipe. If you want to engage with the
public, these are the topics of interest. And they paint a quite clear picture:
what is the role of human in biodiversity loss at the global scale? As I assume
that the volume of non-ecologists is orders of magnitude larger than the volume
of ecologists, these keywords are most likely what is discussed.&lt;/p&gt;

&lt;p&gt;Here is finally a visualization of the network. There is a clearly
non-significant modular structure, but I&#39;ve nonetheless colored the nodes to
better illustrate the relationships:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/tw-ecol-concepts.png&quot; title=&quot;the network&quot; alt=&quot;Figure1&quot; /&gt;
Interestingly (the layout method groups nodes that are close to each other),
there seems to be the &quot;sustainability/ecosystem services&quot; topics
on one side (bottom-left), and the &quot;conservation&quot; topics on the
other side (top-right).&lt;/p&gt;

&lt;p&gt;I&#39;ll keep on running this model from time to time, but I think there is a good
opportunity to design informed strategies for communication with the public.
There is not much effort in collecting these data, and we (scientists) can help
by communicating in issues in which their is a broad interest. And I may end up
doing a best-of of the undergrads complaining on Twitter about how &lt;em&gt;Ecology
101&lt;/em&gt; is hard, that is fun too.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>An additive partition of species-level specificity</title>
     <link href="http://timotheepoisot.fr/2013/10/15/specificity-range-measures/"/>
    <updated>2013-10-15T00:00:00-04:00</updated>
    <content type="html">&lt;p&gt;A while ago, we published a paper in &lt;em&gt;Methods in Ecology &amp;amp; Evolution&lt;/em&gt;,
&lt;a href=&quot;http://timotheepoisot.fr/2012/01/19/measuring-ecological-specificity/&quot;&gt;comparing different ways to estimate ecological
specificity&lt;/a&gt;
on binary (presence/absence) and quantitative data. All the measures we
review, and the one we introduce, work in the same way: for an organism,
species, population, you have an array &lt;strong&gt;P&lt;/strong&gt; of its performances across &lt;em&gt;R&lt;/em&gt;
different environments, which is (1) sorted decreasingly, and (2) ranged so
that &lt;strong&gt;P&lt;/strong&gt;[1] = 1. In this paper, we recommend to use either &lt;em&gt;RR&lt;/em&gt; (which is
a modification of Schoener&#39;s generality), and &lt;em&gt;PDI&lt;/em&gt; (which is first
formally introduced in this paper), which measures how fast
performance decreases when you move away from the optimal environment
(the faster it decays, the more specialized you are).&lt;/p&gt;

&lt;p&gt;The main advantage of &lt;em&gt;PDI&lt;/em&gt; is that it relates extremely well to the
&lt;em&gt;definition&lt;/em&gt; of what specificity is: a rapid decay of performance on resources
which are not your own optimum (regardless of the &lt;em&gt;absolute&lt;/em&gt; performance on
this optimal resource, which is why we set &lt;strong&gt;P&lt;/strong&gt;[1] = 1). &lt;em&gt;PDI&lt;/em&gt; is
nothing more that the sum of performance lost by not being on the optimal
resource, divided by the number of non-optimal resources. Somewhere in the end
of the paper, we write that when the data are binary, &lt;em&gt;RR&lt;/em&gt; and &lt;em&gt;PDI&lt;/em&gt; are
equivalent. For some reason that still eludes me, the demonstration did not
make its way in the final version of the paper. So let&#39;s start with it. First,
for an organism of which the performances are measured in &lt;em&gt;R&lt;/em&gt;
environments, of which it has a performance higher than 0 in &lt;em&gt;r&lt;/em&gt;, we
define &lt;em&gt;RR&lt;/em&gt; as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; RR = \frac{R-r}{R-1} &lt;/script&gt;


&lt;p&gt;and &lt;em&gt;PDI&lt;/em&gt; as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; PDI = \frac{\sum_{i=2}(\mathbf{P}_1-\mathbf{P}_i)}{R-1} &lt;/script&gt;


&lt;p&gt;It&#39;s easy to prove that if all values of &lt;strong&gt;P&lt;/strong&gt; are either 0 or 1, then &lt;em&gt;PDI&lt;/em&gt;
= &lt;em&gt;RR&lt;/em&gt;. In this situation, the species in question has &lt;em&gt;r&lt;/em&gt; times 1 in &lt;strong&gt;P&lt;/strong&gt;,
and &lt;em&gt;R-r&lt;/em&gt; times 0 in &lt;strong&gt;P&lt;/strong&gt;. In other words, for the first &lt;em&gt;r&lt;/em&gt; elements of
&lt;strong&gt;P&lt;/strong&gt;, &lt;strong&gt;P&lt;/strong&gt;[1]-&lt;strong&gt;P&lt;/strong&gt;[i] = 0, and for the remaining (&lt;em&gt;R-r&lt;/em&gt;) elements,
&lt;strong&gt;P&lt;/strong&gt;[1]-&lt;strong&gt;P&lt;/strong&gt;[i] = 1. The numerator of &lt;em&gt;PDI&lt;/em&gt; is thus &lt;em&gt;R-r&lt;/em&gt;, so the
expression of &lt;em&gt;PDI&lt;/em&gt; is the same as the expression of &lt;em&gt;RR&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Previously, we and others discussed the fact that specificity ought to be
separated in its &quot;interactions&quot; and &quot;impacts&quot; components, &lt;em&gt;i.e.&lt;/em&gt; (i) how many
resources do I exploit, and (ii) how are my performances distributed. With &lt;em&gt;RR&lt;/em&gt;
and &lt;em&gt;PDI&lt;/em&gt;, it might seem that we have the adequate tool to measure both sides
of the specificity. There&#39;s more to do with these measures, though. Because
&lt;em&gt;RR&lt;/em&gt; is included in &lt;em&gt;PDI&lt;/em&gt; (it&#39;s easy to show that &lt;em&gt;PDI&lt;/em&gt; &amp;geq; &lt;em&gt;RR&lt;/em&gt;), &lt;em&gt;PDI&lt;/em&gt; also
measures an &quot;association&quot; component of specificity. But then again, the
specificity of impact makes sense if you first consider the specificity of
associations before. So we can start thinking of this problem as an additive
partition of specificity:&lt;/p&gt;

&lt;p&gt;Specificity = associations + impacts&lt;/p&gt;

&lt;p&gt;Luckily, we know that &lt;em&gt;RR&lt;/em&gt; measures the specificity of associations, and the
impacts are accounted for by &lt;em&gt;PDI&lt;/em&gt;, so in measurable terms, the above becomes&lt;/p&gt;

&lt;p&gt;&lt;em&gt;PDI&lt;/em&gt; = &lt;em&gt;RR&lt;/em&gt; + &lt;em&gt;I&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This additive partition makes sense, because if you go back to the
demonstration of &lt;em&gt;PDI = RR&lt;/em&gt;, it&#39;s straightforward that whenever performances
are different across resources, the first &lt;em&gt;r&lt;/em&gt; elements of &lt;strong&gt;P&lt;/strong&gt;[1]-&lt;strong&gt;P&lt;/strong&gt;[i]
will be &lt;em&gt;greater&lt;/em&gt; than 0, so the numerator will be &lt;em&gt;greater&lt;/em&gt; than &lt;em&gt;R-r&lt;/em&gt;, so
&lt;em&gt;PDI&lt;/em&gt; will be superior or equal to &lt;em&gt;RR&lt;/em&gt;. How much greater it will be is the
importance of different performances on specificity. Or in other words, when
you do not have informations to measure specificity of impacts, all of the
specificity is because of associations.&lt;/p&gt;

&lt;p&gt;Now what? Well, we can start measuring how much of the total specificity
(&lt;em&gt;PDI&lt;/em&gt;) is due to specificity of associations only (&lt;em&gt;RR&lt;/em&gt;/&lt;em&gt;PDI&lt;/em&gt;), and how much
                                                          is due to specificity
                                                          of impacts only
                                                          (&lt;em&gt;I&lt;/em&gt;/&lt;em&gt;PDI&lt;/em&gt;). As far
                                                                as I can tell,
                                                                the couple
                                                               &lt;em&gt;PDI&lt;/em&gt;-&lt;em&gt;RR&lt;/em&gt; is
                                                               the only pair of
                                                               measures that
                                                               allow to do that
                                                               (though there
                                                               are other
                                                               approaches) in
                                                               a context where
                                                               you can show
                                                               that one measure
                                                               is a component
                                                               of the other.
                                                               Here is a &lt;code&gt;R&lt;/code&gt;
                                                               function to do
                                                               it using the
                                                               &lt;code&gt;ESM&lt;/code&gt; package
                                                               (&lt;a href=&quot;https://r-forge.r-project.org/R/?group_id=593&quot;&gt;from
                                                               R-Forge&lt;/a&gt;).&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;library&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;ESM&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

sap &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;P&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
   Spe &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; pdi&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;P&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   Ass &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; rr&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;P&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   Imp &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Spe &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; Ass
   &lt;span class=&quot;kr&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;list&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
      specificity  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Spe&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      associations &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Ass&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      impacts      &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Imp&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      assoc_rel    &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Ass &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; Spe&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      impac_rel    &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Imp &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; Spe
      &lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let&#39;s do some simple examples, and contrast to species with the same number of
resources, but a very high performance on resource 2, or a very low performance
on resource 3:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; unlist&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;sap&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;c&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.999&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
  specificity associations      impacts    assoc_rel    impac_rel 
  &lt;span class=&quot;m&quot;&gt;0.500500000&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;0.500000000&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;0.000500000&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;0.999000999&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;0.000999001&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; unlist&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;sap&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;c&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
  specificity associations      impacts    assoc_rel    impac_rel 
  &lt;span class=&quot;m&quot;&gt;0.9995000&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;0.5000000&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;0.4995000&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;0.5002501&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;0.4997499&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In the second example, because the performance on the second resource is much
almost zero, the &lt;em&gt;RR&lt;/em&gt; and &lt;em&gt;I&lt;/em&gt; components are almost as important to understand
specificity (whereas it&#39;s not the case in the first example, wherein you can
assume that &lt;strong&gt;P&lt;/strong&gt;[2] = 1 without changing the big picture.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>R, priority effects, and the meta-package</title>
     <link href="http://timotheepoisot.fr/2013/10/12/r-priority-responsibility/"/>
    <updated>2013-10-12T00:00:00-04:00</updated>
    <content type="html">&lt;p&gt;R packages are a superb opportunity to use new methods, and if anything, we
should only rejoice that there are so many of them in ecology. Yet, there is
a point that occurred to me over recent experiences (reviewing code for a few
papers recently, and reading the help page about &lt;a href=&quot;http://www.inside-r.org/packages/cran/bipartite/docs/PDI&quot;&gt;one of my own metrics
in the &lt;code&gt;bipartite&lt;/code&gt; package&lt;/a&gt;), and that I think is rarely discussed.&lt;/p&gt;

&lt;p&gt;When a R package presenting a series of measures becomes available, it will
become the &lt;em&gt;gold standard&lt;/em&gt; of future analyzes in this field. This is an
extremely strong effect. French students in ecology use the (French-developed)
&lt;code&gt;ade4&lt;/code&gt; package, while people in QuÃ©bec use the &lt;code&gt;vegan&lt;/code&gt; package (most likely
because of its ties with the &lt;em&gt;Numerical Ecology&lt;/em&gt; book, as Gavin
Simpson mentioned in the comments). There is a priority effect when
a package is released, and metrics that are not implemented in this package,
because they are more difficult to apply, will be less widely used. This is
a central point for methodological research papers. Describing the method is
a small fraction of the work. People are more likely to apply what is
proposed if there is a tool to do it, and as a consequence, (&lt;em&gt;I assume
that&lt;/em&gt;) methods are used more if there is an easy and well-known way to
apply them.&lt;/p&gt;

&lt;p&gt;This is especially true of large-scale packages, that aim to become your single
interface with the metrics and analyzes in a domain. Ecologists are familiar
with &lt;code&gt;bipartite&lt;/code&gt;, &lt;code&gt;vegan&lt;/code&gt;, &lt;code&gt;picante&lt;/code&gt;, &lt;code&gt;ape&lt;/code&gt;, for example. These &quot;mega-packages&quot;
implement a lot of measures, and for this reason, some will look through the
manual to see what kind of analyzes they can do. I most likely still do it
myself, especially so for side-projects where I&#39;m not entirely familiar with
the methodological literature yet. So based on my experience, this is
particularly problematic for newcomers to a field, because when you don&#39;t know,
you assume that the default values were chosen as defaults by
someone knowing what he was doing. Then I remember the &lt;a href=&quot;http://www.python.org/dev/peps/pep-0020/&quot;&gt;&lt;em&gt;Zen of
Python&lt;/em&gt;&lt;/a&gt;: &lt;em&gt;&quot;In the face of ambiguity, refuse the temptation
to guess.&quot;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;And as we all have different sensibilities, it shows in the way we think about
code, and in the way we write it. The default options of a function, for
example, can be a matter of choice, and especially so when this function
becomes as broad as &quot;measure diversity&quot;. This partiality is not an issue when
developing tools that you will use for your own research. But when developing
tools intended for a broad audience, then this code becomes a service, and it
is our duty as service providers that the defaults reflect our own biases the
least, and the current consensus the most.&lt;/p&gt;

&lt;p&gt;This is why extremely big packages &lt;em&gt;can&lt;/em&gt; be a problem under some circumstances:
they break Doug McIlroy&#39;s &lt;em&gt;Unix philosophy&lt;/em&gt;:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;This is the Unix philosophy: &lt;em&gt;Write programs that do one thing&lt;/em&gt; and do it
well. &lt;em&gt;Write programs to work together&lt;/em&gt;. Write programs to handle text
streams, because that is a universal interface.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;When we, as users, rely on a big package to do our work, we assume that every
component of this package is (i) well implemented, (ii) conform to the original
paper, and (iii) up-to-date. Given the frequency of commits in the biggest
ecological packages, I&#39;m sure this is the case, but it&#39;s worth keeping in mind.
My personal preference goes to using a lot of packages doing a small number of
things. For my two papers relying on heavily on the development of comparison
of new methods, I started by writing the R package (well I started at the
drawing board, but that part is not relevant), then used the package to
perform the analyzes described in the paper. The idea is that whenever someone
will (hopefully) read the paper, there will be a package doing &lt;em&gt;only&lt;/em&gt; what is
described in the paper.&lt;/p&gt;

&lt;p&gt;And I understand that some people would rather have one big package doing
everything they need. There are arguments for that of course, namely the fact
that with as many users, errors are most likely to be uncovered, and the
quality of the package will increase (and the state of the R packages in
ecology tends to confirm that). But on the other hand, when you increase
the number of pieces, you increase the probability that one of them will behave
in a way you would not have anticipated. With this regard, packages that
require you to do a lot of small steps by hand have a great advantage, you know
what is happening in real time (then again, I like C and doing every thing by
hand, so perhaps I just enjoy hurting myself).&lt;/p&gt;

&lt;p&gt;In any case, the &quot;default&quot; behavior of programs is something that should be
held under intense scrutiny by the community, and improved code review will
definitely help. This is also where big packages can somehow escape the system:
even if they are reviewed formally (as opposed to being reviewed informally,
which they are in real time as people use them), this will not happen for
each release, and some new releases may introduce things that can be argued
against. In short, my point is: R packages (or packages in other languages, but
you get the idea) tend to rapidly represent the &quot;canon&quot; of the analyzes
to perform, and for this reason, we should always keep a critical eye on them.
Also, keep on writing open-source code, that will enrich the code base
available to the community.&lt;/p&gt;

&lt;p&gt;With that in mind, what would my &lt;em&gt;ideal&lt;/em&gt; super-big package be? Ideally, it
would only be a wrapper around other packages, each doing a single task, &lt;em&gt;Unix
philosophy&lt;/em&gt;-style. Proudly following the ecological tradition of sticking
&lt;em&gt;meta&lt;/em&gt; to every non-meta thing, it would obviously be called a &lt;em&gt;meta-package&lt;/em&gt;.
It would automate the process of taking data from one format to the other, and
propose pipelines to do automated analysis, but with users checking each step
of the process. As such a package would glue together other programs, there
will be little development required. The role of the maintainers, however,
would be to vet the new releases of the packages, find suitable additions,
and so forth. This would be a good example of the community coming
together to merge disparate tools. Social coding platforms allow to do
this almost effortlessly, and it would have the benefit of showing &lt;em&gt;how
the sausage is made&lt;/em&gt;, since each individual package would be usable on its
own. This would also shift the balance towards a system in which there is
a strict one paper / one package relationship, which I think would
considerably clarify the type of analyzes included in a package, in
addition to matching these analyzes to the primary literature.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>Inferring the structure of food webs</title>
     <link href="http://timotheepoisot.fr/2013/09/21/predicting-network-structure/"/>
    <updated>2013-09-21T00:00:00-04:00</updated>
    <content type="html">&lt;p&gt;We&#39;ve just published a paper in &lt;em&gt;Methods in Ecology and Evolution&lt;/em&gt;, about the
&lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12103/abstract&quot;&gt;inference of food web structure based on species trait data&lt;/a&gt;. That&#39;s
a seriously cool result (we think). There is a serious challenge when you work
with food webs: you need to know which species are interacting, and to do so,
you need to observed interactions in the fields. It&#39;s challenging, because
it must be done extensively, so as not to miss interactions.&lt;/p&gt;

&lt;p&gt;But on the other hand, there are well described relationships between traits of
interacting species. In the particular case of food webs, notably, there is
a log-log relationship between the body size of a predator and the body size of
its prey. The existence of this relationship can be used to, given
a distribution of body sizes, predict the interactions within a species
assemblage.&lt;/p&gt;

&lt;p&gt;So what we have been doing, is designing a statistical method to, given a list
of species and body size, callibrate a model of food web structure, to predict
the interactions. As we discuss in the paper, the more traits you put in your
prediction, the better it will get. Using a dataset of fishes, we generated
a convicing network using only body size and bathymetry.&lt;/p&gt;

&lt;p&gt;We think this result is cool, because by opposition to the &quot;observational&quot;
method of constructing food webs, you only need to know (well) a small number
of interactions. As we show in the paper, whenever you have a high RÂ² of the
trait-trait-interaction relationship, the infered network is close to the
observed one. These methods can therefore allow to generate rough estimates of network structure.&lt;/p&gt;

&lt;p&gt;Interestingly, they will also help in the generation of predictions. As we
illustrate in the paper, we can tweak the body size distribution, and see how
much the network structure changes. For example, this can help simulate the
effect of body size decrease through over-fishing, or the extinction of the
larger species. There is still a bit of work to do to be able to predict the
structure of other types of networks, but it was definitely comforting to see
that, even with straightforward statistics, we were able to achieve it.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>Is programming an acquired taste?</title>
     <link href="http://timotheepoisot.fr/2013/09/11/programming-innate-acquired/"/>
    <updated>2013-09-11T00:00:00-04:00</updated>
    <content type="html">&lt;p&gt;Two weeks ago, the whole lab headed up to a remote place in the Rimouski
countryside for a summers school on &lt;em&gt;Ecological Modeling&lt;/em&gt;. It was a whole lot
of fun, and a really good way to get back to some things I&#39;ve always wanted to
study more in depth. So far, so good. Most of the days were courses in the
morning, and programming in the afternoon. There were also some pen-and-paper
model solving, but I won&#39;t talk much about these.&lt;/p&gt;

&lt;p&gt;While the students were sweating on the programming problems, we discussed the
fact that programming is maybe an innate skill. Not in the sense that some
people are unable to learn how to code functional things, but in the sense that
some people find it intuitive to express things in terms of &lt;code&gt;for&lt;/code&gt;, &lt;code&gt;while&lt;/code&gt;,
&lt;code&gt;if&lt;/code&gt;, and so forth (so forth is &lt;em&gt;not&lt;/em&gt; a valid algorithmic structure). That
may be true, that some people will find it easier to split up a large
problem in a series of short instructions. If you can do that intuitively,
then you&#39;ll probably have no problem structuring your code.&lt;/p&gt;

&lt;p&gt;But this particular skill can be trained, I think. One of the things I asked of
the students was to code something implementing Williams &amp;amp; Martinez &lt;em&gt;niche
model&lt;/em&gt; of food webs. The title of the assignment may be &quot;Write a program
implementing the NM in R&quot;. I took the opportunity of turning that into an
exercise about how to organize code. The whole problem can be easily
sub-divided into a series of smaller problems. In this case, we first need to
generate species, then to find out the interactions between them. The first
step, of generating species, itself involves several sub-steps, that can each
go into a function, or can be handled by logical tests.&lt;/p&gt;

&lt;p&gt;Perhaps there will be a &lt;em&gt;happy few&lt;/em&gt; that will take this series of steps by
themselves. Looking back on the first programs I wrote (I am unlucky enough to
have kept some of my spaghetti code from my first year of Masters, ...),
I&#39;m definitely not one of these people. As in most things, the key to
developing a feeling of how to organize things will come with practice, and
thinking about what you felt comfortable with. Which is why I always
encourage people to &lt;a href=&quot;http://timotheepoisot.fr/2013/04/12/learn-code-ecology/&quot;&gt;write code when they want to learn&lt;/a&gt;. I seriously
don&#39;t think there is a better way to increase your skills.&lt;/p&gt;

&lt;p&gt;So, is programming an acquired taste? Probably. It may seems frustrating at
first to need to specify things clearly at a very small scale. Some people may
call it intuitive, and be comfortable with the &lt;code&gt;if&lt;/code&gt;s and the &lt;code&gt;while&lt;/code&gt;s really
quickly, but I don&#39;t think these are majority. And that might explain why it&#39;s
not uncommon to hear students say that they aren&#39;t able to code well (aside
from the fact that, well, monkeys and typewriters...). Learning how to
code is like learning a language with a different grammatical structure. On
your first day, Python and &lt;a href=&quot;http://en.wikipedia.org/wiki/Brainfuck&quot;&gt;brainfuck&lt;/a&gt;
look awfully similar. Things you conceptualize easily will not translate well
in your editor because you need to use a different structure of thought to
express them. The more you practice, the more natural it will seem. For this
reason, I don&#39;t really believe in programming classes (for biologists, I don&#39;t
know how it goes in other majors) in which you only need to write-down
some code, press enter, and see that it does what your instructor says it does.
Letting students experiment, and fail, and sort out their experiences of what
worked, what did not, and what they felt comfortable with, could probably do
a lot to improve coding literacy.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>Teach more maths to the undergrads!</title>
     <link href="http://timotheepoisot.fr/2013/08/28/lack-quantitative-training/"/>
    <updated>2013-08-28T00:00:00-04:00</updated>
    <content type="html">&lt;p&gt;Fred Barraquand, a bunch of INNGE-and-friends-of-INNGE people, and I, just
published a &lt;a href=&quot;https://peerj.com/preprints/53/&quot;&gt;preprint&lt;/a&gt; at &lt;em&gt;Peer J&lt;/em&gt; about quantitative training in
early-career ecologists (or rather, the lack thereof). Talking about how much
maths is needed during training is all the rage in the ecological community,
and both sides of the argument have really good arguments.&lt;/p&gt;

&lt;p&gt;So Fred decided to do a survey of early-career (and older) ecologists through
&lt;em&gt;ECOLOG-L&lt;/em&gt;. Quoting directly from the abstract, this is what we found:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We found a clear self-perceived lack of quantitative training: &lt;strong&gt;75% are not
satisfied with their understanding of mathematical models&lt;/strong&gt;; 75% feel the level
of mathematics was âtoo lowâ in their ecology classes; 90% wanted more
mathematics classes for ecologists; and 95% more statistics classes.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;There are a lot of number to process, but basically all point to the same
message: with a few more years of experience after your undergraduate degree,
you wish you have studied more maths (it&#39;s especially true for me). We
conclude the paper by exploring recommendations about how to (re)design
curriculum using the results from this survey.&lt;/p&gt;

&lt;p&gt;I discussed this result with my B.Sc. class last fall. We had a fun exchange,
and over time, the polarization between animal-watchers and number-crunchers
(the 2 or 3 of them) decreased. But here is the fun thing. When I asked the
students whether the found plausible that they&#39;ll wish for strong
quantitative training in a few years, they said yes. Immediately afterwards,
when I asked what they would do if we increased the part of quantitative
training up to 30% of all classes, as survey respondent suggested. To a vast
majority, they would not enroll.&lt;/p&gt;

&lt;p&gt;So, it seems like we&#39;ll need to sneakily teach quantitative skills...&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>In defense of narrative reviews</title>
     <link href="http://timotheepoisot.fr/2013/08/24/in-defense-narrative-reviews/"/>
    <updated>2013-08-24T00:00:00-04:00</updated>
    <content type="html">&lt;p&gt;Early in July, Christopher Lortie published a really great preprint on &lt;em&gt;PeerJ&lt;/em&gt;,
about the &lt;a href=&quot;https://peerj.com/preprints/39/&quot;&gt;opportunities of systematic reviews for ecology&lt;/a&gt;. It&#39;s
extremely insightful, and a good read. The abstract starts by &lt;em&gt;Narrative
reviews are dead&lt;/em&gt;. No they&#39;re not, nor shouldn&#39;t they be! &lt;em&gt;Narrative&lt;/em&gt; reviews are those we are used to read now: a story built on the
collection of some pieces of litterature. On the other hand, systematic reviews
or meta-analyses are, well, systematic, and try to reach a consensus (at least
a generalizable result) through the analysis of everything published on
a particular topic (I abusively generalize, but go read the paper and you&#39;ll
get the idea).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;&lt;em&gt;There is something fascinating about science. One gets such wholesale returns of conjecture out of such a trifling investment of fact.&lt;/em&gt; &lt;br/&gt;
Mark Twain&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I get the idea that narrative reviews are more likely to reflect the author(s)
personal biases or interests, and in our quest for absolute impartiality, it
would make sense to get rid of this. This is the whole point of Chris&#39;s
paper: as the goal of a review is to give an overview of the
state-of-the-art in a particular field, then non-narrative reviews are the
&lt;em&gt;better&lt;/em&gt; tool. Nonetheless, I think this is an over-simplification of the
way people &lt;em&gt;use&lt;/em&gt; review papers.&lt;/p&gt;

&lt;p&gt;As a Ph.D. candidate, &lt;em&gt;narrative&lt;/em&gt; reviews helped me a lot when I was trying to
get an idea of the history of the field. Being able to understand how current
ideas emerged from previous observations and results, and seeing which papers
influenced which authors, was important when I had to develop my own research
questions. I won&#39;t spend time on the argument that narrative reviews are great
teaching material, and that it&#39;s a gentler introduction to a field to be walked
through a concept rather than to read through tables of ANOVA. I enjoy these
much more when I have an idea of the &quot;story&quot; behind.&lt;/p&gt;

&lt;p&gt;Essentially, narrative reviews should not be discarded because they serve
a different purpose from systematic reviews. Let&#39;s take the example of the
&lt;a href=&quot;http://en.wikipedia.org/wiki/Rare_biosphere&quot;&gt;&lt;em&gt;rare biosphere&lt;/em&gt;&lt;/a&gt; in environmental microbiology. For several
historical reasons, it has been hypothesized that the distribution of
biodiversity has a long-tail, with a lot of species being in extremely low
abundances. Classical sampling and enumeration technique only described the tip
of the iceberg. Fast forward a few years later, and the existence of the rare
biosphere is well established, and people start questioning its impact in
ecosystem processes, among other things. How we switched from &quot;it probably
exists&quot; to &quot;we need to understand what it&#39;s doing&quot; is best understood with
a little bit of narration. Specifically, the development of molecular biology
methods, and recently high-throughput sequencing, increased our detection
ability by order of magnitudes. The emergence of new tools (which in the
perspective of people like &lt;a href=&quot;http://www.scribd.com/doc/28780545/Gutting-Bachelard-s-Philosophy-of-Science-1987&quot;&gt;Bachelard, only reflects the emergence of new
theories&lt;/a&gt; that progressively materialized) allowed a switch in the way
people studied rare biosphere. A &lt;em&gt;narrative&lt;/em&gt; review of the question will likely
put this element forward.&lt;/p&gt;

&lt;p&gt;A systematic review will hold different informations. While the narrative
review aks &lt;em&gt;How did we get there?&lt;/em&gt;, the systematic review asks &lt;em&gt;Where are we
now?&lt;/em&gt;. They give different answers, but it&#39;s hard to argue that one of these
questions has more merit than the other. Interestingly, more and more reviews
conclude by asking &lt;em&gt;Where to next?&lt;/em&gt;. Both narrative and systematic reviews can
bring different elements, and thus will likely bring different responses. This
multiplicity of points of views should not be so rapidly dismissed. Narrative
reviews should be continued &lt;em&gt;because&lt;/em&gt; they are often not entirely impartial,
reflects biases or personnal preferences, and because they tell
stories. Once we are familiar with this story, and once we know the
different interpretations that people make of the same data, it&#39;s time
to go read the systematic reviews, and the meta-analyses. I appreciate
that people will enjoy one of these more than the other, or would
prefer to read statistics before naration. But I think we should
continue reading, and writing, narrative reviews.&lt;/p&gt;

&lt;p&gt;As a final point, whatever you may think of the issue, go read the &quot;companion&quot;
paper about the &lt;a href=&quot;https://peerj.com/preprints/38/&quot;&gt;&lt;em&gt;interpretation&lt;/em&gt;&lt;/a&gt; of meta-analyses in ecology. It&#39;s
a really good introduction.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>Network structure of obligate and facultative parasites</title>
     <link href="http://timotheepoisot.fr/2013/08/20/network-structure-obligate-facultative-parasites/"/>
    <updated>2013-08-20T00:00:00-04:00</updated>
    <content type="html">&lt;p&gt;We have a new paper &lt;a href=&quot;http://dx.doi.org/10.1017/S0031182013000851&quot;&gt;online at &lt;em&gt;Parasitology&lt;/em&gt;&lt;/a&gt;, on the differences of
the structure between rodents hosts, and their obligates of facultatives
parasites. We had data from a really cool rodents-ectoparasites system, in
which the same hosts have both facultative and obligate parasites. These data
were also collected in different locations, and at different times.&lt;/p&gt;

&lt;p&gt;There are two important things in the results. First, the networks of obligate
and facultative parasites look &lt;em&gt;nothing&lt;/em&gt; alike. One (obligate) is highly
modular and anti-nested, and the other (facultative) is highly nested but not
particularly more or less modular. But when you look at all types of parasites
together, the resulting networks are both more moudular &lt;em&gt;and&lt;/em&gt; more nested than
expected by chance. The take-home of this particular result is: &quot;parasitism&quot; is
a vague notion, and we should not be too surprized when lumping contrasted
ecological strategies makes weird results emerge (also, we should focus on how
different functional groups affect network properties).&lt;/p&gt;

&lt;p&gt;The second part of the result is most likely more appealling to
parasitologists: we show that the networks of obligate parasites vary less in
space and time that the network of facultative ones. Not too suprising when you
think about it long enough: facultative parasites are mostly opportunisitc, and
thus they will probably pick their hosts more randomly (without less
constraints) than obligate ones. There&#39;s also the fact that facultative
parasites tend to occupy sites within the hosts that require less stringent
adaptations. But the key message is still the same: when we lump different
classes of organisms together in a single network, it&#39;s important to quantify
how much each contribute to the overall properties.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>Complexity and connectance of ecological networks</title>
     <link href="http://timotheepoisot.fr/2013/08/19/connectance-complexity/"/>
    <updated>2013-08-19T00:00:00-04:00</updated>
    <content type="html">&lt;p&gt;So my &lt;a href=&quot;http://timotheepoisot.fr/2013/06/19/notes-on-network-complexity/&quot;&gt;blog post on ecological complexity in networks&lt;/a&gt; turned into
a manuscript, now online as a preprint at &lt;a href=&quot;https://peerj.com/preprints/50/&quot;&gt;&lt;em&gt;PeerJ&lt;/em&gt;&lt;/a&gt;. We&#39;ll send it up for
review (at &lt;em&gt;PeerJ&lt;/em&gt; also) in a while, but feel free to read it and give us
feedback using the &lt;a href=&quot;https://github.com/tpoisot/ms_connectance_complexity/issues&quot;&gt;github issues tracker&lt;/a&gt;. We&#39;d like that very much,
actually. And as the github repository is public, feel free to
fork/edit/push modifications if you have something to contribute to
the paper.&lt;/p&gt;

&lt;p&gt;In a nutshell; we argue that connectance is still a central property of
networks. For one thing, (i) it constrains how much different networks exists
for a given species number, which was the point of the original post. But more
importantly, (ii), because a given connectance implies a given number of
interactions, connectance imposes structural constraints on networks
properties. Or in other words, all important moments of the degree distribution
vary with connectance. And it works quite well with Erdos-Renyi graphs, and
with the famous &quot;niche&quot; model of food webs.&lt;/p&gt;

&lt;p&gt;It&#39;s kinda cool, because connectance is &lt;em&gt;simple&lt;/em&gt; to measure, and even people
that no background in network analysis will get what it means. It also means
that from a purely statistical point of view, any effect of the degree
distribution must be controlled for an effect of connectance.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;P.S.&lt;/em&gt;: This is a good example of &lt;a href=&quot;http://timotheepoisot.fr/2013/08/05/codelust/&quot;&gt;Codelust&lt;/a&gt; in practice. The
original script were written over a particularly greasy and disappointing club
sandwich eaten alone in my office, so at least something productive came out of
this particularly bleak view of the academic daily life...&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>Writing manuscripts in Linux (like a boss)</title>
     <link href="http://timotheepoisot.fr/2013/08/18/writing-linux-like-a-boss/"/>
    <updated>2013-08-18T00:00:00-04:00</updated>
    <content type="html">&lt;p&gt;Hey, it&#39;s been a year since I &lt;a href=&quot;http://timotheepoisot.fr/2012/08/03/new-linux-setup/&quot;&gt;switched to Linux&lt;/a&gt;. I&#39;ve tried a few
different distros, and finally settled on &lt;a href=&quot;https://www.archlinux.org/&quot;&gt;Arch Linux&lt;/a&gt; with
&lt;a href=&quot;http://openbox.org/&quot;&gt;OpenBox&lt;/a&gt;. It&#39;s ligthweight, uncluttered, and I love. But that&#39;s not what
I&#39;m going to talk about.&lt;/p&gt;

&lt;p&gt;I may have mentionned in the past that I used &lt;em&gt;Sublime Text 2&lt;/em&gt;. During my quest
for ever-increasing minimalism, I&#39;ve made the switch to &lt;code&gt;vim&lt;/code&gt;. And I now have
a &lt;em&gt;cool&lt;/em&gt; writing setup. Most of what I do is writing markdown files, and using
makefiles to use pandoc to convert them to PDF (read more about it
&lt;a href=&quot;http://timotheepoisot.fr/2013/05/18/make-pandoc/&quot;&gt;here&lt;/a&gt;). The only thing that missed was a lightweight PDF reader,
able to auto-refresh things. I have now found &lt;a href=&quot;http://pwmt.org/projects/zathura/&quot;&gt;&lt;em&gt;zathura&lt;/em&gt;&lt;/a&gt;. I am supper happy.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;zathura&lt;/em&gt; is command-line based, and uses keyboard for interaction. So whenever
I am working on a document in a terminal (&lt;code&gt;rxvt-unicode&lt;/code&gt;, in case you wonder,
with the &lt;em&gt;excellent&lt;/em&gt; font &lt;a href=&quot;http://damieng.com/blog/2008/05/26/envy-code-r-preview-7-coding-font-released&quot;&gt;Envy Code R&lt;/a&gt;), I just need to do a quick
&lt;code&gt;zathura my-doc.pdf&lt;/code&gt;, and I can see the output. Because &lt;em&gt;zathura&lt;/em&gt;
auto-refreshes things, whenever I &lt;code&gt;make pdf&lt;/code&gt;, the output is up to date. Just
like that.&lt;/p&gt;

&lt;p&gt;And the point is: the productivity boost is amazing. I do everything from within the terminal (or rather, terminals, I have a few of them open at any time). And it&#39;s fast, ligthweight, and &lt;em&gt;responsive&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;And just in case you are curious&lt;/strong&gt;, a list of the &lt;em&gt;vim&lt;/em&gt; plugins I use:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/tpope/vim-pathogen&quot;&gt;pathogen&lt;/a&gt; to easily install plugins&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/airblade/vim-gitgutter&quot;&gt;vim-gitgutter&lt;/a&gt; to see how my local file differs from the git version&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/vim-pandoc/vim-pandoc&quot;&gt;vim-pandoc&lt;/a&gt; for a lot of pandoc-related stuff, including &lt;em&gt;bibtex reference auto-completion&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
  </entry>
  
  <entry>
     <title>F1000Research announces new Ecology section</title>
     <link href="http://timotheepoisot.fr/2013/08/08/F1000R-ecology-section/"/>
    <updated>2013-08-08T00:00:00-04:00</updated>
    <content type="html">&lt;p&gt;F1000Research just put up a &lt;a href=&quot;http://blog.f1000research.com/2013/08/08/ecologists-join-our-open-science-ecosystem/&quot;&gt;new blog post&lt;/a&gt; to announce that they
will concentrate some of their efforts to increase the proportion of ecology
papers they publish. We should all be excited about this. First, the article
processing fees will be waived until 2014, so here goes a good opportunity to
see for yourself how this journal works. Second, the publication model of
F1000Research is extremely interesting. After initial checking, the paper is
put on-line, and appears as a preprint. Reviews appear in real-time, and are
not anonymous. Further changes to the paper are well documented.&lt;/p&gt;

&lt;p&gt;As we discuss in &lt;a href=&quot;http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001563&quot;&gt;Phil&#39;s paper&lt;/a&gt; paper on preprints in biology, this
immediate publication model is good, especially for early-career people. When
you need to have something to show for your work in order to apply for
a fellowship, it&#39;s understandable that you don&#39;t want to wait for up to a whole
year for your paper to run the gauntlet of peer review. F1000Research also
integrates tightly with figshare, and datasets are required to be open when the
paper is submitted. That&#39;s a really good practice, and it will further the
integration between datasets and papers.&lt;/p&gt;

&lt;p&gt;One of the coolest things for field ecologists (I&#39;ve been informed that ecology
is the study of populations, and not a branch of applied mathematics or
computer sciences...?) is that F1000Research will now offer to publish
&lt;em&gt;observation articles&lt;/em&gt;. People in the medical sciences have many case studies,
that are extremely important to the field as a whole. In ecology, despite
the recognition that general laws are elusive and not as general as they
appear, there seems to be a shortage of places to publish interesting
field/lab observations. I&#39;m sure many people will be deligthed to learn that
the modern publishing world still cares about the naturalist tradition of
ecology.&lt;/p&gt;

&lt;p&gt;So you can head to the &lt;a href=&quot;http://f1000research.com&quot;&gt;F1000Research&lt;/a&gt; website and
see for yourself!&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>Codelust</title>
     <link href="http://timotheepoisot.fr/2013/08/05/codelust/"/>
    <updated>2013-08-05T00:00:00-04:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Wanderlust&lt;/strong&gt; &lt;em&gt;(noun)&lt;/em&gt;: a strong desire to travel.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Codelust&lt;/strong&gt; &lt;em&gt;(noun)&lt;/em&gt;: a strong desire to code rather than actually work, and how you can use it for good.&lt;/p&gt;

&lt;p&gt;I&#39;m lucky. I have stacks of datasets on my laptop, and good ecological
questions to ask, and (hopefully) that will result in a few papers. But that&#39;s
not &lt;em&gt;fun&lt;/em&gt;. I know what I have to do, how I have to do it, and I&#39;m mostly
re-using existing tools. So over the last month or so, I&#39;ve been consumed by
&lt;em&gt;codelust&lt;/em&gt;, a strong desire to open a text editor, and write a program that
does something I&#39;ve never done before.&lt;/p&gt;

&lt;p&gt;At first sight, it&#39;s a complete waste of time. But I&#39;ve noticed that it&#39;s
better to let myself take one hour or two a few days a week, than fight against
the need to do new stuff just for the sake of it, and for a few reasons.&lt;/p&gt;

&lt;p&gt;First, it&#39;s a good way not to grow tired of your current projects. There are
times when you can&#39;t see anymore of a particular dataset or paper for the day,
and it&#39;s better to sharpen your computing skills than roam around the
office trying to find other bored people to socialize with (at least,
it&#39;s more productive in the long term). That&#39;s a good way to keep
your sanity. I&#39;ve always been a big fan of Google&#39;s &lt;a href=&quot;http://www.wisebread.com/why-you-should-follow-googles-20-rule&quot;&gt;20% time
rule&lt;/a&gt;, that states that 20% of your time should be spent on
something &lt;em&gt;not&lt;/em&gt; directly related to what you&#39;re supposed to be. The
precise amount of time dedicated to goofing around will vary. but the
general idea is that you will eventually use this time to do new,
creative things, and you will eventually have a long-term benefit.&lt;/p&gt;

&lt;p&gt;Second, episodes of codelust are good times to try &lt;em&gt;something new&lt;/em&gt;. I&#39;m not
talking about creating something new from a scientific point of view (though
we&#39;ll get to that), but more of trying a new technique, library, or
something like that. It recently led me to replace &lt;code&gt;tsv&lt;/code&gt; by &lt;code&gt;json&lt;/code&gt; as the
output format for a model I&#39;m working on. I&#39;ve learnt a great deal, and
identified new areas for improvement. This will probably help me save time in
other projects, and now I have one more tool in my toolbox.&lt;/p&gt;

&lt;p&gt;Additionaly, codelust (if you&#39;re the theory type) is a good way to get
innovative projects started. What happpens if I change this model in this way?
How does the relationship between these and these outputs reacts to another
factor? With most simple, canonical ecological models, simple questions are
easy to explore. If you&#39;re lucky (failing that, you&#39;ll have to be smart), you
can stumble upon an interesting result that is worthy of being expanded into
a &quot;real&quot; paper. Try not to grow bored of &lt;em&gt;this&lt;/em&gt; paper and engage in more
codelusting, that would probably disprove the point I just made...&lt;/p&gt;

&lt;p&gt;And as a final point, codelust will keep you sharp. It&#39;s easy to write one
R package and use it for a while. Forcing yourself to write new code, using new
methods, every once in a while, will keep you up to date, increase the range of
situations you&#39;ve already covered, and give you a more solid experience.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>Should we share reviews?</title>
     <link href="http://timotheepoisot.fr/2013/07/10/sharing-reviews/"/>
    <updated>2013-07-10T00:00:00-04:00</updated>
    <content type="html">&lt;p&gt;Britt Koskella  &lt;a href=&quot;http://brittkoskella.wordpress.com/2013/07/08/and-the-reviews-are-in/&quot;&gt;uploaded reviews, and her replies&lt;/a&gt;, for her recent
&lt;em&gt;Current Biology&lt;/em&gt; paper (which, by the way, is great). She also announced that
she will do the same thing &lt;em&gt;every time&lt;/em&gt; a paper is published. We&#39;ve had
a discussion in the lab over the few months, about whether or not we should do
almost the same thing: we wanted to put online a very tongue-in-cheek &lt;em&gt;Hall of
fame&lt;/em&gt; of the &quot;worse&quot; comments we&#39;ve received, for papers that ended up being
published. We decided not to make it (public). The &lt;a href=&quot;http://theseamonster.net/2013/05/are-unreasonably-harsh-reviewers-retarding-the-pace-of-coral-reef-science/&quot;&gt;SeaMonster&lt;/a&gt;
also did a very good job of deconstructing bad practices in refereeing, and
discuss how preprints can compensate for the fact that pre-publication review
exists.&lt;/p&gt;

&lt;p&gt;Should we share reviews &lt;em&gt;at all&lt;/em&gt;? If we want to share reviews, in the sense of,
we put them online for all to see, along with the replies, there are
a few things to consider. First, in most situations, it will still not
break the assymetry in the review process. The burden of proof is still
on the author, and by the time you can show how bad a review is (which
is what the &lt;em&gt;SeaMonster&lt;/em&gt; people did in the above reference), the
paper has most likely been rejected. That&#39;s a great way to vent out your
frustration and get support from other people, but it&#39;s not really
helping in getting your paper out there.&lt;/p&gt;

&lt;p&gt;Sharing reviews and replies can also show that you&#39;ve been taking the revisions
seriously, but that&#39;s part of our job description anyway. If you don&#39;t do
mistakes, you&#39;re doing science wrong, and if you don&#39;t correct these mistakes,
you&#39;re doing science &lt;em&gt;really&lt;/em&gt; wrong, or something like that. So posting the reviews and the replies is a good way to contribute some trivia, but I can&#39;t see how it can be really meaningful.&lt;/p&gt;

&lt;p&gt;Now, read the following sentence carefully.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The field has responded with enthusiasm, and reviewers seem to really get it, look for what is good and what needs to be improved in the paper. You donât need to find a reason to reject.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is from &lt;a href=&quot;http://exchanges.wiley.com/blog/2012/07/02/your-starter-for-tenan-interview-with-professor-allen-moore-head-of-the-department-of-genetics-university-of-georgia-athens-usa-and-editor-of-ecology-and-evolution-a-new-open-access-jour/&quot;&gt;Allen Moore&lt;/a&gt;, editor in chief of &lt;em&gt;Ecology and
Evolution&lt;/em&gt;. This single sentence is all the difference between a good and a bad
review. Your task is to help make the paper better (unless the paper is so bad
that it&#39;s not possible to save it, but I&#39;ve probably had only one of
these in my 30+ reviews so far). That&#39;s an even more straightforward
definition of the [&lt;em&gt;Golden Rule&lt;/em&gt;][goldenrule] of reviews: don&#39;t be an assh*le.&lt;/p&gt;

&lt;p&gt;This is where posting reviews can be good: we can (perhaps) make the reviews
better. We can give credit where credit is due, and blame where blame is due.
But that&#39;s only true if the reviews are not anonymized. If we want the sharing
of reviews to have an impact, then it is necessary that reviews are not
anonymous. This is the purpose of journals like &lt;em&gt;F1000Research&lt;/em&gt;. If you want to
see an example, go see the reviews, replies, and revisions, of our paper about
&lt;a href=&quot;http://f1000research.com/articles/1-21/v2&quot;&gt;terminal investment in bacteria&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Will it improve the quality of reviews (and, if better reviews mean better
papers, the quality of the literature)? Results published in BMJ suggest
&lt;a href=&quot;http://www.bmj.com/content/341/bmj.c5729&quot;&gt;it&#39;s not the case&lt;/a&gt;. But there are two striking things with this paper.
First, the rate of &lt;em&gt;refusal&lt;/em&gt; increased when the editors told the reviewers that
their review might be posted, online, in a non-anonymous way. Second, the
reviewers that accepted took, on average, longer to review. And the additional
delay makes sense: if your name is attached to it, then a review is a valid
piece of scientific output, and you will make sure that it&#39;s good.&lt;/p&gt;

&lt;p&gt;Here is how I&#39;d like to have conducted this study. Get all the reviews, and
&lt;em&gt;once they are turned in&lt;/em&gt;, ask the authors if they agree to have them published
online, with their name attached to it. Then have editors rate the reviews from
each group, and check the proportion of positive/negative recommandations in
each group. I&#39;m now signing all of my reviews, even the negative ones. And
check for the age of the people in each group also, just in case. I&#39;m still
hoping it won&#39;t make any difference, but perhaps the really bad kind of reviews
(the biased ones, trashing a paper just because they don&#39;t like it, or because
it contradicts some kind of agenda they set), will be filtered out. You don&#39;t
want the whole world to see that you are a terrible persone (unless you are
a clear sociopath, that is).&lt;/p&gt;

&lt;p&gt;But I feel like that rate of refusal to share the reviews should not be that
high. Fear of future payback might be a valid one, but hopefully with a very
small number of people. And besides, they&#39;re not much to be angry about in
reviews anyway. Because reading through most of them, the &quot;worse&quot; kind of
reviews I write and receive are either &quot;Your paper is sound, but it needs
a little but more work to be &lt;em&gt;awesome&lt;/em&gt;&quot;, or &quot;Hey, you probably missed this
little flaw, correct it and we&#39;re good&quot;. There&#39;s the occasional blatant mistake
on my side, or crazy referee, but most of the time, it&#39;s just this perfect
blend of stimulating and annoying. There&#39;s no reason to hate people on these
grounds, and there&#39;s no reason to be afraid of attaching your name to that
either.  And you know what? The most awesome thing happenned to &lt;a href=&quot;http://ibartomeus.wordpress.com/2013/06/27/signing-reviews-pays-back-and-about-sharing-good-and-bad-news/&quot;&gt;Ignasi
Bartomeus&lt;/a&gt;. He got a &lt;em&gt;thank you note&lt;/em&gt; from the authors of a paper he
&quot;helped&quot; rejecting. &lt;em&gt;That&#39;s&lt;/em&gt; how you do it.&lt;/p&gt;

&lt;p&gt;So where does that leaves us? Sharing reviews can be a good way to share your
frustration, or illustrate all the is broken with a system where you have to
defend your work against people you don&#39;t know the names of (although we have
to keep in minds that the editors know what they&#39;re doing, and won&#39;t
probably pick referees at random...). But if we take the extra step of
having non-anonymous reviews, then sharing it (an not on our blogs or twitter,
but on the journal web-site) will perhaps rekindle our complicated
relationship with the peer-review procces. Because for all the frustration it
does us, it&#39;s still done by our &lt;strong&gt;peers&lt;/strong&gt;. People we hang with at meetings, drink
beers with, do awesome science with. We are inspired by the papers they write.
We find ideas in their data. We&#39;re not enemies; let&#39;s not behave as if it were
the case (except reviewer #3, this guy is the worse).&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>What do I read (according to Zotero)?</title>
     <link href="http://timotheepoisot.fr/2013/07/09/zotero-number-papers/"/>
    <updated>2013-07-09T00:00:00-04:00</updated>
    <content type="html">&lt;p&gt;I&#39;ve been playing around a bit with &lt;a href=&quot;http://zotero.org/&quot;&gt;Zotero&lt;/a&gt; these last few days.
Zotero stores the data about your library in a &lt;code&gt;sqlite3&lt;/code&gt; database, called
&lt;code&gt;zotero.sqlite&lt;/code&gt;, and stored in your Zotero folder (&lt;code&gt;~/Zotero/&lt;/code&gt; in my case, but
most likely something else in yours). The internal structure of the
database is very weird, and it took my a bit of poking around in &lt;code&gt;sqliteman&lt;/code&gt; to
figure it out. But in the end, I was able to come up with a solution to count
the number of papers in each journal, stored in my library.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;python&lt;/code&gt; script is &lt;a href=&quot;https://gist.github.com/tpoisot/5958933&quot;&gt;available as a gist&lt;/a&gt;. When you run it
(assuming you&#39;ve made a copy of the &lt;code&gt;zotero.sqlite&lt;/code&gt; in the folder where the
script is, you will end up with a &lt;code&gt;journals.tsv&lt;/code&gt; file, in which the two
columns are, respectively, the number of papers, and the journal title.&lt;/p&gt;

&lt;p&gt;Because despite the feature being &lt;em&gt;in progress&lt;/em&gt; for a few years now there is
still no bacth edit of journal titles (that I know of), it took me a bit of
cleaning-up in open office to check for duplicates, especially since my
current database is following me since the first year of my masters (so 2006).
So, what do I read? Here is the top 15:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:left;&quot;&gt; Journal                   &lt;/th&gt;
&lt;th style=&quot;text-align:left;&quot;&gt; Count &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; Ecology Letters           &lt;/td&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; 202   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; Ecology                   &lt;/td&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; 198   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; PLoS One                  &lt;/td&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; 196   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; American Naturalist       &lt;/td&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; 181   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; PNAS                      &lt;/td&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; 156   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; Oikos                     &lt;/td&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; 135   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; Trends Ecol Evol          &lt;/td&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; 132   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; Procs R Soc B             &lt;/td&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; 123   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; Evolution                 &lt;/td&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; 120   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; Nature                    &lt;/td&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; 119   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; Science                   &lt;/td&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; 108   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; J Evol Biol               &lt;/td&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; 101   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; J Anim Ecol               &lt;/td&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; 71    &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; J Biogeogr                &lt;/td&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; 46    &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; Phil Trans B              &lt;/td&gt;
&lt;td style=&quot;text-align:left;&quot;&gt; 45    &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;So, what to make of this information? Note that by the time we reach number 12
or so in that list, the paper count decreases extremely sharply. Also, the top
five is not reallu surprising. I was expecting to see &lt;em&gt;Nature&lt;/em&gt; and &lt;em&gt;Science&lt;/em&gt;
rank higher on the list. There is still some inertia, as well, with &lt;em&gt;J Evol
Biol&lt;/em&gt; and &lt;em&gt;Evolution&lt;/em&gt; occupying high positions because I&#39;ve read these some
much during my PhD and Masters. So I guess that (i) I&#39;m definitely an ecologist
(also, &lt;em&gt;J Theor Biol&lt;/em&gt; and &lt;em&gt;Theor Pop Biol&lt;/em&gt; are lurking not far below), and (ii)
despite this having a very very long tail, I mostly read things coming up in
a few journals.&lt;/p&gt;

&lt;p&gt;Now, the really pressing issue... How does it relates to their &lt;em&gt;Impact Factor&lt;/em&gt;?
A quick survey of the table above shows that some high scoring journals are
quite low on the list, and some not-so-high scoring ones are nearing the top.
A quick googling got me the most recent IF for the first 30 or so journals of my
list. The results are plotted below (put your mouse over a dot to see the journal name).&lt;/p&gt;

&lt;div class=&#39;scatterplot d3&#39;&gt;&lt;/div&gt;


&lt;script type=&quot;text/javascript&quot; src=&quot;/data/zotero-if.js&quot;&gt;&lt;/script&gt;


&lt;p&gt;It doesn&#39;t look like there is much effect of the IF on wether or not I&#39;ll save
the paper in my library. And we can also see a small group of three journals
I read much more than their IF would &quot;predict&quot; (&lt;em&gt;Ecology&lt;/em&gt;, &lt;em&gt;PLoS One&lt;/em&gt;, and &lt;em&gt;The
American Naturalist&lt;/em&gt;). So that turned out to be a fun little lunch coding
project. Don&#39;t hesitate to look at the code, I&#39;m sure there is plenty of things
to make better.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>A quick way to parallelize</title>
     <link href="http://timotheepoisot.fr/2013/07/08/parallel/"/>
    <updated>2013-07-08T00:00:00-04:00</updated>
    <content type="html">&lt;p&gt;I&#39; ve often been confronted to the situation when I want to run a bunch of
simulations, that will take long if I run them in batch, but long enough that
I want to put them on the lab cluster. A fairly common situation is when I want
to feed some sequentially numbered output files to a script performing all kind
of black magic (&lt;em&gt;a.k.a.&lt;/em&gt; data analysis).&lt;/p&gt;

&lt;p&gt;While looking for a solution, I came across &lt;a href=&quot;http://www.gnu.org/software/parallel/&quot;&gt;GNU &lt;code&gt;parallel&lt;/code&gt;&lt;/a&gt;. This
little command-line program will just execute whatever you feed him on the
different CPUs. For example, if I want to run all of the files in my &lt;code&gt;output/&lt;/code&gt; folder into a python script, I can write something like:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;bash&quot;&gt;ls output/*.dat | sort | parallel ./results.py --file output/&lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That will take the files, then output then one by one, and pass them to
&lt;code&gt;parallel&lt;/code&gt;. When I run that, all of my CPU start working on the task, so I can
cut the treatment time in four (more than what I could achieve with code
optimization!). It&#39;s definitely poor man parallelization, but it works, and
it&#39;s faster then setting up a bunch of simulations on a cluster.&lt;/p&gt;

&lt;p&gt;That&#39;s one of the (many many) nice sides of working only with GNU/Linux now.
I&#39;m learning a lot of nifty little tricks, and it&#39;s starting to really pay off
in terms of how fast I can throw command lines instructions at a problem until
it disappears (or until I accidentaly delete the files, look, I&#39;m still
learning...)..&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>Fixes for the ManuscriptCentral PDF conversion problem</title>
     <link href="http://timotheepoisot.fr/2013/06/24/manuscript-central-pdf-fix/"/>
    <updated>2013-06-24T00:00:00-04:00</updated>
    <content type="html">&lt;p&gt;While submitting a MS to &lt;em&gt;Methods in Ecology and Evolution&lt;/em&gt;, I ran into a PDF
conversion problem. It was not the first time it happenned, and always on
journals using &lt;em&gt;manuscriptcentral&lt;/em&gt; as an upload system, and always with
&lt;code&gt;LaTeX&lt;/code&gt;-generated PDF. My previous fix for this, in the proud quick-and-dirty
tradition of the academic world, was to (1) import the PDF in google documents,
(2) print it as a PDF, (3) upload the PDF and (4) drink the shame
away.&lt;/p&gt;

&lt;p&gt;Only this time, I wanted to understand what was happening, and to fix it as much
as I could. Well, as it turns out, the &lt;em&gt;manuscriptcentral&lt;/em&gt; system only accepts
PDF up to version 1.5 of the standard. Reasonably recent &lt;code&gt;LaTeX&lt;/code&gt; distributions
will generate 1.7 or 1.8, so it&#39;s obviously not going to work (but on the
up-side, it will give you one of the most depressing and amusing error
message ever: your PDF file cannot be converted in the correct PDF format). So to fix it, there are a number of workarounds.&lt;/p&gt;

&lt;p&gt;First, you can add&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;latex&quot;&gt;&lt;span class=&quot;k&quot;&gt;\pdfminorversion&lt;/span&gt;=4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;to your &lt;code&gt;LaTeX&lt;/code&gt; preamble, if compiling with &lt;code&gt;pdflatex&lt;/code&gt;, as explained
&lt;a href=&quot;http://www.jakubkonka.com/2012/11/07/latex-manuscript-central.html&quot;&gt;here&lt;/a&gt;. That&#39;s one additional command, which is not needed in most other situations, but that&#39;s one way to fix it.&lt;/p&gt;

&lt;p&gt;There is a second workaround, which is to compile with &lt;code&gt;latex&lt;/code&gt; instead of
&lt;code&gt;pdflatex&lt;/code&gt; (haven&#39;t tested this one, but it should work), then upload the whole
dependencies when you submit. Note that your figures needs be &lt;code&gt;eps&lt;/code&gt; for this to
work, and you&#39;ll probably be limited by the packages installed on the remote
server, the list of which is not known. The &lt;em&gt;Methods in Ecology and Evolution&lt;/em&gt;
staff was extremely quick in pointing that out after I raised the issue on
Twitter, so a big thank you to them.&lt;/p&gt;

&lt;p&gt;The solution I found this morning is the following:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;bash&quot;&gt;pdftops ms.pdf ms.ps
ps2pdf13 ms.ps ps.pdf
rm ms.ps
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Very simply put, that will convert your pdf into a postscript file, then convert
it into a PDF 1.3 file, that can be uploaded with no problems. You can just add
that to a &lt;code&gt;makefile&lt;/code&gt;, by the way:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;make&quot;&gt;&lt;span class=&quot;nf&quot;&gt;mc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;ms.pdf&lt;/span&gt;
    pdftops ms.pdf ms.ps
    ps2pdf13 ms.ps ms_mc.pdf
    rm ms.ps
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Once your paper is ready, a simple &lt;code&gt;make mc&lt;/code&gt; will create a &lt;code&gt;ms_mc.pdf&lt;/code&gt; file,
which is suitable for upload. So here you go, happy upload to all of you, and may the editorial decisions ever be in your favor.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>A niche to fill in ecological publishing?</title>
     <link href="http://timotheepoisot.fr/2013/06/23/niche-fill-ecological-papers/"/>
    <updated>2013-06-23T00:00:00-04:00</updated>
    <content type="html">&lt;p&gt;I have been wondering, over the past few weeks, about places to submit
really short, methodological minded papers, in ecology. As far as I know,
there is currently no journal for that. I&#39;m specifically talking
about papers that are not groundbreaking, possibly extremely
technical, but still hold value because they deal with arguments that
happen often. As a good example, the reply of &lt;a href=&quot;http://arxiv.org/abs/1301.3651&quot;&gt;Rohr and
colleagues&lt;/a&gt; to the &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/22722863&quot;&gt;James and colleagues&lt;/a&gt; paper is interesting, but technical. It&#39;s unlikely to stimulate a debate in the whole community, but for the people active on the topic, it&#39;s something to read and think of (regardless of who you side with on the issue discussed).&lt;/p&gt;

&lt;p&gt;So where to publish this? &lt;em&gt;&lt;a href=&quot;http://www.methodsinecologyandevolution.org/view/0/authorGuidelines.html#Editorial%20Policy&quot;&gt;Methods in Ecology and Evolution&lt;/a&gt;&lt;/em&gt; seems to
be &lt;em&gt;the&lt;/em&gt; ideal place for that, but their guidelines do not offer the
opportunity to submit this kind of papers. &lt;em&gt;F1000Research&lt;/em&gt; and &lt;em&gt;PeerJ&lt;/em&gt; are good
places as well, but fairly expensive is you want to do a one-off thing. You
always have the opportunity to publish things in your blog, but unless you are
a big-name blogger, your result will go un-noticed (which is why I&#39;ll work on
turning &lt;a href=&quot;http://timotheepoisot.fr/2013/06/19/notes-on-network-complexity/&quot;&gt;this entry&lt;/a&gt; into a short note somewhere.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;ArXiV&lt;/em&gt; is another possibility, but the readership in biological sciences is
&lt;a href=&quot;http://www.plosbiology.org/article/metrics/info%3Adoi%2F10.1371%2Fjournal.pbio.1001563&quot;&gt;too low at the moment&lt;/a&gt;. So there is possibly a niche to fill for an
adventurous journal. Let&#39;s give us a place to write short notes, so that the
active debates are &lt;em&gt;documented&lt;/em&gt;, no matter what the number of people debating
is. What do you think?&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>A note on network complexity</title>
     <link href="http://timotheepoisot.fr/2013/06/19/notes-on-network-complexity/"/>
    <updated>2013-06-19T00:00:00-04:00</updated>
    <content type="html">&lt;p&gt;I have seen the word &lt;em&gt;complexity&lt;/em&gt; used to mean approximately anything in
ecological network litterature, and to be  extremely honest, I&#39;m entirely guilty
of that myself. We can probably find as many uses for &lt;em&gt;complexity&lt;/em&gt; in the
network litterature as there are uses for &lt;em&gt;paradigm&lt;/em&gt; in Thomas Kuhn&#39;s &lt;em&gt;The
Structure of Scientific Revolutions&lt;/em&gt; (which is to say, probbaly way too much,
and certainly enough to confuse people). It&#39;s getting bad enough that some
of us (and again, I&#39;m probably guilty of that) use &lt;em&gt;complex network&lt;/em&gt; any time we
should just say &lt;em&gt;network&lt;/em&gt;. Or &lt;em&gt;species-rich network&lt;/em&gt; (which we can argue is not
necessarily complex), or &lt;em&gt;densely connected network&lt;/em&gt; (same thing).&lt;/p&gt;

&lt;div class=&quot;alert alert-info&quot;&gt;
&lt;h4&gt;Warning!&lt;/h4&gt;
This was supposed to be a short post. It&#39;s not. Unless you are terribly interested in networks, you can safely skip this one. If you have an eye for statistics, please read through, there are likely mistakes to correct...&lt;/div&gt;


&lt;p&gt;Consider this post my humble attempt to clarify things. The thing is, networks
are physically constrained, in that you can&#39;t put more edges in them than the
number of nodes will allow. In ecological terms, a predator won&#39;t eat a prey
more than once. So for example, one could argue that a very densely connected
network is not complex. The &lt;em&gt;dynamics of the community&lt;/em&gt; probably will be, but
the network in itself has no exceptional properties to speak of. In the same
line of thinking, a minimally connected network is simple as well. It can have
1000 species in a linear chain, that won&#39;t give it any properties worth being
excited about (in terms of topology that is, because a super-long food chain is
pretty exciting by itself).&lt;/p&gt;

&lt;p&gt;So let&#39;s start this way. What are the lower and upper bounds of the number of
links you can put in a network made of a given number of species? The key issue
is that there should be no species with a degree of 0, which is to say
everything should have at least one partner. To keep things simple, we&#39;ll assume
that we work on binary undirected adjacency matrices, with no self-loops.
Finding the lower bound is simple enough. If you want a strongly connected
graph, so that you can reach any node from any other node by travelling along
edges, the minimal number of edges if you have N nodes is simply N-1. If you
don&#39;t care about your graph being strongly connected, then the minimal number of
edges is (N+1)/2 if N is odd, and N/2 if N is even.&lt;/p&gt;

&lt;p&gt;Whether you care or not about the graph being strongly connected, the maximal
number of edges in a graph with N nodes, &lt;em&gt;i.e.&lt;/em&gt; a complete graph, is N(N-1)/2.
This is a starting point: for any number of N species, we can predict the
interval in which the number of interactions will fall. Now, keep in mind that
(species indentity nonwithstanding), there is only one way to have a complete
graph, and only one way to have the minimally connected graph. This is a cool
thing right here: most of the action will happen for intermediate number of
edges within the range! And a thing that serves our purpose: the maximal number
of edges is the maximal number of &quot;slots&quot; that we can fill with edges. This
becomes a quite simple combination problem.&lt;/p&gt;

&lt;p&gt;So here is an interesting questions: knowing N, and a number of edges L, how
many different graphs can I produce? This is simple enough to solve, we just
need to calculate how many ways there are to put L elements in N(N-1)/2
elements. Let&#39;s call M the maximal number of edges, and the solution to this
problem is simply M!/(L!(M-L)!), which we will note C(M,L). For example, with
4 nodes, the maximal number of edges M is 6. How many ways are there to
distribute 5 edges between 4 nodes? Using the formula above, we get 6.&lt;/p&gt;

&lt;p&gt;But that&#39;s not quite the original problem: among these 6 combinations, how many
of them leave at least one node unconnected? It took me a while to figure out
the rather simple solution. In the simple system with 4 species, and 5 edges, if
we assume that one species has no connection, then we will look for the number
of combinations of 5 edges, in the maximal number of edges allowed by 3 nodes.
With three nodes, there is at most 3 edges, and as it turns out, it&#39;s impossible
to make 5 edges fit in only 3 slots. So with this simple exercice, we can say
that there are 6 different networks of 5 edges and 4 nodes that have no
unconnected nodes.&lt;/p&gt;

&lt;p&gt;Let&#39;s now try a slightly more complicated (noticed how I didn&#39;t wrote &lt;em&gt;complex&lt;/em&gt;?
Good.) situation. Assuming 5 nodes, we will have between 3 and 10 edges.
A very interesting question is: if I have 3 edges to distribute in this network,
how many combinations can I do with no unconnected edges? C(10,3) gives 120 of
them. But, we need to account for the fact that some of these combinations
will result in nodes having no edges. Specifically, there are 20 ways to
distribute 3 edges in a 4 node network. Note that these 20 ways also include
all the possible combinations of 3 edges in a 3 nodes network, so just
counting the number of networks connecting at most N-1 is good. So here is the
routine: removing one node, how many combination can I make with the same
number of edges? The answer is simply the number of times I can distribute
L edges in a network of size N-1. In our example, as stated above, this is 20.
But we need to do that for &lt;em&gt;each&lt;/em&gt; node in the network, so there are actually
100 (5 times 20) combinations we need to remove. This left us with only 20
&quot;good&quot; networks.&lt;/p&gt;

&lt;p&gt;Let&#39;s wrap things up. If we call m(N) the mininal number of edges in a network
of N nodes, and M(N) the maximal number of edges in the same network, and C(X,L)
the number of ways to distribute L edges in X possible edges configurations,
then we can calculate the number of networks with N links and L edges for
which no node is unconnected. This is quite simply C(M(N),L)-N*C(M(N-1),L),
and we will call this R(N,L). You may not realize it yet, but this actually
quite cool. What does it means for complexity? Well, that   a network with
a given number of species will be more or less constrained in the diversity of
structure it can adopt, as a function of the number of interactions.&lt;/p&gt;

&lt;p&gt;It is actually easy to measure the amount to which a network is constrained.
Simply dividing R(N,L) by C(N,L) will give an idea of how difficult is is to
find a network with no free nodes given N and L. There are important
applications, like predicting the time needed to generate null models of network
structure. But also, we can ask how many &lt;em&gt;different&lt;/em&gt; networks we will be able to
generate for a given network size.&lt;/p&gt;

&lt;p&gt;I ran some simple tests using 10 nodes, and here is what happenned.&lt;/p&gt;

&lt;p&gt;First, the size of the &quot;network space&quot; is hump-shaped, which means that for
intermediate number of links, you have much more work to do if you want to
explore the possible configurations that your network can take. Things seems to
be &lt;em&gt;more&lt;/em&gt; complex (or at least have more potential for complexity) at
intermediate connectance values.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/netspacedistrib.png&quot; title=&quot;Figure 1&quot; alt=&quot;Figure1&quot; /&gt;
Note that the number of networks with no free nodes is, all things considered,
quite close to the whole network space, so you&#39;ll have to be quite unlucky
not to have all nodes connected. But how strong exactly is the constraint
on network structure? Let&#39;s measure 1 minus the ratio of networks having no
free nodes:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/netspaceconstraint.png&quot; title=&quot;Figure 2&quot; alt=&quot;Figure2&quot; /&gt;
Quite simply, it shows that when the networks are not strongly connected, you
have a lot of chance to pick at random a network in which nodes are not
connected. This has important consequences for the geenration of null models: it
is &lt;em&gt;extremely difficult&lt;/em&gt; to generate random networks based on large, sparsely
connected networks. You know what is large and sparsely connected? All
ecological networks. Yeah!&lt;/p&gt;

&lt;p&gt;So, it&#39;s time to wrap things up. First, networks have more potential for
complexity when the connectance takes on intermediate values. That&#39;s a cool
little piece of information with some ecological potential. Second, the space of
extremely connected, and extremely sparse networks, is small, so both are
extremely constrained. Finally, sparse networks are difficult to generate if you
want all species to be connected. In short, if you want 1000 null replicates,
you&#39;ll need to run a lot more trials than that.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
     <title>The structure of natural bacteria-phage networks</title>
     <link href="http://timotheepoisot.fr/2013/05/30/bacteria-soil-networks/"/>
    <updated>2013-05-30T00:00:00-04:00</updated>
    <content type="html">&lt;p&gt;Freedom! The final chapter of my thesis is now online on the website of &lt;a href=&quot;http://www.ecologicalprocesses.com/content/2/1/13/abstract&quot;&gt;&lt;em&gt;Ecological Processes&lt;/em&gt;&lt;/a&gt;. We use data form a field isolated bacteria-phage system to have a network approach of the specialist-generalist coexistence problem. The main result is that, even when controlling for phylogenetic dispersal of hosts, it&#39;s better to be a generalist. This signal is highly consistent across the five networks we studied, and goes in the way of recent results on coevolution: if you let a system coevolve long enough, it&#39;s expected that generalists will emerge.&lt;/p&gt;
</content>
  </entry>
  
</feed>
