---
type: note
title: Responsibility and sensationalism in high-profile papers
author: Tim
layout: post
tags:
- scientific publishing
- ethics
---

Last week, we had an inter-lab journal club during which we discussed
a very polemic paper in ecology. The discussion quickly turned into the
responsibility of people at the various levels of the scientific publishing
process, and the risks associated with counter-mainstream results appearing
in high-profile journals. Most of what follows is my recollection of the
different arguments we raised for and against self-censorship in scientific
publishing. Which paper we discussed is not relevant for the rest of this
post, so I'll try to summarize the issue in extremely broad terms. There is a
well-known relationship between concept *B* and property *C*. Changes in *B*
are expected, under some circumstances, to result in changes in *C*. *B*
is a complex thing with several layers to it. A set of studies showed no
variation of *A*, which is a component of *B*, and so it seems to question
the relationship between *B* and *C*.

For different reasons, whether there is (i) a change in *A*, and (ii)
a link between *B* and *C*, is a pretty big deal when it comes to policy
making. "Proving" that the relationship is not general, or not important,
or even wrong, is susceptible to have real-world consequences. In addition,
it would be a big deal for theoretical science as well, because it will mean
that what we thought (and still think) of an important concept in ecology,
is wrong and need to be re-built. But it would also mean that, as far as
policy makers are concerned, we can persist in several "business as usual"
practices; after all, there are now data to back them up!

So, should we practice self-censorship? Absolutely not. The "cost" of making
all information public has been particularly discussed in the context of
bio-terrorism; understanding the risks requires results to be published, but
publishing results can increase [the ease to produce bioweapons][bioter1]. Some
argue that [not all results should be published][bioter2], but determining
which should, and which should not, is a tricky issue. Some ecological results
have very strong relevance for policy-making (and lobbies), and there is a
risk that some will have a political reading of some papers.

The question is, therefore, who is responsible for "customer service"[^1]
once a new result appears? First thing first, the authors. I have the
feeling that a perverse effect of the publish-or-perish mentality is a
quest for sensationalism (which seems to be confirmed by this [analysis of
adverbs used in PMC papers][adv], where more abstracts emphasize interest,
surprise, and novelty). And when we (scientists) read a paper making bold
claims, we are able to rationalize them because it's not the first paper we
are reading. We are able to weigh one result against dozens of others. But
my very personal and humble opinion is that, when the result we report is
likely to make a splash, we have to consider whether the way we express it
is likely to be mistaken by the public.

Second, editors have a role to play. There is a well documented relationship
between [impact factor and likelihood of retractation][retract], and a tendency
for the media to cover [not the best conducted studies][plosmedia] (perhaps,
has we hypothesized, because most science is normal, and normal science is
very rarely cool for people outside the field). It would make sense that the
boxes with comments from the editor, that seem to appear in more and more
journals, be dedicated to putting sensational papers into the broader context.

And finally, science journalists. One of the things that never fails
to amaze/annoy me is that, when a vastly mediatised paper ends up being
retracted, there is almost never any indication of that in the media (our
short attention span be damned). Which means that unless you specifically
follow people discussing the issue, do you know that bacteria are still
unable to use arsenic in their DNA? I get that saying "Nope, turned out
we were pretty much right about how life works" is unlikely to make the
headlines (unless you want to, you know, *inform*). As this post [really
clearly states][emb], the ability to react *right now* on a big story is
important for the career of journalists. It lead to (in my mind) absurd
situations in which journalists can prepare their copy before the majority
of the scientific community knows the results.

As a final note, I have nothing against sensationalism. If anything (if
it is backed by facts and sound interpretation), sensational ideas and
results are a great way to make people interested in science. Telling
a story about the population dynamics of an emblematic animal makes
for better communication than the subtleties of a stochastic, spatially
explicit generalized Lotka-Volterra model (somehow). But everything even
remotely close to [post-normal science][pns] should be written carefully,
contextualized carefully, and reported on carefully.

[^1]: I forgot who came up with the "customer service of scientific results" idea, but I find it brilliant...

[bioter1]: http://www.ncbi.nlm.nih.gov/books/NBK98397/
[bioter2]: http://onlinelibrary.wiley.com/doi/10.1353/hcr.2007.0046/abstract
[adv]: http://nsaunders.wordpress.com/2013/07/16/interestingly-the-sentence-adverbs-of-pubmed-central/
[retract]: http://retractionwatch.com/2011/08/11/is-it-time-for-a-retraction-index/
[plosmedia]: http://www.plosone.org/article/info:doi%2F10.1371%2Fjournal.pone.0085355
[emb]: http://scienceblogs.com/notrocketscience/2009/07/04/does-science-journalism-falter-or-flourish-under-embargo/
[pns]: http://en.wikipedia.org/wiki/Post-normal_science
