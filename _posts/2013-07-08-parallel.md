---
layout: post
title: A quick way to parallelize
author: Tim
type: note
tags:
- GNU
- free software
---

I have often been confronted to the situation when I want to run a bunch of
simulations, that will take long if I run them in batch, but long enough that
I want to put them on the lab cluster. A fairly common situation is when I
want to feed some sequentially numbered output files to a script performing
all kind of black magic (*a.k.a.* data analysis).

While looking for a solution, I came across [GNU `parallel`][gnupar]. This
little command-line program will just execute whatever you feed him on
the different CPUs. For example, if I want to run all of the files in my
`output/` folder into a python script, I can write something like:

~~~ sh
ls output/*.dat | sort | parallel ./results.py --file output/{}
~~~

That will take the files, then output then one by one, and pass them to
`parallel`. When I run that, all of my CPU start working on the task, so I can
cut the treatment time in four (more than what I could achieve with code
optimization!). It's definitely poor man parallelization, but it works, and
it's faster then setting up a bunch of simulations on a cluster.

That's one of the (many many) nice sides of working only with GNU/Linux now.
I'm learning a lot of nifty little tricks, and it's starting to really
pay off in terms of how fast I can throw command lines instructions at a
problem until it disappears (or until I accidentally delete the files, look,
I'm still learning...).

[gnupar]: http://www.gnu.org/software/parallel/
